---
layout: posts
comments: true
title: "[Spark]ìŠ¤íŒŒí¬ ë³µìŠµ - 4(Streaming/Mlib/ë²ˆì™¸/íŒŒì´í”„ë¼ì¸)"
categories: Spark
tag: [Spark, ìŠ¤íŒŒí¬, Hadoop, í•˜ë‘¡, Pyspark]


toc: true
toc_icon: "cog"
toc_sticky: true
date: 2022-03-17
last_modified_at: 2022-03-17
---



# Spark 4ì¼ì°¨

## Spark Streaming

* ì°¸ê³   : [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
* ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ë¥¼ mini-batch sizeë¡œ ë¶„ë¦¬ -> RDDì™€ ìœ ì‚¬í•˜ê²Œ ì²˜ë¦¬

![image](https://spark.apache.org/docs/latest/img/streaming-flow.png)

* DStream(Discretized Stream[ë°ì´í„°ì˜ ì—°ì†ì ì¸ íë¦„]) : ë°ì´í„° ì¶”ìƒí™”
  * ì—°ì†ì ì¸ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì—ì„œ ìƒì„±ëœ ì—°ì†ì ì¸ RDD ì‹œí€€ìŠ¤
  * ì†Œì¼“, ë©”ì‹œì§• ì‹œìŠ¤í…œ, ìŠ¤íŠ¸ë¦¬ë° api ë“±ì˜ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ë°›ì•„ì„œ ìƒì„±
  * RDDê°€ ì‘ì„±ë˜ëŠ” ê²ƒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì…ë ¥ ë°ì´í„° ì €ì¥(ë‚´ë¶€ì ìœ¼ë¡œ RDD)
  * Tumbling window, Sliding window

![image](https://spark.apache.org/docs/latest/img/streaming-arch.png)



* spark ì‚¬ì´íŠ¸ ì˜ˆì œ

1. `vim streaming_test.py`
2. ì‚¬ì´íŠ¸ ì˜ˆì œ ë³µì‚¬í•´ì„œ ë¶™ì—¬ë„£ê¸° í›„ ì €ì¥

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Create a local StreamingContext with two working thread and batch interval of 1 second
sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 1)

# Create a DStream that will connect to hostname:port, like localhost:9999
lines = ssc.socketTextStream("localhost", 9999)

# Split each line into words
words = lines.flatMap(lambda line: line.split(" "))

# Count each word in each batch
pairs = words.map(lambda word: (word, 1))
wordCounts = pairs.reduceByKey(lambda x, y: x + y)

# Print the first ten elements of each RDD generated in this DStream to the console
wordCounts.pprint()

ssc.start()             # Start the computation
ssc.awaitTermination()  # Wait for the computation to terminate
```

![image](https://user-images.githubusercontent.com/75322297/158711927-35a43109-b88f-48f8-9c36-043511f93fd2.png)

3. `nc -lk 9999` ì…ë ¥

4. ìƒˆ í”„ë¡¬í”„íŠ¸ ì°½ ì‹¤í–‰ í›„ `spark-submit streaming_test.py localhost 9999`

   * ê·¸ëŸ¬ë©´ ë­ê°€ ê³„ì† ì‘ë™ë˜ëŠ” ê²ƒì´ ë³´ì¸ë‹¤.

5. ê¸°ì¡´ì˜ í”„ë¡¬í”„íŠ¸ì°½ì—ì„œ ë­”ê°€ë¥¼ ì…ë ¥í•´ë³´ì.

   * ê·¸ëŸ¬ë©´ ìƒˆ í”„ë¡¬í”„íŠ¸ì°½ì—ì„œ 5ì´ˆë§ˆë‹¤ ì…ë ¥í•´ì¤€ ê°’ë“¤ì„ ë°›ì•„ì¤€ë‹¤.

   ![image](https://user-images.githubusercontent.com/75322297/158712183-d6b74cfe-8a57-4c80-8a77-e8f188f600e8.png)



<br>

<br>



## MLlib

* java, scala, python, r ë“±ì˜ ì¸í„°í˜ì´ìŠ¤ì— ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.
* RDD ê¸°ë°˜ì˜ **M**achine **L**earning **LIB**raryë‹¤.
* `pyspark.mllib` -> **pyspark.ml**(DataFrame ê¸°ë°˜ì˜ Machine Learning Packageë¡œ ì „í™˜)
  * pipline : ê° stage ìƒì„± ë° ì—°ê²°, ì‹¤í–‰
  * spark.mllib -:  ì € ìˆ˜ì¤€ rdd api ì‚¬ìš©
  * spark.ml : dataframe api ì‚¬ìš©

* ì˜ˆì œ

```python
# ë²¡í„° ìƒì„±
from pyspark.ml.linalg import Vectors
denseVec = Vectors.dense(1.0, 2.0, 3.0)
size = 3
idx = [1, 2]
values = [2.0, 3.0]
sparseVec = Vectors.sparse(size, idx, values)
# ìƒ˜í”Œ ë¡œë”©
df = spark.read.json("/home/big/data/simple-ml/")
df.show()
# ê³ ê° ê±´ê°• ë°ì´í„°ì…‹
# color : ê±´ê°• ë“±ê¸‰
# lab : ì‹¤ì œ ê±´ê°• ìƒíƒœ
# value1 / value2 : ì‚¬ì´íŠ¸ ì²´ë¥˜ ì‹œê°„ / êµ¬ë§¤ ì†Œìš” ì‹œê°„

###
from pyspark.ml.feature import RFormula
# RFormula : ë°ì´í„° ë³€í™˜ ì„¤ì •
# MLlibì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„° í˜•íƒœë¡œ ë³€í™˜ (Double / Vector[Double] : ë ˆì´ë¸” / ê°’)
# ~ : targetê³¼ term ë¶„ë¦¬
# + : ì—°ê²°
# - : ì‚­ì œ
# : ìƒí˜¸ì‘ìš© (ê³±ì…ˆ)
# . : ëª¨ë“  ì»¬ëŸ¼
# ëª¨ë“  ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ, value1ê³¼ color / value2ì™€ colorë¥¼ ê³±í•´ì„œ ìƒˆë¡œìš´ ê°’ìœ¼ë¡œ ë³€ê²½
supervised = RFormula(formula="lab ~ . + color:value1 + color:value2")

# dataframe
fittedRF = supervised.fit(df)
preparedDF = fittedRF.transform(df)
preparedDF.show(5, False)
# ì—¬ê¸°ê¹Œì§€ê°€ ë°ì´í„° ì „ì²˜ë¦¬

# trainset / testset ë¶„ë¦¬
train, test = preparedDF.randomSplit([0.7, 0.3])
# ëª¨ë¸ ì•Œê³ ë¦¬ì¦˜
from pyspark.ml.classification import LogisticRegression
lr = LogisticRegression(labelCol="label", featuresCol="features")
# lrì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„° (í•˜ì´í¼íŒŒë¼ë¯¸í„°)
print(lr.explainParams())
# í•™ìŠµ ëª¨ë¸ ìƒì„±
fittedLR = lr.fit(train)
# í•™ìŠµ ëª¨ë¸ë¡œ ì˜ˆì¸¡í•  ë•Œ í˜•íƒœ í™•ì¸ 
fittedLR.transform(train).select("label", "prediction").show()
### ì‚¬ì „í•™ìŠµì´ë¼ê³  í•¨

# íŒŒì´í”„ë¼ì¸
train, test = df.randomSplit([0.7, 0.3])
# ë³€í™˜
from pyspark.ml.feature import RFormula
rForm = RFormula()

from pyspark.ml.classification import LogisticRegression
lr = LogisticRegression().setLabelCol("label").setFeaturesCol("features")
from pyspark.ml import Pipeline
# ë³€í™˜ ë° ëª¨ë¸ íŠœë‹ì„ pipelineì˜ ë‹¨ê³„ë¡œ ìƒì„±
stages = [rForm, lr]
pipeline = Pipeline().setStages(stages)

# ëª¨ë¸ í•™ìŠµ
# ParamGridBuilder : í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°í•©í•´ì£¼ëŠ” ê°ì²´
# elasticNetParam : 0 ~ 1 ì‚¬ì´ì˜ ë¶€ë™ì†Œìˆ˜ì  ì§€ì •í•˜ì—¬ ê°€ì¤‘ì¹˜ ìƒì„±
# regParam : 0ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì€ ê°’ ì§€ì •. ê°€ì¤‘ì¹˜ ìƒì„±
from pyspark.ml.tuning import ParamGridBuilder
params = ParamGridBuilder().addGrid(rForm.formula, ["lab ~ . + color:value1", "lab ~ . + color:value1 + color:value2"]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).addGrid(lr.regParam, [0.1, 0.2]).build()
# rformular 2ê°œ (color:value1, color: value2)
# eleasticnetparam 3ê°œ (0.0, 0.5, 1.0)
# regparam 2ê°œ (0.1,  0.2)
# í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•˜ì—¬ ì´ 12ê°œ í•™ìŠµ

# í‰ê°€ ê°ì²´
# ì´ì§„ ë¶„ë¥˜ í‰ê°€
# areaUnderROC : ê·¸ë˜í”„ ì•„ë˜ìª½ ë©´ì ì„ ê³„ì‚°í•˜ì—¬ ëª¨ë¸ í‰ê°€ (ì–¼ë§ˆë‚˜ ì˜ ë§ì¶”ê³  ìˆëŠ”ì§€)
from pyspark.ml.evaluation import BinaryClassificationEvaluator
evaluator = BinaryClassificationEvaluator().setMetricName("areaUnderROC").setRawPredictionCol("prediction").setLabelCol("label")

# k-fold (k-ê²¹ êµì°¨ê²€ì¦)
# training setì„ kê°œì˜ foldë¡œ ë‚˜ëˆ„ê³ , fold ì¤‘ í•˜ë‚˜ë¥¼ validation setìœ¼ë¡œ ì‚¬ìš© (ë‚˜ë¨¸ì§€ëŠ” training set)
from pyspark.ml.tuning import TrainValidationSplit
tvs = TrainValidationSplit().setTrainRatio(0.75).setEstimatorParamMaps(params).setEstimator(pipeline).setEvaluator(evaluator)

# íŒŒì´í”„ë¼ì¸ êµ¬ë™ (í•™ìŠµ)
tvsFitted = tvs.fit(train)

# í‰ê°€
evaluator.evaluate(tvsFitted.transform(test))

```







<br>

<br>



## ë²ˆì™¸ : ì±„íŒ…ë¡œê·¸íŒŒì¼ ê°€ì§€ê³  ë¶„ì„í•˜ê¸°

* ë¯¸ë¦¬ `pip install numpy`ë¡œ ë„˜íŒŒì´ íŒ¨í‚¤ì§€ë¥¼ ë‹¤ìš´ë°›ì•„ ë†”ì•¼ í•œë‹¤.

* ìš°ë¦¬ë“¤ì˜ ì¤Œ ì±„íŒ…ì°½ ëŒ€í™”ê°€ ë‹´ê²¨ìˆëŠ” `all.csv ` ë¥¼ ìš°ë¶„íˆ¬ /home ìœ¼ë¡œ ì˜®ê²¨ì¤€ë‹¤.

  ![image](https://user-images.githubusercontent.com/75322297/158712368-9de1241e-2d18-4bc3-a447-2bd6d96aa002.png)

* `cat all.csv` ë¡œ ë‚´ìš© í™•ì¸

  ![image](https://user-images.githubusercontent.com/75322297/158712534-bd4649a2-5d1f-44fb-804c-e74d51d68070.png)

* í•˜ë‘¡ìœ¼ë¡œ íŒŒì¼ ì˜®ê¸°ê¸° : `hdfs dfs -put ./all.csv /home/big/all.csv`

* ì˜ ì˜¬ë¼ê°”ëŠ”ì§€ í™•ì¸ : `hdfs dfs -ls /home/big`

  ![image](https://user-images.githubusercontent.com/75322297/158712775-58481b88-0371-4b95-b485-0f57d2aac883.png)

* íŒŒì´ìŠ¤íŒŒí¬ ì‹¤í–‰ : `pyspark`

* pysparkì—ì„œ all.csvíŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

  * `multi = spark.read.format("csv").option('header', "true").load("/home/big/all.csv")`

* ëŒ€í™”ë‚´ìš© ì¤„ ìˆ˜  í™•ì¸ : `multi.count()`

  ![image](https://user-images.githubusercontent.com/75322297/158713059-0752a117-20ef-47b2-84c3-84495b5ed463.png)

* í…Œì´ë¸” ìƒì„±  í›„ í™•ì¸

  * `multi.createOrReplaceTempView("multi")`
  * `multi.show(multi.count())`
  * ì»¬ëŸ¼ì€ ì´ë ‡ê²Œ êµ¬ì„±ë˜ì–´ìˆìŒ

  ![image](https://user-images.githubusercontent.com/75322297/158713186-cef10a83-d84b-4896-9f99-db073cad0273.png)

  

* ì±„íŒ…ì„ ê°€ì¥ ë§ì´ ë³´ë‚¸ ì‚¬ëŒ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ë³´ì.(7ë“±ì„ í–ˆë‹¤..ã…‹ã…‹ã…‹ã…‹ğŸ˜…)

```python
from pyspark.sql.functions import col
multi.groupBy("chat_from").count().orderBy(col("count").desc()).show()
```

![image](https://user-images.githubusercontent.com/75322297/158713327-4c931ec0-6c39-4cdd-9eef-6c50e2386d31.png)

* ê°•ì‚¬ë‹˜ì—ê²Œ DMì„ ê°€ì¥ ë§ì´ ë³´ë‚¸ ì‚¬ëŒ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê¸°

```python
multi.where(col("chat_to") == "ì´ë™í—Œê°•ì‚¬").groupBy("chat_from", "chat_to")\
.count().orderBy(col("count").desc()).show()
```

![image](https://user-images.githubusercontent.com/75322297/158713715-2bd4ae77-99ed-482b-8873-c13eecbba69b.png)



* ì‹œê°„ ë³„ ì±„íŒ… íšŸìˆ˜

```python
from pyspark.sql.functions import substring
multi.groupBy(substring("chat_time", 1, 2)).count().orderBy(substring("chat_time", 1, 2)).show()
```

![image](https://user-images.githubusercontent.com/75322297/158713938-a82d6830-35a5-45f4-bc2a-1e2f043ec11a.png)



* ë‚ ì§œ ë³„ ì±„íŒ… íšŸìˆ˜

```python
multi.groupBy("chat_date").count().orderBy("chat_date").show()
```

![image](https://user-images.githubusercontent.com/75322297/158714084-2bf8c774-4267-4958-ac89-a7f945aa0811.png)



* ë‚ ì§œì™€ ì‹œê°„ ë³„ ì±„íŒ… íšŸìˆ˜

```python
multi.groupBy("chat_date", substring("chat_time", 1, 2))\
.count().orderBy("chat_date",substring("chat_time", 1, 2)).show()
```

![image](https://user-images.githubusercontent.com/75322297/158714407-398045f3-1df7-4764-b20c-2621da865cb7.png)



* ì‹œê°„ ë³„  ê° ì¸ì›ë“¤ì˜ ì±„íŒ… ìˆ˜

```python
hour_chat = multi.groupBy(substring("chat_time", 1, 2).alias("hour"), "chat_from")\
.count().orderBy(substring("chat_time", 1, 2), col("count").desc())

hour_chat.show(hour_chat.count())
```

![image](https://user-images.githubusercontent.com/75322297/158714701-f2240ea8-a5bd-43ea-8edd-fc9c623f9ae5.png)

* ë°”ë¡œ ìœ„ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•´ë³´ì.

```python
hour_chat.write.format("csv").mode("overwrite").save("hour_chat")
```

ë¡œì»¬ í˜¸ìŠ¤íŠ¸ì—ì„œ í™•ì¸í•˜ë‹ˆ RDDê°€ í•˜ë‚˜ì”© ë‚˜ë‰˜ì–´ì„œ ì €ì¥ë˜ì–´ìˆë‹¤.

![image](https://user-images.githubusercontent.com/75322297/158715057-339317d6-6e23-4725-9edd-b15b47f4482b.png)

```python
hour_chat.coalesce(1).write.format("csv").mode("overwrite").save("/home/big/hour_chat")
```

![image](https://user-images.githubusercontent.com/75322297/158715247-0b090b0c-2cad-470b-a4c6-ce0dd3f5e874.png)

`coalesce()`ë¥¼ ì‚¬ìš©í•´ì„œ ë°ì´í„°ë¥¼ ë‚˜ë‰˜ì§€ ì•Šê³  í•˜ë‚˜ì— ëª°ì•„ì„œ ì €ì¥ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.



<br>



* ì´ì œ ì´ ê³¼ì •ë“¤ì„ íŒŒì¼ë¡œ ë§Œë“œëŠ” ë°©ë²•ì„ ì§€ê¸ˆë¶€í„° í•´ë³´ì.
* `vim hour_chat.py` ìƒì„± í›„ ë‚´ìš© ì‘ì„±

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, substring

# SparkSessionì™€ í´ëŸ¬ìŠ¤í„°ë§¤ë‹ˆì ¸ yarnì„ ì‚¬ìš©í•´ ì„¸ì…˜ ê°ì²´ ìƒì„±
spark = SparkSession.builder.master("yarn").appName("hour_chat").getOrCreate()

multi = spark.read.format("csv").option("header", "true").load("/home/big/all.csv")

hour_chat = multi.groupBy(substring("chat_time", 1, 2).alias("hour"), "chat_from")\
.count().orderBy(substring("chat_time", 1, 2), col("count").desc())

hour_chat.coalesce(1).write.foramt("csv").mode("overwrite").save("/hour_chat")
```

* ìš°ë¶„íˆ¬ì—ì„œ ì„¤ì¹˜ : `pip instal pyspark` 
* íŒŒì´ì¬ìœ¼ë¡œ pyspark íŒŒì¼ ì‹¤í–‰ :  `python hour_chat.py`
  * `spark-submit hour_chat.py` ë„ ê°€ëŠ¥
* ì˜ ì˜¬ë¼ê°”ëŠ”ì§€ í™•ì¸ : `hdfs dfs -cat /hour_chat/*.csv`

* í•˜ë‘¡ ë„ê¸° : `stop.all.sh`







<br>

* `vim multi.sh` ìƒì„± í›„ ì‘ì„± í›„ ì €ì¥!

```python
# í•˜ë‘¡ ë“± On
start-all.sh

# ëŒ€ê¸° ì‹œê°„ ì„¤ì •
sleep 20

pyspark
```

* `sudo chomd 755 multi.sh`
  * ê¶Œí•œ ì†ì„±ì„ ë³€ê²½í•´ì£¼ì.
* `./multi.sh`  ì‹¤í–‰
  * í•˜ë‘¡ì´ ì¼œì§€ê³  pysparkê¹Œì§€ ì¼œì§„ë‹¤.



<br>

<br>



## Zeppelin ì—°ê²°

* ì œí”Œë¦° ì—°ê²°

1. `sudo vim ~/.bashrc`

   ```js
   # zeppelin
   export ZEPPELIN_HOME=/home/big/zeppelin
   export PATH=$PATH:$ZEPPELIN_HOME/bin
   ```

2. `source ~/.bashrc`

3. `cd $ZEPPELIN_HOME/conf`

   * `ls`

   ![image](https://user-images.githubusercontent.com/75322297/158719468-c33d1cd0-2a4c-4434-bc98-04632ee3872b.png)

4. `cp zeppelin-env.sh.template zeppelin-env.sh`

5. `vim zeppelin-env.sh`

   * ì¢Œí‘œ 19,1

   * ```java
     export JAVA_HOME=/home/big/java
     ```

   * ì¢Œí‘œ 79,1

   * ```java
     export PARK_HOME=/home/big/spark
     ```

   * ì¢Œí‘œ 89,1

   * ```java
     export HADOOP_CONF_DIR=/home/big/hadoop/etc/hadoop
     ```

6. `cp zeppelin-site.xml.template zeppelin-site.xml`

7. `vim zeppelin-site.xml`

   * ì¢Œí‘œ 24,1 value ë¶€ë¶„(ì–´ë””ì„œë“  ì ‘ì† ê°€ëŠ¥í•˜ê²Œ)

   * ```java
     <value>0.0.0.0</value>
     ```

   * ì¢Œí‘œ 30,1 valueë¶€ë¶„

   * ```java
     <value>8082</value>
     ```

8. `cd`

9. `start-all.sh`

10. `zeppelin-daemon.sh start`

    ![image](https://user-images.githubusercontent.com/75322297/158720682-266266be-e397-4a39-995e-0271b0ab01e4.png)

11. FireFoxì—ì„œ  localhost:8082 ë¡œ ì ‘ì†

![image](https://user-images.githubusercontent.com/75322297/158720788-35034db7-e39a-4d33-aeea-b5ca0383064a.png)

12. [anonymous] - [interpreter] í´ë¦­

![image](https://user-images.githubusercontent.com/75322297/158720880-62572206-0977-46cc-a31e-d4ae9a4aaf1d.png)

13. spark ê²€ìƒ‰ í›„ Edit í´ë¦­

    ![image](https://user-images.githubusercontent.com/75322297/158721078-aeee3430-a50b-47e3-9397-6eb8b63c8fbf.png)

14. ë‚´ìš© ìˆ˜ì • í›„ SAVE

    * spark.master -> yarn
    * spark.submit.deployMode -> client
    * ![image](https://user-images.githubusercontent.com/75322297/158721211-299e24db-181a-4e55-adf6-051b1fb87172.png)

15. [Notebook] - [Create New Note] ì—ì„œ ë…¸íŠ¸ ìƒì„±

16. ë…¸íŠ¸ë¶ì— ë‚´ìš© ì‘ì„± í›„ Run

    ```python
    %pyspark
    
    test = [1,2,3,4,5]
    test_rdd = sc.parallelize(test)
    
    test_rdd.collect()
    ```

17. `zeppelin-daemon.sh stop`, `stop-all.sh`





<br>

<br>



## MySQL ì—°ê²°

### ì„¤ì¹˜

1. `sudo apt install mysql-server -y`

2. `sudo service mysql start`

3. `sudo mysql_secure_installation`

   * ë³µì¡í•œ íŒ¨ìŠ¤ì›Œë“œ í• êº¼? No
   * anonymous user ì‚­ì œí• êº¼? Yes
   * ì›ê²© ì ‘ì† í—ˆìš©? Yes
   * db ì œê±° ê°€ëŠ¥, ì ‘ê·¼ ê°€ëŠ¥ ? Yes
   * ì£¼ìš”í•œ í…Œì´ë¸” ì§€ê¸ˆ ë‹¤ì‹œ ë¦¬ë¡œë“œ? Yes

4. `cd /etc/mysql/mysql.conf.d`

5. `sudo vim mysqld.cnf`

   * ì–´ë–¤ IPë“  ì ‘ì† í—ˆìš©í•˜ê²Œ ë§Œë“¤ê¸°

   * 31, 32ë²ˆì§¸ ì¤„

   * ```java
     bind-adderss = 0.0.0.0
     mysqlx-bind-address = 0.0.0.0
     ```

6. `cd`

7. `sudo mysql -u root -p`

8. `use mysql;`

9. `alter user 'root'@'localhost' identified with mysql_native_password by '1234';`

10. `CREATE USER 'root'@'%' IDENTIFIED BY '1234';`

    * `%`ëŠ” ip 0.0.0.0 ê³¼ ê°™ë‹¤.

11. `GRANT ALL PRIVILEGES ON *.* TO 'root'@'localhost' WITH GRANT OPTION;`

    * ëª¨ë“  ê¶Œí•œì„ rootì˜ localhostì—ê²Œ ì¤Œ

12. `GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' WITH GRANT OPTION;`

    * ëª¨ë“  ê¶Œí•œì„ %í•œí…Œ ì¤Œ

13. `flush privileges;`

    * ë¶€ì—¬í•œ ê¶Œí•œ ì €ì¥

14. `exit`



### ì—°ê²°

1. Mysql ì‚¬ì´íŠ¸ë¡œ ê°€ì„œ Community Downloads -> Connector/J

![image](https://user-images.githubusercontent.com/75322297/158725947-6cf6a39d-fe28-4582-a1a8-b906fcd175a0.png)



2. Ubuntu Linux 20 ë²„ì „ ì„ íƒ í›„ ë‹¤ìš´ë¡œë“œ í´ë¦­

   ![image](https://user-images.githubusercontent.com/75322297/158726113-40012211-84c0-49b1-8cc1-96d698014587.png)

3. ë§í¬ ì£¼ì†Œ ë³µì‚¬

   ![image](https://user-images.githubusercontent.com/75322297/158726161-086aaa16-d17f-4c69-86c8-8618feb46a25.png)

4. ìš°ë¶„íˆ¬ì—ì„œ ì„¤ì¹˜

   * `wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java_8.0.28-1ubuntu20.04_all.deb`

5.  ì••ì¶• í’€ê¸°

   * `sudo dpkg -i mysql-connector-java_8.0.28-1ubuntu20.04_all.deb`

6. `cd /usr/share/java/`

   * `ls`

     ![image](https://user-images.githubusercontent.com/75322297/158726415-8ff4877f-2c30-440e-a93e-c632288cce63.png)

7. `cd $SPARK_HOME/conf`

8. `vim spark-defaults.conf`

   * ì‘ì„±

     ```java
     spark.jars   			/usr/share/java/mysql-connector-java-8.0.26.jar
     ```

9. pyspark ì¼œê¸°

   * `start-all.sh`
   * `pyspark`

10. pysparkì—ì„œ mysqlì— ì ‘ì†í•˜ê¸° ìœ„í•œ ë³€ìˆ˜ ë§Œë“¤ê¸°

    ```python
    user="root"
    password="1234"
    url="jdbc:mysql://localhost:3306/mysql"
    driver="com.mysql.cj.jdbc.Driver"
    dbtable="test"
    ```

11. DBì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°

    ```python
    test_df = spark.read.format("jdbc").option("user", user).option("password", password)\
    .option("url", url).option("driver", driver).option("dbtable", dbtable).load()
    
    # .optionsë¡œë„ ê°€ëŠ¥í•˜ë‹¤.
    # test_df = spark.read.format("jdbc").options(user=user, password=password, url=url, driver=driver, dbtable=dbtable).load()
    ```

12. ì—°ê²°ëœ DBì˜ Table í™•ì¸

    ```python
    test_df.show()
    ```

13. ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ Insertì¤€ë¹„

    ```python
    test_insert = [(3, "mysql"), (4, "zeppelin")]
    insert_df = sc.parallelize(test_insert).toDF(["id", "name"])
    insert_df.show()
    ```

14. ì§€ì •í•œ DBì˜ í…Œì´ë¸”ì— append(Insert)

    ```python
    insert_df.write.jdbc(url, dbtable, "append", properties={"driver": driver, "user": user, "password": password})
    ```

15. ì œëŒ€ë¡œ ë“¤ì–´ê°”ëŠ”ì§€ í™•ì¸

    ```python
    test_df.show()
    ```

    

<br>

<br>



## MongoDBì™€ ì—°ê²°

### ì„¤ì¹˜

* ì°¸ê³  : [https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/](https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/)

1. MongoDB í™ˆí˜ì´ì§€ [Resources] - [Documentation] - [Server] - [Installation]

   ![image](https://user-images.githubusercontent.com/75322297/158737687-d103c7db-55b4-4136-ae01-876c774e8202.png)

2. í•´ë‹¹ ë§í¬ ë³µì‚¬

   ![image](https://user-images.githubusercontent.com/75322297/158737824-26f5a8e6-f835-437a-9de6-1c204bec4513.png)

3. ìš°ë¶„íˆ¬ì—ì„œ ì„¤ì¹˜

   * `wget -qO - https://www.mongodb.org/static/pgp/server-5.0.asc | sudo apt-key add -`
   * `echo "deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/5.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-5.0.list`

4. ìš°ë¶„íˆ¬ íŒ¨í‚¤ì§€ APT ì—…ë°ì´íŠ¸ ë° ì—…ê·¸ë ˆì´ë“œ

   ```java
   sudo apt-get update
   sudo apt-get upgrade -y
   ```

5. mongodb íŒ¨í‚¤ì§€ ì„¤ì¹˜

   ```java
   sudo apt install -y mongodb-org
   ```

6. MongoDB ì‹œë™ ê±¸ê¸°

   ```java
   sudo systemctl start mongod
   ```

7. mongodb ì‹¤í–‰ : `mongo`



### pysparkë‘ ì—°ê²°

1. ë°ì´í„° ì‚½ì… í›„ í™•ì¸, í™•ì¸ë˜ë©´ ì¢…ë£Œ

   ```java
   db.test.insert({"id":"10", "name":"mongo"})
   db.test.find()
   ```

2. `vim spark-defaults.conf` ì— ë‚´ìš© ì‘ì„±

   ```python
   spark.mongodb.input.uri		mongodb://localhost/test
   spark.mongodb.output.uri	mongodb://localhost/test
   # spark.jar.packages			org.mongodb.spark:mongo-spark-connector_2.12:3.0.1
   ```

3. spark.jars.packagesì„¤ì •ì´ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•Šì•„ì„œ, ì‹¤í–‰í•  ë•Œ --packages ì˜µì…˜ìœ¼ë¡œ  dependencyëª…ì‹œ

   * `pyspark --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1`
   * jarë“¤ì„ ëª¨ë‘ ê°€ì ¸ì˜´

4. pyspark ì‹¤í–‰ : `pyspark`

5. ì˜ ì—°ê²°ë˜ì—ˆëŠ”ì§€ í…ŒìŠ¤íŠ¸

   * testë¼ëŠ” DBì—ì„œ testë¼ëŠ” collectionì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ í™•ì¸

     ```python
     test = spark.read.format("mongo").option("database", "test").option("collection", "test").load()
     test.show()
     ```

   * ë°ì´í„°í”„ë ˆì„ ë§Œë“¤ì–´ì„œ insertí•´ë³´ê¸°

     ```python
     insert_df = spark.createDataFrame([("11", "mongo-spark")],["id", "name"])
     insert_df.write.format("mongo").option("database", "test").option("collection", "test").mode("append").save()
     ```

   * insert ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸

     ```python
     test = spark.read.format("mongo").option("database", "test").option("collection", "test").load()
     test.show()
     ```

     

