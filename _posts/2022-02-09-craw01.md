---
layout: posts
comments: true
title: "파이썬을 이용한 크롤링 연습 1"
categories: Web
tag: [크롤링, crawling, python, 파이썬]


toc: true
toc_icon: "cog"
toc_sticky: true
date: 2022-02-09
last_modified_at: 2022-02-09

---



# ⛏크롤링(Crawling)

## 알고가면 좋은 것들

* JSON이란?(참고) : [https://www.json.org/json-en.html](https://www.json.org/json-en.html)

* robots.txt

  * 검색 로봇에게 사이트 및 웹페이지를 수집할 수 있도록 허용하거나 제한하는 것

  * 네이버

    * `www.naver.com/robots.txt`

    * 그러면 파일 하나가 보일 것인데 확인해보면 이러한 내용이 적혀져있다.

    * ```
      User-agent: *
      Disallow: /
      Allow : /$ 
      ```

    * `User-agent: *` : 지금 홈페이지에 접속하는 대상, `*`은 모두를 대상으로 잡음

    * `Disallow: /` : 루트부터 허용하지 않음

    * `Allow: /$` : 시작화면만 허용

  * 공공데이터포털

    * `https://www.data.go.kr/robots.txt`

    * ```
      User-agent: *
      #DaumWebMasterTool:4978f9b94e10dbe5672e5aab80f8868382a8d6b2319a391dc7904bd20ced54f9:mgC7BXBG6UtCrUlKPK7+eA==
      ```

    * 모두 다 크롤링을 해도 된다는 얘기

* BeautifulSoup4 : [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

* xml.etree : [https://docs.python.org/3/library/xml.etree.elementtree.html?highlight=xml%20etree](https://docs.python.org/3/library/xml.etree.elementtree.html?highlight=xml%20etree)

* PyPi : [https://pypi.org/](https://pypi.org/)



<br>

## 환경 설정

* 가상환경 생성
  * `conda create -n mycrawling pyhon==3.9`
* Pycharm에서 새 프로젝트 생성
* 가상환경 설정
  * ![image](https://user-images.githubusercontent.com/75322297/153098301-c8aed7e4-55df-417c-8155-66ee58657e94.png)
* 새 프로젝트 생성
* beautifulsoup4 라이브러리 설치
  * `pip install beautifulsoup4`
* beautifulsoup4
  * html, xml와 같은 문서에서 데이터를 끌어오는 파이썬 라이브러리
* 참고 사이트 : [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
* paser, parsing 이란?
  * **파싱 (Parsing : 구문분석)**
  * 어떤 문장을 트리구조로 나타낸 것을 의미한다. 
* 크롤링 : 다 긁어모아오는것
* 스크래핑 : 크롤링으로 긁어온 것 중 필요한 것만 가져오는 것





<br>

​	

## 네이버 영화에서 현재 상영작의 영화제목과 별점 크롤링하기

* 프로젝트에 naver 폴더 생성 후 movies.py 생성

  * ![image](https://user-images.githubusercontent.com/75322297/153100226-38346883-6624-481f-a151-08b3226e1c53.png)

* movies.py에 내용 작성

  * 모듈 호출

    * ```python
      from bs4 import BeautifulSoup
      import urllib.request
      ```

  * ```python
    # 해당 url에 요청 
    # resp는 텍스트로 이루어져있다.
    resp = urllib.request.urlopen('https://movie.naver.com/movie/running/current.naver#')
    print(resp)
    ```

  * 실행 해보면 응답한 객체가 반환된다. 

    * ```
      <http.client.HTTPResponse object at 0x0000018EABB4CBE0>
      ```

  * ```python
    # 응답된 문서를 파이썬의 내장 paser를 이용해 pase tree를 생성
    soup = BeautifulSoup(resp, 'html.parser')
    print(soup)
    ```



* 개발자도구를 사용해 영화 제목을 보면 `<a>`태그 안에 내용이 작성되어 있다.

* 이것들은 클래스가 "lst_dsc"인 `<dl>`태그로 묶여있다.

  * ![image](https://user-images.githubusercontent.com/75322297/153104294-a180d50f-edaf-4731-9c0d-90e4cc63a3a4.png)

* 클래스와 태그네임을 활용해 불러오자.

  * ```python
    movies = soup.find_all('dl', class_='lst_dsc')
    ```

* 0번지를 확인해보자

  * ```python
    print(movies[0])
    ```

  * 확인해보면 영화제목과 별점을 나타내는 태그들을 포함한 `<dl>`태그 하나의 내용을 확인할 수 있을 것이다.

* 그러면 목록에 있는 모든 영화의 제목과 별점을 가져오자.

* 제목과 별점을 갖고 있는 **태그와 그들이 갖고 있는 속성(클래스, 네임)등 을 잘 관찰해야한다.**

  * ```python
    for movie in movies:
        # 제목
        title = movie.find('a').get_text()
        # 별점
        star = movie.find('span', class_='num').text
        print(f'{title} [{star}]')
    ```

  * `.find()`는 해당 태그의 첫번째 태그만 가져온다.

  * 그 태그가 가지고있는 TextContent를 가져오려면 `.get_text()`나 `.text`를 사용해주면 된다.

* 결과

  * ```python
    나일 강의 죽음 [8.00]
    킹메이커 [7.47]
    355 [8.56]
    해적: 도깨비 깃발 [6.24]
    극장판 안녕 자두야: 제주도의 비밀 [8.98]
    미싱타는 여자들 [9.25]
    스파이더맨: 노 웨이 홈 [8.87]
    만년이 지나도 변하지 않는 게 있어 [8.00]
    씽2게더 [9.43]
    가슴이 떨리는 건 너 때문 [10.00]
    듄 [7.96]
    해탄적일천 [8.23]
    어나더 라운드 [8.25]
    효자 [9.51]
    드라이브 마이 카 [8.56]
    경관의 피 [7.38]
    하우스 오브 구찌 [7.93]
    애니멀 체인지 [0.00]
    해리 포터와 불사조 기사단 [7.03]
    프리! 더 파이널 스트로크 전편 [9.26]
    .
    .
    .
    
    ```





<br>



## 네이버 웹툰 중 수요웹툰의 웹툰명과 별점 가져오기

* requests 설치

  * `pip install requests`

* naver 폴더 안에 webtoons.py 생성

  * `# -*- coding:utf-8 -*-`

    * 이렇게 적어놓으면 적어놓은 .py파일에 utf-8로 인코딩된다.

  * 모듈 호출

    * ```python
      from bs4 import BeautifulSoup
      import requests
      import json
      ```

  * 내용

    * ```python
      # 수요웹툰 주소를 url 저장
      url = 'https://comic.naver.com/webtoon/weekdayList?week=wed'
      # url을 get방식으로 요청해 응답 받은 객체를 resp에 저장
      resp = requests.get(url)
      ```

    * ```python
      # 응답된 문서를 파이썬의 내장 paser를 이용해 pase tree를 생성
      soup = BeautifulSoup(resp.text, 'html.parser')
      ```

* 페이지에서 개발자도구를 사용해보면 `class`가 `img_list`인 `<ul>`로 묶여있는 것을 확인할 수 있다.

  * ![image](https://user-images.githubusercontent.com/75322297/153109507-adb7c4f7-58ac-479e-9b04-f3c2ed7d150b.png)

  * ```python
    # class='img_list'인 <ul>을 찾아서 webtoons에 저장
    webtoons = soup.find('ul', {'class': 'img_list'})
    ```

* 그 다음 제목과 별점이 담긴 `<dl>`태그를 가져와야한다.

  * ![image](https://user-images.githubusercontent.com/75322297/153110781-baeddcb7-fc33-4178-b163-d41f5cbe406c.png)

  * ```python
    # .select는 파이썬에서 사용하는 css선택자
    dl_list = webtoons.select('dl')
    ```

  * 제목은 `<dl>`태그 하위의 `<dt>`태그 하위의 `<a>`태그의 'title'속성에 적혀있다.

  * ![image](https://user-images.githubusercontent.com/75322297/153111228-59a9a8a7-e471-4797-9bce-86f48bca02bd.png)

  * 별점은 `<dl>`태그 안에 `<strong>`태그가 갖고 있는 텍스트 콘텐츠에 있다.

  * ![image](https://user-images.githubusercontent.com/75322297/153111637-38d873fe-79e3-47a9-905e-18f28a336470.png)

* 반복문을 사용해 값 가져오기

  * ```python
    lst = list()
    for dl in dl_list:
        # 각 <dl>안의 <a>태그를 찾아 <a>태그의 title 속성값을 가져와 title에 저장
        title = dl.find('a')['title']
        # 각 <dl>안의 <strong>태그를 찾아 텍스트값을 가져와 star에 저장
        star = dl.find('strong').text
    
        # tmp란 딕셔너리자료 생성
        tmp = dict()
        # {'title': title}
        tmp['title'] = title
        # {'star': star}
        tmp['star'] = star
    
        # tmp는 {'title': '쇼미더럭키짱!', 'star': '9.28'} 이런 형태를 갖고 있을 것이다.
        
        # lst에 append 해준다.
        lst.append(tmp)
    
    print(lst)
    ```

  * print(lst)의 출력 결과는 이렇다.

  * ```
    [{'title': '쇼미더럭키짱!', 'star': '9.28'}, {'title': '전지적 독자 시점', 'star': '9.97'}, {'title': '화산귀환', 'star': '9.96'}, {'title': '헬퍼 2 : 킬베로스', 'star': '8.93'}, ...]
    ```

* 크롤링한 데이터들을 json 파일로 저장해보자.

  * ```python
    res = dict()
    res['webtoons'] = lst
    ```

    * res라는 딕셔너리를 만들어주고, key값을 'webtoons', value 값으로 lst로 넣어준다.

  * `print(res)`로 확인해보면 다음과 같이 출력된다.

    * ```
      {'webtoons': [{'title': '쇼미더럭키짱!', 'star': '9.28'}, {'title': '전지적 독자 시점', 'star': '9.97'}, {'title': '화산귀환', 'star': '9.96'}, ... ,...]}
      ```

  * json 형식으로 변환해보자.

    * `.dumps()` 메소드는 python의 객체를 JSON 문자열로 반환해주는 메소드다.

    * ```python
      res_json = json.dumps(res, ensure_ascii=False)
      ```

    * `enusure_ascii=False` 속성은 한글 문자 같은 것들이 아스키코드로 인코딩 되는 것을 방지하는 역할을 한다.

    * 만약 False로 값을 두지 않을 때 출력하면 이렇게 나오게 된다.

    * ```
      {"webtoons": [{"title": "\uc1fc\ubbf8\ub354\ub7ed\ud0a4\uc9f1!", "star": "9.28"}, {"title": "\uc804\uc9c0\uc801 \ub3c5\uc790 \uc2dc\uc810", "star": "9.97"}, ... ]}
      ```

    * 속성을 False로 설정하고 print(res_json) 해보자.

    * ```
      {"webtoons": [{"title": "쇼미더럭키짱!", "star": "9.28"}, {"title": "전지적 독자 시점", "star": "9.97"}, {"title": "화산귀환", "star": "9.96"}, ..]}
      ```

    * 아까 res를 출력했을 때 처럼 똑같이 나온다.

  * 이제 json 파일을 만들 수 있다.

    * ```py
      with open('webtoons.json', 'w', encoding='utf-8') as f:
          f.write(res_json)
      ```

    * webtoons란 json 파일을 생성하고 인코딩은 utf-8로 설정한다. 

    * 그리고 내용은 `res_json`을 적어준다.

  * 확인

    * ![image](https://user-images.githubusercontent.com/75322297/153116692-480df87f-3f53-4cf6-ac87-a361f09f0e83.png)







<br>



## OpenApi 사용 (공공데이터포털)

* openApi란?

  * 원하는 형태로 요청하면 가지고 있는 데이터를 응답해주는 시스템

* 공공데이터포털 사이트를 이용해 크롤링 연습을 해보자.

  



### 제목 긁어오기

* 우선 교육을 검색해 파일데이터로 간다.

* 그리고 5페이지로 가본다.

* 주소를 분석해보자.

  * ```
    https://www.data.go.kr/tcs/dss/selectDataSetList.do?dType=FILE&keyword=%EA%B5%90%EC%9C%A1
    &detailKeyword=
    &publicDataPk=
    &recmSe=N&detailText=
    &relatedKeyword=
    &commaNotInData=
    &commaAndData=
    &commaOrData=
    &must_not=
    &tabId=
    &dataSetCoreTf=
    &coreDataNm=
    &sort=
    &relRadio=
    &orgFullName=
    &orgFilter=
    &org=
    &orgSearch=
    &currentPage=5
    &perPage=10
    &brm=&instt=
    &svcType=
    &kwrdArray=
    &extsn=
    &coreDataNmArray=
    &pblonsipScopeCode=
    ```

* 뒤에 값들이 없는 것은 사실상 지워줘도 된다 그러면

  * ```
    https://www.data.go.kr/tcs/dss/selectDataSetList.do?dType=FILE&keyword=%EA%B5%90%EC%9C%A1
    &currentPage=5
    ```

* 이렇게만 남길 수 있다.

* 이제 우리는 목록에 존재하는 제목들을 가져올 것이다.

* openapi 폴더 생성

* openapi 폴더에 crawling_loop.py 생성

  * ![image](https://user-images.githubusercontent.com/75322297/153121214-9fad2675-6d25-4e5d-8a8b-b274849d48aa.png)

  * 모듈 호출

    * ```python
      from bs4 import BeautifulSoup
      import requests
      ```

  * 내용

    * ```python
      url='https://www.data.go.kr/tcs/dss/selectDataSetList.do?dType=FILE&keyword=%EA%B5%90%EC%9C%A1&currentPage=5'
      
      resp = requests.get(url)
      # 응답받은 객체의 내용들을 html형식으로 구문분석을해서 soup에 저장
      soup = BeautifulSoup(resp.text, 'html.parser')
      ```

* 개발자도구로 어떤형식인지 확인

  * ![image](https://user-images.githubusercontent.com/75322297/153122227-204f384b-dc61-4bfe-b17e-6f2ed2925846.png)

  * 클래스가 title인 span을 가져와야한다.

  * ```python
    titles = soup.find_all('span', class_='title')
    ```

* 그리고 반복문을 사용해 title들의 텍스트 콘텐츠를 출력

  * ```python
    for title in titles:
        print(title.text.strip())
        # 공백이 너무 많아 strip()으로 공백 제거
    ```

  * ```
    한국인터넷진흥원_교육영상자막신
    한국인터넷진흥원_교육영상자막
    아시아문화원_교육프로그램
    인천광역시_교육통계
    주택도시보증공사 교육정보
    한국산업기술평가관리원 교육정보
    국가철도공단_교육정보
    한국산업은행_교육정보
    한국보건의료연구원 교육정보
    대구광역시교육청_교육통계연보
    ```

* 지금 이 과정은 5페이지의 제목들만 가져온 것인데, 1부터 10페이지 까지의 제목들을 가져와보자.

* 우선 페이지의 요소를 확인해보자

  * ![image](https://user-images.githubusercontent.com/75322297/153125020-6bd09ed2-9191-44e8-a075-a6bb31d02910.png)
  * 각 페이지들의 숫자는 class가 pagination인  `<nav>`태그 하위의 `<a>`들에 텍스트컨텐츠로 담겨져있다.

* 리스트를 만들고 a태그들을 가져와보자.

  * ```python
    lst = list()
    # class='pagination'의 하위의 a들을 모두 선택
    pages = soup.select('.pagination a')
    
    ```

* 반복문을 사용해 `처음페이지, 이전페이지, 다음페이지, 마지막페이지`를 제외시킨 값을 lst 리스트에 추가해주자.

  * ```python
    for page in pages:
        if len(page.text) <= 2:
            lst.append(page.text)
    ```

  * page는 각 `<a>`태그를 뜻한다. 그 `<a>`태그들의 텍스트컨텐츠의 길이가 2와 같거나 작다면 lst에 텍스트컨텐츠를 추가해준다.

  * **여기서 `len()`을 사용하지 않고 `.isdigit()`을 사용하면 페이지 수가 100과 같이 세 자리 수가 넘어가도 사용할 수 있을 것 같다.**

* 그러면 lst에는 다음과 같이 값이 저장이 되있을 것이다.

  * ```
    ['1', '2', '3', '4', '6', '7', '8', '9', '10']
    ```

  * 페이지 값들을 구한 것이다.

* 그러면 이제 url 주소의 currentPage 속성을 바꿔 1~10 페이지의 제목들을 출력해보자.

  * ```python
    for i in lst:
        # f-string을 사용해 페이지를 바꿔줌
        url = f'https://www.data.go.kr/tcs/dss/selectDataSetList.do?dType=FILE&keyword=%EA%B5%90%EC%9C%A1&currentPage={i}'
        resp = requests.get(url)
        soup = BeautifulSoup(resp.text, 'html.parser')
        titles = soup.find_all('span', class_='title')
        for title in titles:
            print(title.text.strip())
    ```

  * lst안의 요소들을 반복한다. 1,2,3,4,5..,10 이렇게 담겨져 있기 때문에 i값을 currentPage={i}에 담아준다.

  * 그러면 위에서 제목을 끌어왔던 반복문을 똑같이 적어주면, 1부터 10페이지까지 주소를 변경해 각 페이지의  제목들을 끌어온다.







### 코로나 데이터

* 공공 데이터 포털의 **'공공데이터활용지원센터_보건복지부 코로나19 시·도발생 현황'** 데이터를 활용할 것이다.

* 미리 데이터를 사용할 것이라고 신청해놓고 승인 받으면 데이터를 사용할 수 있다.

* **EndPoint**는 **root**라고 생각해주면 된다.

* **서비스URL** 이 **URL** 주소다.

* **일반 인증키[Encoding]** 가 **service_key** 다.

* 요청 변수와 출력결과는 [여기서 확인](https://www.data.go.kr/tcs/dss/selectApiDataDetailView.do?publicDataPk=15043378)

* openapi 폴더에 covid.py 생성 후 내용 작성

  * 모듈 호출

    * ```python
      from xml.etree import ElementTree
      import requests
      ```

    * `xml.etree`는 파이썬의 내장함수

    * 참고 : [https://docs.python.org/3/library/xml.etree.elementtree.html?highlight=xml%20etree](https://docs.python.org/3/library/xml.etree.elementtree.html?highlight=xml%20etree)

  * 내용

    * ```python
      service_key = '신청해 받은 일반 인증키(Encoding)를 붙여주세요~~'
      url = f'http://openapi.data.go.kr/openapi/service/rest/Covid19/getCovid19SidoInfStateJson?ServiceKey={service_key}'
      ```

    * url에 **서비스URL**을 붙여넣고 요청변수의 서비스키를 추가해준다. 요청변수를 추가해줄때는 `?`을 붙여줘야한다.

    * 서비스키 : `SerivceKey={}`

  * print해서 실행 하면 링크가 하나 뜬다. 눌러보면 xml문서를 확인할 수 있다.

    * ![image](https://user-images.githubusercontent.com/75322297/153129511-c4987dfb-eb8e-4938-88b2-5a7ab59c3d3c.png)

  * 여기서 오늘 총 확진자를 구해줄 것이다.

    * ```
      <item>
      <createDt>2022-02-09 10:38:16.257</createDt>
      <deathCnt>6943</deathCnt>
      <defCnt>1131239</defCnt>
      <gubun>합계</gubun>
      <gubunCn>合计</gubunCn>
      <gubunEn>Total</gubunEn>
      <incDec>49567</incDec>
      <isolClearCnt>719627</isolClearCnt>
      <localOccCnt>49402</localOccCnt>
      <overFlowCnt>165</overFlowCnt>
      <qurRate>2191</qurRate>
      <seq>15001</seq>
      <stdDay>2022년 02월 09일 00시</stdDay>
      <updateDt>null</updateDt>
      </item>
      ```

* xml형태의 문서를 문자열로 응답받아보자.

  * ```python
    resp = requests.get(url)
    ```

* xml형태의 문자열을 가지고 parse tree를 만들어 준다.

  * ```python
    tree = ElementTree.fromstring(resp.text)
    ```

* 반복문을사용해 `<gubun>`태그들의 텍스트를 가져올 수 있다.

  * ```python
    for item in tree[1][0]:
        print(item.find('gubun').text)
    ```

  * ```
    검역
    제주
    경남
    경북
    전남
    전북
    충남
    충북
    강원
    경기
    세종
    울산
    대전
    광주
    인천
    대구
    부산
    서울
    합계
    ```

  * `tree[1][0]`은 `body > items`를 가리킨다.(tree가 `<response>`를 뜻한다.)

* 이제 오늘의 확진자 합계수(국내,해외 포함)을 출력해보자.

  * 모듈 호출

    * ```python
      import re 
      ```

  * 내용

    * ```python
      for item in tree[1][0]:
          if item.find('gubun').text == '합계':
              stdDay = re.sub(r'(\D)+', '', item.find('stdDay').text)
              stdDay = stdDay[2:4] + "/" + stdDay[4:6] + "/" + stdDay[6:8]
              incDec = item.find('incDec').text
              localOccCnt = item.find('localOccCnt').text
              overflowCnt = item.find('overFlowCnt').text
              print(f'[{stdDay}]')
              print(f"일일합계:{incDec}")
              print(f"국내발생:{localOccCnt}")
              print(f"해외발생:{overflowCnt}")
      ```

  * 출력 결과

    * ```
      [22/02/09]
      일일합계:49567
      국내발생:49402
      해외발생:165
      ```





<br>





## 인스타그램에서 크롤링하기

### 실습 1

* 인스타그램의 검색창에서 원하는 태그를 검색해 이미지들의 주소(src)를 크롤링 해보자.

* `https://www.instagram.com/explore/tags/검색할태그` 주소를 이용한다.

* 검색할 태그에 python이라고 적어놨고, 이미지가 담겨있는 `<div>`태그를 확인하니 `class='KL4Bh'`였다.

  * ![image](https://user-images.githubusercontent.com/75322297/153173132-b80151d4-7373-4325-aadf-c41e30228e7d.png)

* 프로젝트에 insta라는 폴더를 생성하고 insta01.py라는 파일을 생성했다.

* `insta01.py` 생성

   * 모듈

     * ```python
       from bs4 import BeautifulSoup
       import requests
       ```

   * 내용

     * ```python
       tag = input('search tags: ')
       url = f'https://www.instagram.com/explore/tags/{tag}'
       resp = requests.get(url)
       soup = BeautifulSoup(resp.text, 'html.parser')
       
       print(soup.find('div', class_='KL4Bh'))
       ```

     * 검색할 내용을 tag에 입력받아준다.

     * f-string을 이용해 입력받은 값을 경로에 추가해서 url에 받아준다.

     * url에 대한 요청을 문자열로 응답받아서 resp에 저장해준다.

     * 응답받은 객체의 내용(`.text`)을 `'html.parser'`를 이용해 html문서형태로 분석해준다.

     * soup에서 `class='KL4Bh'`인 `<div>`태그들을 가져와서 출력한다.

* 실행해보면, None값이 나올 것이다. 왜 아무것도 가져오지 못할까?

* 인스타그램의 페이지는 클라이언트에서 서버에 요청할 때, 서버에서는 정해진 형태만 **렌더링**하고 응답하는 중간에서 클라이언트에서 요청한 값을 **CSR** 하는 시스템 같다.

* 그래서 실질적으로 가져오는 것이 원하는 데이터들이 아닌, 빈 껍데기이기 때문에 아무 값도 얻을 수 없는 것이라고 추측된다.

* **이럴 경우 Selenium을 활용하면 렌더링이 완료된 상태에서 코드를 가져오면 문제를 해결할 수 있다.**





### 실습 2

#### Selenium

* 브라우저를 자동화
* 브라우저 열기/닫기, 클릭,입력 등의 작업을 코딩을 통해 자동으로 실행하게끔 만들어준다.
* 공식사이트 : [https://www.selenium.dev/](https://www.selenium.dev/)

##### 설치

* google에 webdriver 검색
* ChromeDriver 접속
* 크롬 버전확인
  * ![image](https://user-images.githubusercontent.com/75322297/153140294-f42b44d4-ce9e-4630-bd89-58bbe0ab4974.png)
  * ![image](https://user-images.githubusercontent.com/75322297/153140511-8f7050c0-73bf-4bd9-ae37-62d8b2e3402f.png)
* 다운로드(구글버전확인 -> 운영체제확인)
  * ![image](https://user-images.githubusercontent.com/75322297/153140645-84b2faf9-a6ba-48a9-8ad0-2f75a7b3b719.png)
  * ![image](https://user-images.githubusercontent.com/75322297/153140792-fe1c3a03-b2d3-45df-8df7-e913595eddd0.png)
* 받은 파일 압축 풀기
* 프로젝트에 drivers라는 폴더 생성 후 압축 풀은 파일 복붙

  * ![image](https://user-images.githubusercontent.com/75322297/153141156-d0de271c-b3dc-4d09-a366-ebc73ee3cd6e.png)
* 터미널로 이동 후 셀레니움 설치

  * `pip install selenium`



* `insta02.py` 생성

  * 모듈 호출

    * ```python
      from selenium import webdriver
      from bs4 import BeautifulSoup
      import requests
      ```

  * 내용

    * ```python
      tag = input('search tags: ')
      url = f'https://www.instagram.com/explore/tags/{tag}'
      ```

    * ```python
      # 다운 받은 webdriver를 가져와
      service = webdriver.chrome.service.Service('../drivers/chromedriver.exe')
      # 경로를 설정해주고, 브라우저를 자동으로 실행하는 명령을 driver에 저장해준다.
      driver = webdriver.Chrome(service=service)
      
      # 3초 기다렸다가 url 가져오기
      driver.implicitly_wait(3) 
      driver.get(url)
      
      # 현재 렌더링된 화면의 코드를 가져와서 html구문으로 분석
      soup = BeautifulSoup(driver.page_source, 'html.parser')
      
      # class가 KL4Bh인 div태그들을 찾아서 img_list에 저장
      img_list = soup.find_all('div', class_='KL4Bh')
      
      for img in img_list:
          print(img)
      ```

* 지금 결과를 적고 싶었지만, 인스타그램에서 접속에 제한을 걸어놓아서 데이터는 가져오지 못했다.

* **실습1과 달리 차이가 있는 점은 셀레니움에서는 렌더링이 완료된 코드를 가져오는 `.page_source`를 사용할 수 있기 때문이다.**







### 실습 3

* 이번엔 셀레니움을 이용해 id, password를 자동으로 적고 로그인까지 해주는 실습을 해보자.

* `insta03.py` 생성

  * 모듈 호출

    * ```python
      from selenium import webdriver
      from selenium.webdriver.common.by import By
      from time import sleep
      ```

* 

* 인스타그램 로그인 창에서 속성 확인

  * ID적는 부분
    * ![image](https://user-images.githubusercontent.com/75322297/153145465-458e7c0d-4d51-4323-b4cf-d841b4a3b7e7.png)
    * name = "username"
  * PASSWORD적는 부분
    * ![image](https://user-images.githubusercontent.com/75322297/153146339-8ab925bb-baa6-4313-b71c-822f7f5e7948.png)
    * name = "password"
  * 로그인 버튼
    * ![image](https://user-images.githubusercontent.com/75322297/153167233-598844ff-737a-440c-97c4-3f0700717af7.png)
    * `id=loginForm`인 `<form>`태그 하위 `<div>`태그의 하위 3번째 `<div>`태그
    * `#loginForm > div > div:nth-child(3)`



#### ID와 PASSWORD 값 받기

* `input()`을 사용해 아이디와 비밀번호 값을 입력받자.

  * ```python
    input_id = input('id 입력 : ')
    input_pw = input('pw 입력 : ')
    ```

#### 로그인 페이지 띄우기

* 인스타그램 로그인 페이지에서 가서 주소를 복사한뒤 자동으로 로그인화면이 띄워지게 만들자.

  * ```python
    service = webdriver.chrome.service.Service('../drivers/chromedriver.exe')
    driver = webdriver.Chrome(service=service)
    driver.get('https://www.instagram.com/accounts/login/')
    sleep(5) # 5초간 대기
    ```

  * 화면을 띄우고 아이디와 비밀번호가 입력되야 하는데 로딩이 늦어질 수도 있으므로 sleep()을 사용해준다.

  * 실습 2에서는 `driver.implicitly_wait() `을 사용했지만, 실습 3에서는 내장 모듈인 `time`의 `sleep()`함수를 이용

#### ID 자동입력 

* ```python
  id = driver.find_element(By.NAME, "username")
  id.send_keys(input_id)
  ```

  * BeautifulSoup를 사용할 때는 `find()`, `find_all()`, `select()`를 사용하여 요소들을 선택한다.('태그명')
  * 셀레니움은 `find_element()`를 사용해서 요소들을 가져온다. `By`함수를 사용하면 `name`, `class`, `id` 등의 속성들을 선택할 수 있다. 
  * `.send_keys()`는 선택한 요소에 값을 보내는 메서드다.
  * 결국,  input_id에서 받은 입력값을 name이 'username'인 부분에 전달해주는 것이다.



#### PASSWORD 자동입력

* ```python
  password = driver.find_element(By.NAME, "password")
  password.send_keys(input_pw)
  sleep(2)
  ```

  * input_pw에서 받은 입력값을 name이 'password'인 부분에 전달해주는 것이다.
  * 전달해주고 2초간 대기시킨다.



#### 로그인 버튼 자동 클릭

* ```python
  driver.find_element(By.CSS_SELECTOR, "#loginForm > div > div:nth-child(3)").click()
  ```

  * `By.CSS_SELECTOR`는 CSS의 선택자를 사용해서 요소를 선택한다는 것이다.
  * `id`가 'loginForm'인 태그에서 하위 `<div>`태그를 선택하고 그 하위에서 세 번째 `<div>`태그를 선택해 클릭해주는 것이다.(로그인 버튼)



#### 알림설정 나중에 하기

* 로그인을 성공하면 알림 설정을 유도하는 알림창이 뜬다.

  * ![image](https://user-images.githubusercontent.com/75322297/153156173-dad9bc03-a161-4595-909c-464ea04c0537.png)

* 여기서 나중에하기를 자동으로 누르게 만들어보자.

* ```python
  later = driver.find_element(By.XPATH, '/html/body/div[6]/div/div/div/div[3]/button[2]')
  later.click()
  ```

  * 여기서는 `XPATH`를 사용해 요소를 찾아주고 있다.
  * XPATH는 구문분석(parsing)이 완료된 tree에서 요소를 찾을 때 처럼 경로를 사용한다.
  * ![image](https://user-images.githubusercontent.com/75322297/153156345-44855c22-c554-4f17-90f1-d1b81025490a.png)
  * 개발자 도구 콘솔로 나중에 하기를 눌러서 요소에 마우스 우클릭을 해준다.
  * [Copy] - [Copy full XPath]를 눌러준다.
  * 그러면 나중에 하기 버튼이 있는 `div`가 선택되는 경로를 사용할 수 있다.

* 실행해보면 나중에 하기가 자동으로 눌릴 것이다.

