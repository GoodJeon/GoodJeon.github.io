---
layout: posts
comments: true
title: "[Spark]하둡과 스파크 개념/설치"
categories: Spark
tag: [Spark, 스파크, Hadoop, 하둡, Pyspark]


toc: true
toc_icon: "cog"
toc_sticky: true
date: 2022-03-14
last_modified_at: 2022-03-14

---



# Hadoop🐘

* 공식 홈페이지 : [https://hadoop.apache.org/](https://hadoop.apache.org/)

## 개념

* 개발한 분의 딸의 애착인형 이름이 **하둡**

* 대용랑 데이터를 분산 저장 및 처리할 수 있는 **자바 기반의 오픈소스 프레임워크**
* 구글이 논문으로 발표한 Google FileSystem 및 MapReduce 구현
* 여러 대의 서버에 데이터를 분산 저장
* 각 서버에 분산되어 있는 데이터를 동시 처리
* 데이터를 복제하여 저장(데이터 유실 시 복구 용이)



<br>



## 모듈

* Commons :  다른 모듈을 연결 및 지원하는 **기본** 모듈
* HDFS : 대용량 데이터 분산 파일 시스템
* MapReduce : 데이터셋 병렬 처리
* Yarn : 작업 예약 및 리소스 관리



### (1) HDFS

* Hadoop Distributed File System

* 구글파일시스템을 기반으로 만든 대용량 분산 저장 / 처리 파일 시스템

* NameNode와 DataNode를 가지는 **Master - Slave 구조**

  * Block 구조 파일 시스템
  * 노드
    * NameNode 
      * 메타 데이터 관리
      * 블록 관리
    * Secondary NameNode
      * 체크포인트 노드(fsimage + edit)
      * 네임스페이스 동기화
    * DataNode
      * 데이터 저장

  ![image](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png)

* 하둡의 가정 및 목표

  1. 장애 복구
  2. 스트리밍 방식의 데이터 버근
  3. 대용량 데이터 저장
  4. 데이터 무결성
  5. 이기종 하드웨어 및 소프트웨어 플랫폼 간의 이식성





### (2) Map Reduce

* 참고 : [https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html](https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)

* 대량의 데이터를 병렬로 분석해서 분산 처리 지원하는 소프트웨어 프레임워크
* 함수형 프로그래밍 + 분산 컴퓨팅
* 데이터 전송, 분산 병렬 처리 등은 MapReduce 프레임워크가 자동으로 처리해서 개발자는 MapReduce 알고리즘에 맞게 분석프로그램을 개발하면된다.
* Map -> Reducer(Shuffle, Sort, Reduce)
  * Map(Data Transformation) : 입력한 데이터를 split하고 key / value로 해석한다. 그리고 레코드를 map이 받아서 처리한다.
  * Shuffle : Map Task에서 처리된 데이터를 섞는다.
  * Sort : 정렬하고 Reducer로 전달
  * Reduce(Data Aggregation) : 처리되어 전달된 결과를 집계한다.
* MapReduce Framework
  * JobClient : Hadoop MapReduce API
  * JobTracker : JobScheduling 및 TaskTracker 에 Job 할당(일반적으로 NameNode 실행)
  * TaskTracker
    * 요청된 job을 받아 맵리듀스(task) 실행
    * JobTracker 에게 HeartBeat 전송
    * DataNode에서 실행
* Hadoop 1.x MapReduce의 문제점
  * JobTracker가 스케줄링, 태스크 관리 기능을 수행 -> 작업 관리와 자원 분배의 비효율성
  * JobTracker 미 실행시 TaskTracker 실행해도 MapReduce 불가
  * MapReduce API 로 개발된 애플리케이션만 실행 가능





### (3) Yarn

![image](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif)

* 참고 : [https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html)
* **Y**et **A**noter **R**esource **N**egotator 의 약자
* 리소스 관리 및 작업 예약하고 모니터링한다.
* JobTracker를 분리
  * ResourceManager : Yarn Application 시작 / DataNode 리소스 할당
  * ApplicationMater : Task Scheduling 및 실행 관리
  * JobHistoryServer : 모든 Job 에 대한 metadata 관리
* TaskTracker의 단일 계산 리소스 관리 부분은 NodeManager가 담당한다.
  * NodeManager : Container 단위로 단일 서버 리소스 관리





<br>



## 환경설정/준비(설치, 기타)

❗**우분투 리눅스가 깔려있어야합니다.**❗

* 우선 우분투 가상컴퓨터에서

  * `sudo apt install openssh-server ssh-askpass -y`
  * `ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa`
    * 네임노드와 데이터노드가 서로 접속할 때 시큐어 셀이 필요하지 않게 자동으로 키를 만드는 과정
  * `cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys`
    * 만들어진 키를 authorized_keys에 넣어줌

* java 개발자용 다운해야함

  * OpenJDK(java development kit) vs Amazon Corretto

  * Amazon Corretto를 다운받을 것이다.

    [https://aws.amazon.com/ko/corretto/?filtered-posts.sort-by=item.additionalFields.createdDate&filtered-posts.sort-order=desc](https://aws.amazon.com/ko/corretto/?filtered-posts.sort-by=item.additionalFields.createdDate&filtered-posts.sort-order=desc)

  * 11version 다운 클릭

    ![image](https://user-images.githubusercontent.com/75322297/157816644-9fc43494-b824-4688-a6b8-a733e6a41756.png)

  * 해당 내용을 우분투에 복사 후 입력

    ![image](https://user-images.githubusercontent.com/75322297/157816781-364c4151-621e-4443-8cd8-902b47249e8f.png)

  * amazon-corretto가 우분투 안에 압축파일로 있을 것이다.

    ![image](https://user-images.githubusercontent.com/75322297/157818653-b8b3aa41-1cf5-4f13-96f9-3b0c908fb7d9.png)

  * 압축 풀기

    * `tar xvzf amazon-corretto-11-x64-linux-jdk.tar.gz` 입력

  * java라는 바로가기 만들어주기

    * `ln -s amazon-corretto-11.0.14.10.1-linux-64/ java` 입력

  * `ls -al` 로 숨겨진 파일을 확인하고, 숨겨져있던  `.bashrc` 파일의 내용을 vim 에디터로 수정하자.

    * `.bahsrc`는 터미널이 실행될 때 가장 먼저 읽혀지는 파일
    * `sudo vim ~/.bashrc` 
    * 맨 밑으로 내려간다.
    * 단축키 o 입력 해서  다음줄로 넘어가며 insert mode로 전환
    * ![image](https://user-images.githubusercontent.com/75322297/157819721-ab2a4668-2af3-4478-940b-228e7345d375.png)
    * `export JAVA_HOME=/home/계정명/java` 입력
    * `export PATH=$PATH:$JAVA_HOME/bin` 입력
    * esc 단축키 누른 후 `:wq!` 입력

  * vim 에디터에서 나와서 `source ~/.bashrc` 입력

    * `source`는 변경사항을 지금 당장 적용시켜달라는 명령

  * java버전과 javac버전 확인

    * `java -version`
    * `javac -version`
    * ![image](https://user-images.githubusercontent.com/75322297/157822775-131484ca-28b3-4ad4-9c1e-998877d26b39.png)

* 파이썬도 설치해야한다.(**3.7**이하 버전)

  * 파이썬 3.8버전이 기존에 존재하지만 **제플린과 호환이 안되기 때문에** 따로 설치해줘야한다.

    ![image](https://user-images.githubusercontent.com/75322297/157825047-436d538b-7bd8-4424-9d50-ea05a98b7ffc.png)

  * `sudo add-apt-repository ppa:deadsnakes/ppa` 입력

    * 3.7버전 설치를 파일을 제공해주는 repo를 추가

  * `sudo apt update` 입력

    * 패키지 업데이트

  * `sudo apt install python3.7 -y`

    * 파이썬 3.7버전 설치

  * `sudo vim ~/.bashrc` 

    ![image](https://user-images.githubusercontent.com/75322297/157824782-dac37fef-28c2-46d7-857b-e3f1815324e6.png)

    * `alias python=python3.7` 입력
    * `alias python3=python3.7` 입력
    * `:wq!`로 저장하고 vim 에디터 종료

  * `source ~/.bashrc` 로 변경사항 적용

  * `python --version`, `python3 --version`으로 버전 확인

    ![image](https://user-images.githubusercontent.com/75322297/157824969-21d5af7d-89ee-4ce0-b184-61391bc20789.png)

  * `sudo apt install python3-pip -y`

    * 파이썬 내장패키지 설치

  * `.bashrc` 파일 켜서 내용 추가

    * `sudo vim ~/.bashrc`
    * `alias pip="python3.7 -m pip"`  내용 추가
    * ![image](https://user-images.githubusercontent.com/75322297/157827994-eb00f52c-74b1-4cf0-9fc4-931636e57e9a.png)
    * `:wq!`로 vim 종료

  * `source ~/.bashrc` 로 변경사항 적용

  * `pip --version`으로 버전 확인

    * ![image](https://user-images.githubusercontent.com/75322297/157828200-21b8b1ac-eb90-458c-af1c-c8947660edab.png)
    * 3.7 버전으로 잘 적용된 것을 확인

* 이제 하둡을 다운 받을 차례다.

  * 3.31버전 다운받을 것이다. 따라서 백업사이트로 가야한다.

  * [하둡백업페이지](https://downloads.apache.org/hadoop/common/hadoop-3.3.1/)

  * tar.gz파일을 찾아서 링크주소를 복사해온다.

    ![image](https://user-images.githubusercontent.com/75322297/157829062-098a6d1f-6530-484a-89df-388143323e0f.png)

  * 우분투에서 `wget https://downloads.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz` 입력 해서 압축파일 다운

  * 압축 풀기

    * `tar xvzf hadoop-3.3.1.tar.gz`

  * hadoop 이라는  바로가기 만들어주기

    * `ln -s hadoop-3.3.1 hadoop`

  * 배쉬가 실행될 때 하둡이 켜지게 환경변수 설정(PATH 등 설정)

    * `sudo vim ~/.bashrc` 실행 후 작성
    * ![image](https://user-images.githubusercontent.com/75322297/158085660-e45985ed-d9dd-4e4f-8ffe-ff8013959e55.png)

  * 이번엔 동일파일에서 하둡을 사용할 수 있는 유저 설정

    * ![image](https://user-images.githubusercontent.com/75322297/158085984-0d861d66-98b9-46f4-b0d2-9ceacd075c30.png)
    * `big`자리에 자신이 원하는 계정명을 적어주면 된다.
    * 다하고 esc 후 `:wq!`: 로 저장 후 에디터를 종료

  * 변경사항 바로 적용

    * `source ~/.bashrc`

  * 경로 바꾸기

    * `cd $HADOOP_CONF_DIR`
    * `ls`
    * ![image](https://user-images.githubusercontent.com/75322297/158086609-12dd680d-6e26-47c0-aa31-be20a8e729cb.png)

  * 건드려야하는 파일 몇 가지가 있다.

    * hadoop-env.sh
    * core-site.xml
    * hdfs-site.xml
    * mapred-site.xml

  * hadoop-env.sh

    * `vim hadoop-env.sh`
    * 54번째 줄 부터 3개
      * ![image](https://user-images.githubusercontent.com/75322297/158086782-bca4331a-3bd5-4a97-9136-63a57b74f578.png)
    * 198번째 줄 부터 1개
      * ![image](https://user-images.githubusercontent.com/75322297/158086895-b64f27ee-b5e4-424c-834d-b5ddd368d0e3.png)
    * 저장 후 에디터 종료

  * core-site.xml

    * 기본적인 파일시스템의 요청 경로를 잡아주는 곳
    * `vim core-site.xml`
    * ![image](https://user-images.githubusercontent.com/75322297/158087035-ca61b1dd-17a8-4dc7-a51c-44c17a85517e.png)
    * 저장 후 에디터 종료

  * hdfs-site.xml

    * 싱글모드
    * replication은 네임노드에서 데이터노드로 복사되는 데이터의 분할수
    * `vim hdfs-site.xml`
    * ![image](https://user-images.githubusercontent.com/75322297/158087313-3aef9812-6d22-4e50-90e0-eddb59a085bb.png)

  * mapred-site.xml

    * map reduce할 때 yarn 프레임 워크를 사용한다고 설정
    * `vim mapred-site.xml`
    * ![image](https://user-images.githubusercontent.com/75322297/158088156-3cd19b64-32e2-4141-9357-585a0a253d1a.png)

  * 설정이 끝났으면 네임/데이터 노드 포매팅 해주기

    * `hdfs namenode -format`
    * `hdfs datanode -format`

  * 프레임워크 실행

    * `start-dfs.sh`
    * `start-yarn.sh`
    * `jps`
    * ![image](https://user-images.githubusercontent.com/75322297/158090642-5c07d70b-15da-426a-935b-a3fbeab6eaf3.png)
    * 만약 다 멈추고 싶다면 `stop-all.sh`
      * `stop-dfs.sh`
      * `stop-yarn.sh`
    * 만약 start부분에서 에러가 난다면
      * `sudo apt purge openssh-server`
      * `sudo apt install openssh-server`

  * `hdfs dfsadmin -report` : 하둡에 관련 정보

  * 파이어폭스를 켜서 localhost:9870으로 가보자.

    * hdfs 관련된 페이지
    * Live Nodes가 1로 되어있어야 정상
    * ![image](https://user-images.githubusercontent.com/75322297/158095330-7b32f35e-9125-4932-90af-27e82cef0d5e.png)

  * 이번엔 localhost:8088로 가보자

    * yarn 관련 페이지
    * ![image](https://user-images.githubusercontent.com/75322297/158095415-6256ff18-75fd-4dc8-8df6-6f8d8641ea8a.png)









# Spark

## 개념

* APACHE SPARK : 빅데이터 처리를 위한 오픈소스 분석 엔진
* spark는 기본적으로 SCALA(Java Virtual Machine 기반)

![image](https://user-images.githubusercontent.com/75322297/158107941-7521c053-51d2-4c11-a0e1-f2a3fe47c043.png)

* 특징
  * 성능과 편의성 모두 고려
  * 내장메모리 방식으로 인한 빠른 속도
  * lazy-evaluation(lazy-execution)
* 구성 요소
  * Driver 
    * sparkSession 생성
    * main 호출
    * 프로그램 스케쥴링
    * 분석 및 배포
  * Cluster Manager
    * Executor(master, slave) 관리
  * Executor
    * 프로세스가 할당한 job을 수행
  * SparkSession
    * 클라이언트의 명령을 cluster(Executor)에서 실행
* 데이터 구조
  * RDD : Resilient Distributed DataSet (내결함성 분산 데이터 셋)
    * 내결함성이란? 
      * 하드웨어 장애와 같은 돌발 사태에 대비할 수 있는 컴퓨터 또는 운영체제의 기능
  * DataFrame : row와 column의 개념을 가지는 자료구조
    * DataSet[Row] (Row 타입으로 구성된 Dataset)
    * 비타입형 api - 런타임 시 데이터의 타입을 확인
  * DataSet : java와 scala만 사용 가능한 자료구조
    * 타입형 api - 컴파일 시 데이터의 타입을 확인
* Spark Core
  * Main Component(기본 기능) : 배치 단위의 RDD (Resilient Distributed Dataset) 연산 및 처리
    * **RDD란 내 결함성 분산 데이터셋으로 장애 복구에 용이하다.**





<br>



## 환경설정(설치)

* Spark 홈페이지 : [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)

1. 아파치 스파크 홈페이지의 다운로드로 가서 버전을 3.1.3 버전을 선택하고, pre-built with user-provided Apache hadoop 을 선택 후 3번의 다운로드 링크 클릭

   ![image](https://user-images.githubusercontent.com/75322297/158095586-76adf740-d416-4908-86b7-3338bc54f923.png)

2. 링크 주소 복사.

   ![image](https://user-images.githubusercontent.com/75322297/158095654-90d3e742-fb9d-4026-949a-97b1646a0794.png)

3. 우분투에서 wget을 사용해서 다운

   * `wget https://dlcdn.apache.org/spark/spark-3.1.3/spark-3.1.3-bin-without-hadoop.tgz`

4. 압축해제 후 spark라는 심볼릭 링크 만들기

   * 압축 해제
     * `tar xvzf spark-3.1.3-bin-without-hadoop-tgz`
   * spark 심볼릭 링크 만들기
     * `ln -s spark-3.1.3-bin-without-hadoop spark`

   ![image](https://user-images.githubusercontent.com/75322297/158102848-52c8fe4e-a58d-4071-82c8-6b07c04f484b.png)

5. 경로 잡기

   * `sudo vim ~/.bashrc`

     ![image](https://user-images.githubusercontent.com/75322297/158103206-260ff588-a46d-43f1-a832-0d5362b0bf08.png)

6. 바로 적용시키기

   * `source ~/.bashrc`

7. `cd $SPARK_HOME/conf` 로 경로 이동

8. 파일 복사

   * `cp workers.template workers`

   * `cp spark-env.sh.template spark-env.sh`

9. `vim spark-env.sh`

   ![image](https://user-images.githubusercontent.com/75322297/158114037-285fb2bb-0821-4dfe-8193-d62b29e331e1.png)

   1. 경로들 작성!
   2. export JAVA_HOME=/home/big/java
   3. export HADOOP_CONF_DIR=/home/big/hadoop/etc/hadoop
   4. export YARN_CONF_DIR=/home/big/hadoop/etc/hadoop
   5. export SPARK_DIST_CLASSPATH=$(/home/big/hadoop/bin/hadoop classpath)
   6. export PYSPARK_PYTHON=/usr/bin/python3.7
   7. export PYSPARK_DRIVER_PYTHON=/usr/bin/python3.7

10. `cp spark-defaults.conf.template spark-defaults.conf`

11. `vim spark-defaults.conf`

    * `spark.master							yarn`

12. `cd`  엔터하고

    * `start-dfs.sh` 
    * `start-yarn.sh`
    * `pyspark`

13. 이런 화면이 나온다면 성공⭕

    ![image](https://user-images.githubusercontent.com/75322297/158114376-7877fa0d-66dc-4f60-b405-a8fb08cbd559.png)