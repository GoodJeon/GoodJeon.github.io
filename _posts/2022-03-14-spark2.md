---
layout: posts
comments: true
title: "[Spark]스파크 복습 - 1(RDD)"
categories: Spark
tag: [Spark, 스파크, Hadoop, 하둡, Pyspark]


toc: true
toc_icon: "cog"
toc_sticky: true
date: 2022-03-14
last_modified_at: 2022-03-14


---





# Spark 1일차

* **Tips!**
  * `[Ctrl] + [L]` : clear와 똑같이 화면 청소
  * sc는 sparkContext의 약자
  * 로컬호스트 8088번(`localhost:8088`)에 접속해서 여기 클릭

    * ![image](https://user-images.githubusercontent.com/75322297/158126341-6119706e-9c69-4afe-880a-11a90e3eadc8.png)
    * 이 곳에서 내가 했던 작업들에 대한 여러가지 정보나 로그들을 yarn이 관리하고 있는 것을 확인할 수 있다.
      * ![image](https://user-images.githubusercontent.com/75322297/158126514-331a5afd-0f73-4df6-ab2a-1159a9920bb9.png)
  * zeppelin 노트북 다운받을 때는 두 번째껄로 다운!
    * ![image](https://user-images.githubusercontent.com/75322297/158134923-b701ab76-43dd-4ac4-bb16-89c49414fe87.png)



<br>



## RDD

* 참고 : [https://spark.apache.org/docs/latest/rdd-programming-guide.html](https://spark.apache.org/docs/latest/rdd-programming-guide.html)

### Transformation

* filter, map, flatmap, zip, reduceByKey, sortBy, glom 등...

### Action

* collect, first, stats, take, reduce, fold, aggregate 등..





---



## RDD 실습

### transforamtion과 action

* ```python
  rdd01 = sc.range(0, 1000, 1, 2)
  rdd01.collect()
  ```

  * 0부터 999까지 수를 갖고 있는 배열을 생성

* `.collect()`

  * 갖고 있는 내용 확인

* `.getNumPartitions()`

  * 몇 개의 파티션으로 나뉘어서 저장되어 있는지 확인

* `.take()`

  * 앞에 적어준 값 만큼 가져오는 함수
  * like pandas의 head()

* ```python
  rdd02 = rdd01.filter(lambda x:x % 2)
  rdd02.take(10)
  ```

  * 짝수가 아닌. 즉, 홀수만 필터링해서 rdd02에 저장
  * 앞의 10개 값만 가져오기
  * ![image](https://user-images.githubusercontent.com/75322297/158132020-d0f4a848-b438-43d4-bbc6-b0a380e78337.png)

* ```js
  rdd03 = rdd01.filter(lambda x: not x % 2)
  rdd03.take(10)
  ```

  * not을 사용해서 짝수만 가져오기
  * 앞의 10개 값만 가져오기
  * ![image](https://user-images.githubusercontent.com/75322297/158135372-243205b8-8e5e-4cd1-9e92-c29277f9778c.png)

* ```python
  countries = ['korea','united states america','united kingdom','japan','france','germany','italia','canada','korea']
  g8 = sc.parallelize(countries, 2)
  g8.collect()
  g8 = g8.distinct()
  g8.count()
  ```

  * countries를 파티션 2개로 나눠서 저장한 후  내용 확인
  * 중복되는 내용을 없애고 싶다면 `.distinct()`사용
  * ![image](https://user-images.githubusercontent.com/75322297/158135484-5358ff21-e4f1-4869-9bbe-900a823c902e.png)

* ```python
  g8_upper = g8.map(lambda x: x.upper())
  g8_upper.collect()
  ```

  * `map()`으로 타겟들에 `upper()`함수(대문자) 적용
  * ![image](https://user-images.githubusercontent.com/75322297/158135548-de75232a-1d8f-4831-8c78-17a1015b0e5b.png)

* ```python
  g8_list01 = g8.map(lambda x:list(x))
  g8_list01.collect()
  ```

  * 단어 하나하나가 리스트로 바뀜
  * 맵은 각각의 요소에 적용
  * ![image](https://user-images.githubusercontent.com/75322297/158135618-e855cd0d-320e-4913-8490-f0a5031a8063.png)

* ```python
  g8_list02 = g8.flatMap(lambda x: list(x))
  g8_list02.collect()
  ```

  * 플랫맵은 각각의 요소에 넣은 후 전체에 적용
  * 맵과 차이는 접시하나에 올리냐 여러개에 올리냐 차이인듯(flatMap이 한 접시)
  * ![image](https://user-images.githubusercontent.com/75322297/158135703-419851ec-94ca-444d-93ba-572958cdbe57.png)

* ```python
  counting = sc.range(1, 9, 1, 2)
  counting_g8 = counting.zip(g8)
  counting_g8.collect()
  ```

  * 1부터 9까지의 수를 g8과 zip해줌
  * ![image](https://user-images.githubusercontent.com/75322297/158135771-9a249349-e0b7-4401-9aa0-6ae4fa8c10e2.png)

* ```python
  score = [('강호동',10),('유재석',30),('강호동',30),('신동엽',70),('유재석',60)]
  score_rdd = sc.parallelize(score, 2)
  score_rdd.collect()
  score_rdd_rbk = score_rdd.reduceByKey(lambda x, y: x + y)
  score_rdd_rbk.collect()
  ```

  * 키를 기준(reduceByKey)으로 값들이 밸류를 연산해준다.
  * 여기서는 키값에 맞는 밸류들끼리 더해졌다.
  * ![image](https://user-images.githubusercontent.com/75322297/158135900-93cf11ba-1f32-44f2-b165-e9582eb66a6f.png)

* ```python
  nums = sc.parallelize([1,2,3,1,1,2,5,4],2)
  nums.sortBy(lambda x:x).collect()
  ```

  * `sortBy()`는 정렬을 도와준다.
  * ![image](https://user-images.githubusercontent.com/75322297/158135988-3d3adff8-224d-4d58-96b6-70ff5bffab38.png)

* ```python
  arrs = g8.glom()
  arrs.collect()
  ```

  * `glom()` : 파티션 별로 배열 형태로 그루핑시켜줌
  * glom은 데이터의 크기가 너무 크면 에러가 잘 나니깐 꼭 주의!
  * france부터 japan이 파티션1, korea부터 germany가 파티션 2라고 보면되겠다.
  * ![image](https://user-images.githubusercontent.com/75322297/158136034-295e0cf1-0aed-4d75-8fad-5835690d46ed.png)

* ```python
  nums.max()
  nums.min()
  nums.mean()
  nums.variance()
  nums.stdev()
  nums.stats()
  nums.countByValue()
  ```

  * `min(), max()` : 최대, 최소 값
  * `mean()` : 평균값
  * `variance()` : 분산
  * `stdev()` : 표준편차
  * `stats()` :  개수, 평균, 표준편차, 최대최소값 출력
  * `countByValue()` : 각 요소의 개수 카운트
  * ![image](https://user-images.githubusercontent.com/75322297/158136171-8ca5b51d-68f6-47c4-a882-5368888d1adc.png)

* ```python
  g8.first()
  g8.take(3)
  g8.takeOrdered(3)
  g8.top(3)
  g8.count()
  ```

  * `first()` : 가장 처음에 위치한 값
  * `take`() : 적은 만큼의 앞의 값 가져오기
  * `takeOrdered()` : 정렬해서 값 가져오기
  * `top()` : 내림차순으로 정렬해서 가져오기
  * `count()` : 개수 가져오기
  * ![image](https://user-images.githubusercontent.com/75322297/158136249-2b9f7af1-647f-4ee4-a54d-2f8347cd9474.png)

* ```python
  # 홀수데이터
  rdd02_sum = rdd02.sum()
  rdd02_sum
  # 짝수데이터
  rdd03_sum = rdd03.sum()
  rdd03_sum
  ```

  * `sum()`  : 합 구하기
  * ![image](https://user-images.githubusercontent.com/75322297/158136331-e87bd704-f210-41ec-a726-7e0237fa75a2.png)

* ```python
  rdd02_fold = rdd02.fold(0, lambda x, y:x + y)
  rdd02_fold
  ```

  * `fold(기본값, 연산)` : 데이터를 연산
    * **파티션 단위 연산**
  * 여기서는 홀수(rdd02)들이 다 더해지는 연산
  * ![image](https://user-images.githubusercontent.com/75322297/158136485-886b16c7-95b3-468d-8d2d-594bf6043dcc.png)

* ```python
  rdd02_aggr = rdd02.aggregate(0, max, lambda x, y:x + y)
  rdd02_aggr
  ```

  * ![image](https://user-images.githubusercontent.com/75322297/158136743-d921d2b5-0a5b-43e3-85a3-b81fc7a56aed.png)

* ```python
  rdd02_reduce = rdd02.reduce(lambda x, y: x + y)
  ```

  * `reduce()` : 데이터를 병렬 연산


* g8에서 가장 긴 단어와, 가장 짧은 단어를 찾기

  * ```python
    # 가장 긴 단어
    def g8Max(x, y):
        if len(x) > len(y):
            return x
        else:
            return y
    g8_max_length = g8.reduce(g8Max)
    g8_max_length
    ```

  * ![image](https://user-images.githubusercontent.com/75322297/158115452-d0d4ede2-7ccd-4c13-b310-0bfb9be0aba5.png)

  * ```python
    # 가장 짧은 단어
    g8_min_length = g8.reduce(lambda x, y: x if len(x) < len(y) else y)
    g8_min_length
    ```

  * ![image](https://user-images.githubusercontent.com/75322297/158115950-3fa59404-e6d0-4748-a29a-399629d0ca9f.png)

* ```python
  g8.saveAsTextFile("/tmp/g8")
  result = sc.textFile("/tmp/g8/part-000*")
  result.collect()
  ```

  * g8의 내용을 텍스트 파일로 저장하고 다시 불러오는 과정

  * ![image](https://user-images.githubusercontent.com/75322297/158116829-9796a0c0-2ea2-407a-94d6-d9e620041548.png)

  * 이 파일은 로컬저장소가 아닌 하둡 hdfs에 저장되어있다.

    ![image](https://user-images.githubusercontent.com/75322297/158117617-5a503a94-7639-4f56-9b21-e05eff644182.png)

  * `hdfs dfs -cat /tmp/g8/part-00000`

    ![image](https://user-images.githubusercontent.com/75322297/158117737-ab26be19-0e1f-4890-8221-e2d25dfed256.png)

  * ``hdfs dfs -cat /tmp/g8/part-00001`

    ![image](https://user-images.githubusercontent.com/75322297/158117774-98d71373-10da-46bb-94f0-5de89481352d.png)

  * 파티션 별로 저장되는 것을 눈으로 확인할 수 있다.

* hdfs(하둡의 프레임워크) 안에 폴더 만들기

  * ```js
    hdfs dfs -mkdir -p /home/big
    ```

  * 로컬 호스트 페이지(localhost:9870)에서도 확인할 수 있다.

    * [Utilities] - [Browse the file System]
    * ![image](https://user-images.githubusercontent.com/75322297/158118327-17e293db-a027-4c2b-a610-5508a0e15843.png)



* ```python
  g8 = sc.textFile("/tmp/g8/part-000*")
  g8.collect()
  key = g8.keyBy(lambda x: x[0])
  key.collect()
  key.keys().collect()
  key.values().collect()
  ```

  * `keyBy(내용)` : 첫 글짜를 짤라서 키값으로 만들어 달라는 내용
  * `keys()` : 키값들
  * `values()` : 밸류값들
  * ![image](https://user-images.githubusercontent.com/75322297/158123848-c6366d5d-b5bc-4e99-bf63-4be8e6efc4f6.png)

* ```python
  key.mapValues(lambda x: list(x)).collect()
  key.flatMapValues(lambda x: list(x)).collect()
  ```

  * `mapValues()` : values들 하나하나에 적용
  * `flatMapValues()` : 매핑한 결과를 전체에 적용하기 때문에 `키 1개 : value 1개 이런식으로 적용`
  * ![image](https://user-images.githubusercontent.com/75322297/158124158-4da7b544-035e-43fc-87c3-784b5c4b4f62.png)

* ```python
  key.groupByKey().mapValues(lambda x: list(x)).collect()
  key.groupByKey().mapValues(lambda x: len(x)).collect()
  ```

  * key를 가지고 묶은 상태로 values를 list로 저장
  * `len()`을 사용하면 키 당 만들어진 애들의 개수가 출력
  * ![image](https://user-images.githubusercontent.com/75322297/158124618-bf155702-c479-4bb2-b56c-875693edeb4b.png)

* 그렇다면 `[('g', 'germany'),... ('u', 'united states america, united kingdom')]` 형태로 나오게 하려면 어떻게 해야할까?

  * ```python
    key.reduceByKey(lambda x, y:x + ", " + y).collect()
    ```

  * ![image](https://user-images.githubusercontent.com/75322297/158125559-88abe0f1-57f6-46b6-99cb-f549774e971a.png)

* ```python
  key.countByKey()
  ```

  * key에 맞는 values의 개수가 나옴
  * ![image](https://user-images.githubusercontent.com/75322297/158125700-8b1caf8e-5b38-4999-b796-0e6ad68c7418.png)

* ```python
  g8 = sc.textFile("/tmp/g8/part-000*")
  g8_upper = g8.map(lambda x: x.upper())
  g8_upper.collect()
  ```

  * ![image](https://user-images.githubusercontent.com/75322297/158125979-71ee2365-4a63-4863-a621-a6e689adf21d.png)







<br>

### Spark를 사용해 WordCount(단어 세기)를 해보자.



* data.zip(강사님께 받은 파일)을 드래그앤 드롭으로 우분투로 보내줬다.

  ![image](https://user-images.githubusercontent.com/75322297/158118615-63f07401-3699-4512-b8cd-b0625eea65d0.png)

* 폴더를 만들고 압축파일을 풀어보자.

  * `mkdir data`
  * `unzip data.zip -d ./data`

  ![image](https://user-images.githubusercontent.com/75322297/158121710-f5ce8012-a0eb-48ff-941d-ff339d4f5260.png)

* 이 폴더를 hdfs에 올려보자.

  * `hdfs dfs -put /home/big/data /home/big/data`

  * **앞이 내가 가지고 있는 경로(ubuntu), 뒤쪽이 보낼 위치(hdfs)**

  * `hdfs dfs -ls /home/big/data` 으로 잘 보내졌는지 확인

    ![image](https://user-images.githubusercontent.com/75322297/158122600-64cad634-8a09-40ab-9af3-4af734b8264d.png)



* spark_wc라는 파이썬 파일을 만들어 밑에 내용을 작성하자.

  * `vim spark_wc.py`

  * 밑에 내용 작성하고 저장

  * ```python
    import sys, re
    from pyspark import SparkConf, SparkContext
    
    conf = SparkConf().setAppName('Word Count')
    sc = SparkContext(conf=conf)
    
    if (len(sys.argv) != 3):
        print("wordcount.py input_file output_dir 형태로 실행해 주세요")
        sys.exit(0)
    else:
        inputfile = sys.argv[1]
        outputdir = sys.argv[2]
        
    wordcount = sc.textFile(inputfile)\
               .repartition(10)\
               .filter(lambda x: len(x) > 0)\
                .flatMap(lambda x: re.split('\W+', x))\
                 .filter(lambda x: len(x) > 0)\
                  .map(lambda x:(x.lower(), 1))\
                  .reduceByKey(lambda x, y: x + y)\
                  .map(lambda x:(x[1], x[0]))\
                  .sortByKey(ascending=False)\
                  .persist()
    
    wordcount.saveAsTextFile(outputdir)
    top10 = wordcount.take(5)
    result = []
    for counts in top10:
        result.append(counts[1])
    print(result)
    
    ```

* 셰익스피어.txt(아까 받은 파일)의 내용을 result에 저장시켜 spark_wc.py 를 실행

  * `spark-submit spark_wc.py ~/data/shakespeare.txt ~/result` 

* 가장 많이 나온 5개가 출력된다.

  * ![image](https://user-images.githubusercontent.com/75322297/158129939-d75fb096-c262-4eef-8bd9-5094f3395a87.png)

* 어떤 단어가 몇번 나왔는지 알 수 있다.

  * `hdfs dfs -cat ~/result/part-00000`
  * ![image](https://user-images.githubusercontent.com/75322297/158130114-9c5a0d1b-a664-440a-9d82-9396cd39d862.png)

* 이 파이썬 파일 해석하는게 오늘 숙제!







