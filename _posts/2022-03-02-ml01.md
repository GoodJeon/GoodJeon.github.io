---
layout: posts
comments: true
title: "[ML]머신러닝 기초 및 복습"
categories: ML/DL
tag: [머신러닝, Machine Learning, ML]


toc: true
toc_icon: "cog"
toc_sticky: true
date: 2022-03-02
last_modified_at: 2022-03-02


---







# Big Data

* 4차 산업의 키워드
  * 빅데이터
  * AI
  * IoT
    * 수 많은 컴퓨터들을 연결하는 것
  * Cloud
    * 저장 공간(서버)를 구름 같은 형태로 만들어서 그 곳에 저장하는 것(개인의 컴퓨터 저장소가 아닌)
    * 종류
      * AWS
      * AZure
      * OracleCloud
      * GCP
      * NaverCloud





## 빅데이터

### 3V + 2V + 2V

* 초창기
  * 3V
    * volume - 크기
    * variety - 다양한 형태
    * velocity - 속도
* 그 다음 5V(Visualization)
  * 2V
    * veracity - 정확성
    * value - 가치
* 그 다음 7V 형태로 변화
  * 2V
    * validity - 유효성
    * volatility - 휘발성



### Architecture

* 데이터엔지니어가 해야할 일은 데이터의 전처리 과정이다.
  * 수집
  * 저장
  * 처리
    * 처리 파트의 반 정도를 차지하는 이유는 ML(Machine Learning)을 사용하기도 하기 때문(**Mahout**)
* 수집 -> 저장 -> 처리하는 형태를 파이프라인(Pipeline)을 구축하는 것



<br>

<br>

---



# AI

### 개념

* Artificial Intelligence(인공 지능)

### 목표

* System that Thinking humans
  * 사람처럼 생각하길 바람
  * 내부적으로
  * **학습**
* System that Act like humans
  * 사람처럼 행동하길 바람
  * 외부적으로 
  * **turing test**

### 발전

| 1940년대 | 인공적인 두뇌의 가능성 -> 뇌(뉴런)을 모방해 간단한 연산 모델 구성 |
| -------- | ------------------------------------------------------------ |
| 1950년대 | 황금기로 인공지능 학문이 탄생했으며 넓은 의미의 AI가 탄생했다. |
| 1970년대 | 너무너무 복잡해서 잘 안되었던 시기                           |
| 1980년대 | 비즈니스에 적용 실패                                         |
| 현재     | 제프리 힌튼 교수의 딥러닝으로 두 번째 황금기를 맞이했다. XOR 문제를 해결 |

* `XOR : x와 y의 값이 다르다면 1, 같다면 0`
  * 1과 0 : True
  * 1과 1 : False
  * 0과 0 : False
  * 0과 1 : True






### 종류

* AI > ML > DL
* ML(Machine Learning)
  * 함수를 기반으로 데이터 기반 프로그래밍
  * 컴퓨터가 스스로 학습하게 만들도록 시키는 것
* DL(Deep Learning)
  * 인간의 뉴런을 참조한 것
* 지도 학습 - X(문제), Y(답) 둘다 줌
  * 예측(Prediction - Linear[선형])
    * 연속적인 데이터
    * 수치 계산
    * 입력에 대한 예측
    * 데이터를 가장 잘 표현할 수 있는 선을 생성
  * 분류(classification - logistic[분류])
    * 불연속적인 데이터
    * 확률 계산
    * 이진 분류 - 참/거짓
    * 다중 분류 - 가/나/다/라..
* 비지도 학습 - X만 줌
  * 군집
* 강화 학습
  *  보상



<br>

<br>

---





# ML(머신러닝)

### 개념

* 기존 방식은 입력을 받고 그것을 함수에 넣어준 뒤 출력시켜주는 방식
  * `input x - > function -> output y`
* 기계 학습
  * `training data (x, y) + Learning = Model(가설)`
  * `테스트 데이터 X 투입 -> Model(가설)에 적용 -> Y출력`
  * 학습 
    * h(y) = W * x + b
    * Weight(가중치)와 Bias를 변경하는 일련의 과정



### 순서

* 데이터 준비
  * 학습에 필요한 데이터를 준비하고 전처리 까지!
  * 결측치, 이상치를 제거하거나 대체한다.(최빈값, 평균값, 중위값)
* 데이터 분할
  * Train Set / Test set 으로 분리
  * Training Sets : 모델 학습용 - > 여러 번 평가
  * Test Sets : 모델 성능평가용 -> 한 번 평가
* 준비
  * 사용할 모델을 결정하는 단계
  * 예측, 군집, 
* 학습
  * 데이터를 가지고 모델을 학습하는 단계
* 예측 및 평가
  * 모델을 검증하고 정확도를 측정하는 단계



## Loss Function

* 손실함수
* 머신러닝은 컴퓨터가 가중치(Weight)를 찾아가는 과정

* 종류
  * MSE(Mean Squared Error)
    * h(예측값)과 y(실제값)의 차이를 제곱해서 비교
  * RMSE
  * MAE(Mean Absolute Error)
    * h(예측값)과 y(실제값)의 차이를 절대값으로 비교
  * Cross-Entropy
    * q(실제분포)를 알지 못하는 상황에서 p(예측 분포)를 통해 q를 예측
  * 등



## Optimizer

* Loss Function의 결과값을 최소화 하는 모델의 인자를 찾는 알고리즘
* 경사 하강법(Gradient Descent), 관성(Momentum), 관성과 RMSProp을 합친 Adam 등이 있다.





<br>

<br>



---





# 머신러닝 지도학습

## 실습환경 준비 및 테스트

* 가상환경 생성

  * `conda create -n ai01 python=3.9`

* 파이참 프로젝트 생성 후 만든 가상환경으로 세팅

* `01_sklearn` 디렉토리 생성

  * ![image](https://user-images.githubusercontent.com/75322297/156276374-e605f2b6-14b6-4afb-bd8d-cada69acedd9.png)

* 하위에 파이썬 파일 생성

* 함수를 만들고 실행해보자

  * ```python
    def f(x):
        return list(map(lambda i: 2 * i, x))
    
    
    x = [1,2,3,4,5]
    y = f(x)
    
    print(y)
    ```

* `matplotlib 설치`

  * `pip install matplotlib`

* 모듈 호출

  * ```python
    import matplotlib.pyplot as plt
    ```

* 화면에 출력

  * ```python
    plt.plot(x,y)
    plt.show()
    ```

  * ![image](https://user-images.githubusercontent.com/75322297/156276787-3b48424d-4212-4b68-b7f7-a997e7aec0d2.png)



* 머신러닝 실습할 파이썬 파일 생성

* 머신러닝 시 사용하는 라이브러리

  * scikit learn
  * tensorflow
  * pytorch

* scikit learn 라이브러리 설치

  * `pip install sklearn`
  * scikit-learn 공식 홈페이지
    * [https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)

* 모듈 호출

  * ```python
    from sklearn.linear_model import LinearRegression
    import numpy as np
    import matplotlib.pyplot as plt
    ```

* 내용 작성

  * ```python
    # 배열 생성
    x = np.array([1,2,3,4,5])
    y = np.array([2,4,6,8,10])
    
    # 머신러닝 객체 생성
    linear = LinearRegression()
    
    # [1,2,3,4,5] -> [[1],[2],[3],[4],[5]] (세로형태)로 리쉐잎
    print(x.reshape(-1, 1))
    
    
    linear.fit(x.reshape(-1,1), y)
    
    
    test_x = np.array([6,7,8,9,10])
    predict = linear.predict(test_x.reshape(-1,1))
    print(predict)
    
    plt.plot(test_x, predict)
    plt.show()
    ```

* 결과

  * ```
    [[1]
     [2]
     [3]
     [4]
     [5]]
    [12. 14. 16. 18. 20.]
    ```

  * ![image](https://user-images.githubusercontent.com/75322297/156278123-b2156f84-ffae-40a6-a541-68433af75d6d.png)





<br>

## 예측(Predict) 

* 파라미터 : [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)



###  실습 1

#### 환경 구성

* 공공 데이터 포털의 국방부_육군 신체측청정보 csv파일 다운
  * [https://opendata.mnd.go.kr/openinf/sheetview2.jsp?infId=OA-9425](https://opendata.mnd.go.kr/openinf/sheetview2.jsp?infId=OA-9425)
* 새로운 파이썬 파일 생성
* 받은 csv파일 `soldiers.csv`로 이름 변경 후 파이썬 파일과 같은 경로에 저장
* 몸무게와 키(신장)의 상관관계가 있는지 알아보려고 한다.



* Pandas 패키지 설치

  * `pip install pandas`

* 모듈 호출

  * ```python
    import pandas as pd
    ```



#### 데이터 준비

* ```python
  # 컬럼 네임들을 미리 리스트로 만들어 놓음
  names = ['순번', 'date', '가슴둘레', '소매길이', 'height', '허리둘레', '살높이', '머리둘레', '발길이', 'weight']
  
  # csv데이터를 불러옴
  df = pd.read_csv('soldiers.csv', encoding='euc-kr', names=names, header=0, low_memory=False)
  
  # 해당 컬럼만 가져와 다시 데이터프레임을 생성
  df = df[['date','height','weight']]
  
  
  print(len(df)) # 167983
  
  # 결측치제거 : dropna 
  #inplace=True는 df에 dropna가 적용된 것을 원본으로 바꿔버릴 때 사용
  df.dropna(inplace=True)
  
  print(len(df)) # 167983, 결측치가 없나 봄
  ```

* 년도만 남기기

  * ```
        date    height   weight
    0       20140106   185.500   73.200
    1           2013   167.200   65.100
    2       20140106   179.900   93.200
    3           2013   171.400   71.600
    4       20140106   176.900   67.100
    ```

  * 년도를 확인하면 4자리로 되있는 것을 확인할 수 있고, 년도만 있는 것과 월일도 포함 된 것이 있기 때문에 년도만 남겨서 값들에 적용해야한다.

  * ```python
    df['date'] = list(map(lambda x: int(str(x)[:4]) if len(str(x)) > 4 else x , df['date']))
    print(df)
    ```

  * ```
        date    height   weight
    0       2014   185.500   73.200
    1       2013   167.200   65.100
    2       2014   179.900   93.200
    3       2013   171.400   71.600
    4       2014   176.900   67.100
    ...      ...       ...      ...
    ```

* height 이쁘게 정리

  * 소수점 이하 첫 째 자리까지 줄이고 cm가 있는 부분도 지워주자.

  * ```python
    df['height'] = list(map(lambda x: float(str(x)[:5]) if len(str(x)) > 5 else x , df['height'] ))
    print(df)
    ```

  * ```
        date  height   weight
    0       2014   185.5   73.200
    1       2013   167.2   65.100
    2       2014   179.9   93.200
    3       2013   171.4   71.600
    4       2014   176.9   67.100
    ...      ...     ...      ...
    ```

* 몸무게 이쁘게 정리

  * float 형을 바꾸고 kg도 제거

  * ```python
    df['weight'] = list(map(lambda x: str(x).split(' ')[0], df['weight']))
    ```

  * 이렇게 하면 빈 문자열이 있는 셀들이 존재한다. 따라서 다른 람다함수를 적용

  * ```python
    # 아예 공백이 있는 곳들은 None으로 적용시키고, 값이 있다면 float형으로 
    df['weight'] = list(map(lambda x: float(x) if x else None, df['weight']))
    ```

  * 그리고 결측치 제거 후 길이 확인

  * ```python
    df.dropna(inplace=True)
    print(len(df))
    ```

  * ```
    167957
    ```

  * 길이가 줄어든 것을 확인할 수 있다.

* 무게와 신장 데이터 변수에 저장

  * ```python
    X = df['weight']
    y = df['height']
    
    ```



#### 데이터 분할

* 트레이닝 세트와 테스트 세트를 분리해야 함

* 모듈 호출

  * ```python
    from sklearn.model_selection import train_test_split   
    ```

* 분할

  * ```python
    # random_state는 randomseed같은 것
    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)
    train_X = train_X.values.reshape(-1,1)
    test_X = test_X.values.reshape(-1,1)
    print(train_X)
    ```

  * ```
    84094     74.0
    70451     84.0
    43745     58.7
    55139     90.2
    160473    69.8
              ...
    73349     77.5
    ```





#### 준비

* 모듈 호출

  * ```python
    from sklearn.linear_model import LinearRegression
    ```

* 선형 모델 생성

  * ```python
    linear = LinearRegression()
    ```



#### 학습

* `linear.fit()`으로 트레인 세트 데이터들을 학습 

* ```python
  linear.fit(train_X, train_y)
  ```





#### 예측

* ```python
  # 모델 학습 정도를 확인하기 위해 예측 확인
  predict = linear.predict(test_X)
  print(test_X)
  print(predict)
  ```

* ```
  [[100.7]
   [ 59.7]
   [ 80.5]
   ...
   [ 68.8]
   [ 64.2]
   [ 68.3]]
  [179.28084643 172.68043159 176.02893473 ... 174.14540171 173.40486737
   174.06490885]
  ```



#### 그림으로 그려보자

* 모듈 호출

  * ```python
    import matplotlib.pyplot as plt
    ```

* 코드 작성

  * ```python
    plt.plot(test_X, test_y, 'b.')
    plt.plot(test_X, predict, 'r.')
    
    plt.xlim(20, 150)
    plt.ylim(150, 220)
    plt.grid()
    
    
    plt.show()
    ```

* 그래프 확인

  * ![image](https://user-images.githubusercontent.com/75322297/156285576-83fbc1a8-81c2-4875-81b3-234cea80a77d.png)





### 실습 2

* 실습1과 같은 데이터로

* 몸무게와 키로 년도와 상관관계가 있는지 확인해보자

#### 데이터 준비

* 2013년을 0, 2014년을 1 ... 2017년을 4로 바꿔 데이터프레임의 새로운 컬럼 `date_new`에 넣어준다.

  * ```python
    df['date_new'] = list(map(lambda x: 0 if x == 2013 else 1 if x == 2014 else 2 if x == 2015 else 3 if x == 2016 else 4, df['date'])) 
    ```

* 새로운 컬럼`date_new`를 `X`에 적용

  * ```python
    X = df[['weight', 'date_new']]
    y = df['height']
    ```



#### 예측

* 실습1과 동일하게 출력만 다르게

* 70키로에 2017년

  * ```python
    print(linear.predict([[70, 4]]))
    ```





### 실습 3

* 실습 2에서 그대로 복사해 py 파일 생성

* 모듈 호출

  * ```python
    from sklearn.preprocessing import PolynomialFeatures
    ```



#### 데이터 준비

* 모듈 호출

  * ```python
    import numpy as np
    ```

* 다항회귀 (PolynormialFeatures) : 특성이 두 개. 특성을 곱해서 다차원으로 바꾸는 것

  * [1, a, b, a^2, ab, b^2] <-- 이런 식으로

* 다항회귀 모델 생성

  * ```python
    poly = PolynomialFeatures()
    ```

* 모델을 사용해 X 변경

  * ```python
    X = poly.fit(X).transform(X)
    ```



#### 데이터 분할

* 기존과 같으나 해당 부분 주석처리

  * ```python
    # train_X = train_X.values.reshape(-1,1)
    # test_X = test_X.values.reshape(-1,1)
    ```

#### 예측

* 예측값을 다항회귀에 맞게 변경해서 넣어야한다.

  * ```python
    print(linear.predict(poly.fit(np.array([[70,0]])).transform(np.array([[70,0]]))))
    ```

* 결과

  * ```
    [174.91721837]
    ```

### 그림으로 그려보기

* ![image](https://user-images.githubusercontent.com/75322297/156295167-1e210115-86bc-41a8-b98b-f99351f99425.png)
* 빨간 줄이 곡선형태로 바뀜(^(제곱)이 있어서)





### 실습 4

* 새 파이썬파일 생성

* 공공데이터 포탈에서'교육부 학생건강검사 결과분석 rawdata 서울 2015' csv 파일 다운
* 파일명 : `weight_height.csv` 으로 .py파일과 동일 경로에 위치





#### 데이터 준비

* 데이터프레임 생성 후 확인

  * ```python
    df = pd.read_csv('weight_height.csv', encoding='euc-kr')
    ```

  * ```
        ID       최종가중치  학교ID      도시규모 도시규모별분석용   학년도 광역시도       시도별  ...  혈당식전mgdl 총콜레스테롤mgdl ASTUL  ALTUL  혈색소gdl 간염검사    수축기   이완기
    0     Aa011남10101  169.550665  Aa01  대도시/중소도시   특별/광역시  2015   서울  서울특별시교육청  ...       NaN        NaN   NaN    NaN     NaN 
     NaN   77.0  58.0
    1     Aa011남10102  169.550665  Aa01  대도시/중소도시   특별/광역시  2015   서울  서울특별시교육청  ...       NaN        NaN   NaN    NaN     NaN 
     NaN   83.0  51.0
    ```

* 컬럼 목록 확인

  * ```
    ID       최종가중치  학교ID      도시규모 도시규모별분석용   학년도 광역시도       시도별  ...  혈당식전mgdl 총콜레스테롤mgdl ASTUL  ALTUL  혈색소gdl 간염검사    수축기   이완기
    ```

* 학교명, 학년, 성별, 키, 몸무게 컬럼만 가져온 후 확인

  * ```python
    df = df[['학교명','학년','성별','키','몸무게']]
    ```

  * ```
        학교명  학년 성별      키   몸무게
    0     서울대도초등학교   1  남  125.8  27.3
    1     서울대도초등학교   1  남  124.3  25.4
    2     서울대도초등학교   1  남  119.2  23.5
    3     서울대도초등학교   1  남  115.0  20.0
    4     서울대도초등학교   1  남  120.0  33.5
    ...        ...  .. ..    ...   ...
    ```

* 결측치 제거하고 개수 확인

  * ```python
    print(len(df))
    df.dropna(inplace=True)
    print(len(df))
    ```

  * ```
    9686
    9682
    ```

* 데이터 프레임에 `grade` 컬럼 만들어 주기 

  * 조건

    * 초등학교 0, 중학교 6, 고등학교 9
    * 그 뒤에 학년을 더해주면 됨

  * 작성 후 확인

    * ```python
      df['grade'] = list(map(lambda x: 0 if x[-4:] == '초등학교' else 6 if x[-3:] == '중학교' else 9, df['학교명'])) + df['학년']
      ```

    * ```
          학교명  학년 성별      키   몸무게  grade
      0     서울대도초등학교   1  남  125.8  27.3      1
      1     서울대도초등학교   1  남  124.3  25.4      1
      2     서울대도초등학교   1  남  119.2  23.5      1
      3     서울대도초등학교   1  남  115.0  20.0      1
      4     서울대도초등학교   1  남  120.0  33.5      1
      ...        ...  .. ..    ...   ...    ...
      9681  세종과학고등학교   3  남  176.1  50.4     12
      9682  세종과학고등학교   3  남  174.1  88.8     12
      9683  세종과학고등학교   3  남  169.5  63.2     12
      9684  세종과학고등학교   3  여  159.2  52.9     12
      9685  세종과학고등학교   3  여  162.0  67.7     12
      ```

* `grade` 컬럼을 생성했으니 `학교명`과 `학년` 컬럼을 날려주고 확인

  * ```python
    df.drop(['학교명','학년'], axis='columns', inplace=True)
    print(df)
    ```

  * ```
    성별      키   몸무게  grade
    0     남  125.8  27.3      1
    1     남  124.3  25.4      1
    2     남  119.2  23.5      1
    3     남  115.0  20.0      1
    4     남  120.0  33.5      1
    ...  ..    ...   ...    ...
    9681  남  176.1  50.4     12
    ```

* 컬럼을 영문으로 변경

  * ```python
    df.columns = ['gender', 'height', 'weight','grade']
    ```

* `gender`가 남자면 0, 여자면 1로 변경

  * ```python
    df['gender'] = list(map(lambda x: 0 if x == '남' else 1, df['gender']))
    ```

* 남자 여자 데이터프레임으로 나누기

  * 조건 만들기 : 0이라면(남자라면) is_boy

    * ```python
      is_boy = df['gender'] == 0
      ```

  * not(~) 사용

    * ```python
      boy_df, girl_df = df[is_boy], df[~is_boy]
      ```

* 남성의 키와 몸무게 변수로 저장

  * ```python
    X = boy_df['weight']
    y = boy_df['height']
    ```



#### 데이터 분할

* 모듈 호출

  * ```python
    from sklearn.model_selection import train_test_split
    ```

* 트레이닝/테스트 셋으로 분할

  * ```python
    # 테스트 사이즈는 0.3, random_state 1로 고정
    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)
    ```

* 형태 변환

  * ```python
    train_X = train_X.values.reshape(-1, 1)
    test_X = test_X.values.reshape(-1, 1)
    ```





#### 준비

* 모듈 호출

  * ```python
    from sklearn.linear_model import LinearRegression
    ```

* 모델 생성

  * ```python
    linear = LinearRegression()
    ```







#### 학습

* 모델을 사용해 학습 :`.fit()`

  * ```python
    linear.fit(train_X, train_y)
    ```

#### 예측

* `.predict`()로 예측

  * ```python
    predict = linear.predict(test_X)
    print(predict)
    
    ```

  * ```
    [148.4105717  159.22580237 147.91521762 ... 166.16075945 163.02351696
     168.05961674]
    ```

#### 그래프 

* ```python
  plt.plot(test_X, test_y, 'b.')
  plt.plot(test_X, predict, 'r.')
  
  
  
  plt.xlim(10, 140)
  plt.ylim(100, 220)
  
  plt.show()
  ```

* ![image](https://user-images.githubusercontent.com/75322297/156300705-1d128a06-cb28-40ae-aabe-b66adabd5332.png)









### 실습 5

* 동일 파일로 다항회귀
* **몸무게와 성별이 무슨 관계가 있는지 알아보려고 함**





#### 데이터 준비

* 모듈 호출

  * ```python
    from sklearn.preprocessing import PolynomialFeatures
    ```

* 변수 바꿔주기

  * ```python
    X = df['weight','gender']
    y = df['height']
    ```

* 폴리노미알 모델 생성

  * ```python
    poly = PolynomialFeatures()
    X = poly.fit(X).transform(X)
    ```



#### 데이터 분할

* 동일하나 reshape 한거 주석처리

  * ```python
    # train_X = train_X.values.reshape(-1, 1)
    # test_X = test_X.values.reshape(-1, 1)
    
    ```



#### 준비

* 동일



#### 예측

* 동일

#### 평가

* `.score`로 평가

  * ```python
    accuracy = linear.score(test_X, test_y)
    print('acc:', accuracy)
    ```

  * ```
    acc: 0.8322697807454456
    ```

#### 그래프 그리기

* ```python
  plt.plot(test_X, test_y, 'b.')
  plt.plot(test_X, predict, 'r.')
  
  
  
  plt.xlim(10, 140)
  plt.ylim(100, 220)
  
  plt.show()
  ```

* ![image](https://user-images.githubusercontent.com/75322297/156301582-264f1630-1d8a-4370-b373-0de6f6da4ab5.png)







### 실습 6

* 섭씨에도 화씨로 온도 변경

* 함수 작성

  * ```python
    import numpy as np
    
    def celsius_to_fahrenheit(x):
        return 9.0/ 5 * x + 32
    
    
    
    data_C  = np.array(range(0, 100))
    data_F = celsius_to_fahrenheit(data_C)
    
    inp = int(input('섭씨 온도를 입력해 주세요 : '))
    print(inp, '는 화씨',celsius_to_fahrenheit(inp), '입니다.')
    ```

* 결과

  * ```
    섭씨 온도를 입력해 주세요 : 8
    8 는 화씨 46.4 입니다.
    ```

* **이 과정을 컴퓨터에게 학습시켜보자.**



#### 데이터 준비

* ```python
  X = data_C
  y = data_F
  ```



#### 데이터 분할

* 모듈 호출

  * ```python
    from sklearn.model_selection import train_test_split
    ```

* 테스트 사이즈 30%, 랜덤스테이츠 1로 고정

  * ```python
    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)
    train_X = train_X.reshape(-1, 1)
    test_X = test_X.reshape(-1, 1)
    ```

#### 준비

* 모듈 호출

  * ```python
    from sklearn.linear_model import LinearRegression
    ```

* 모델 생성

  * ```python
    linear = LinearRegression()
    ```

#### 학습

* ```python
  linear.fit(train_X, train_y)
  ```

#### 예측

* ```python
  predict= linear.predict(test_X)
  
  pred_f = linear.predict([[30]])
  print('30 to fahrenheit', pred_f)
  ```

* ```
  30 to fahrenheit [86.]
  ```

#### 평가

* ```python
  accuracy = linear.score(test_X, test_y)
  print(accuracy)
  ```

* ```
  1.0
  ```

* 학습된 것과 실제 값이 100% 일치 한다.(1.0)



<br>



## 분류(Logistic)

* 파라미터 : [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)

### 실습 1

#### 데이터 준비

* 공부시간과 과외시간 형태로 배열을 만들어 준다.

* X - 예 : 1시간 공부하고 0시간 과외하면 [1,0], 8시간 공부하고 1시간 과외하면 [8,1]

* y - 만약, 시험에 떨어졌다면 [0], 붙었다면 [1]

* ```python
  X = [
      [1, 0],
      [2, 0],
      [5, 1],
      [2, 3],
      [3, 3],
      [8, 1],
      [10, 0]
  ]
  
  y = [
      [0],
      [1],
      [0],
      [0],
      [1],
      [1],
      [1]
  ]
  ```



#### 데이터 분할

* 모듈 호출

  * ```python
    from sklearn.model_selection import train_test_split
    ```

* training, test 데이터 분할

  * ```python
    train_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.3, random_state=1)
    ```

#### 준비

* 모듈 호출

  * ```python
    from sklearn.linear_model import LogisticRegression
    ```

* 분류 모델 생성

  * ```python
    logistic = LogisticRegression()
    ```

#### 학습

* 모듈 호출

  * ```python
    import numpy as np
    ```

* 학습

  * ```python
    logistic.fit(train_X, np.ravel(train_y))
    ```

**여기서 잠깐,** `np.ravel()`은 무슨 기능을 하나?

: 다차원 배열을 1차원으로 바꾸는 기능을 한다. 예를 들어, `a = [[1,2],[3,4]]` 라는 배열이 있을 때 `np.ravel(a)`를 해주면 `[1,2,3,4]`의 형태로 바뀐다. 기존 a값의 원소가 바뀌면 ravel한 배열의 원소도 바뀐다는 점을 명심(**얕은복사**)하자!

#### 예측

* ```python
  pred = logistic.predict(test_X)
  
  for i in range(len(test_X)):
      print('{} 시간 공부 {} 시간 과외 : {}'.format(test_X[i][0], test_X[i][1], 'pass' if pred[i] == 1 else 'fail'))
  
  print('acc :', logistic.score(test_X, test_y))
  ```

* ```
  10 시간 공부 0 시간 과외 : pass
  5 시간 공부 1 시간 과외 : pass
  2 시간 공부 0 시간 과외 : fail
  acc : 0.3333333333333333
  ```

* 데이터가 너무 적어서 예측결과가 신뢰도가 떨어질 수 있다.











### 실습 2

* sklearn.dataset에는 다양한 데이터셋이 존재한다.
* `form sklearn.datasets import load_데이터셋명`
* 그 중에서 iris데이터를 가져오자.

#### 데이터 준비

* 모듈 호출

  * ```python
    from sklearn.datasets import load_iris
    ```

* 데이터셋 저장

  * ```python
    iris = load_iris()
    ```

* feature명 확인

  * ```python
    print(iris.feature_names)
    ```

  * ```
    ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
    ```

* target명 확인(꽃 세부 종류들)

  * ```python
    print(iris.target_names)
    ```

  * ```
    ['setosa' 'versicolor' 'virginica']
    ```

* 변수 설정

  * ```python
    X = iris.data
    y = iris.target
    ```

* 데이터프레임 생성

  * ```python
    df = pd.DataFrame(X)
    df.columns= ['sepal length', 'sepal_width', 'petal_length', 'petal_width']
    df['category'] = pd.DataFrame(iris.target_names[y].reshape(-1))
    
    print(df)
    ```

  * ```
        sepal length  sepal_width  petal_length  petal_width   category
    0             5.1          3.5           1.4          0.2     setosa
    1             4.9          3.0           1.4          0.2     setosa
    2             4.7          3.2           1.3          0.2     setosa
    3             4.6          3.1           1.5          0.2     setosa
    4             5.0          3.6           1.4          0.2     setosa
    ..            ...          ...           ...          ...        ...
    145           6.7          3.0           5.2          2.3  virginica
    146           6.3          2.5           5.0          1.9  virginica
    147           6.5          3.0           5.2          2.0  virginica
    148           6.2          3.4           5.4          2.3  virginica
    149           5.9          3.0           5.1          1.8  virginica
    ```

#### 데이터 분할

* 모듈 호출

  * ```python
    from sklearn.model_selection import train_test_split
    ```

* 트레이닝/테스트 세트로 분할

  * ```python
    train_X, test_X, train_y, test_y = train_test_split(X,y, test_size=0.3, random_state=1)
    ```

#### 준비

* 종을 분류할 것이기 때문에 LogisticRegression 활용

* 모듈 호출

  * ```python
    from sklearn.linear_model import LogisticRegression
    ```

* 모델 생성

  * ```python
    logistic = LogisticRegression()
    ```

#### 학습

* ```python
  logistic.fit(train_X, train_y)
  ```

#### 예측

* ```python
  pred = logistic.predict(test_X)
  for i in range(len(test_X)):
      print(f'{test_X[i]} 예측 : {iris.target_names[pred[i]]} / 실제 : {iris.target_names[test_y[i]]}')
  ```

* ```
  [5.8 4.  1.2 0.2] 예측 : setosa / 실제 : setosa
  [5.1 2.5 3.  1.1] 예측 : versicolor / 실제 : versicolor
  [6.6 3.  4.4 1.4] 예측 : versicolor / 실제 : versicolor
  [5.4 3.9 1.3 0.4] 예측 : setosa / 실제 : setosa
  [7.9 3.8 6.4 2. ] 예측 : virginica / 실제 : virginica
  [6.3 3.3 4.7 1.6] 예측 : versicolor / 실제 : versicolor
  [6.9 3.1 5.1 2.3] 예측 : virginica / 실제 : virginica
  [5.1 3.8 1.9 0.4] 예측 : setosa / 실제 : setosa
  [4.7 3.2 1.6 0.2] 예측 : setosa / 실제 : setosa
  [6.9 3.2 5.7 2.3] 예측 : virginica / 실제 : virginica
  ....
  ...
  ...
  ```

#### 평가

* ```python
  print(f'acc : {logistic.score(test_X, test_y)}')
  ```

* ```
  acc : 0.9777777777777777
  ```

* 예측과 실제 값이 약 97.77777% 일치한다.



<br>



## 분류(KNN)

* K-Nearest Neighbors
* 입력값에서 가장 가까운 k개의 데이터를 비교
* k개 중 가장 많은 class로 분류
* 일반적으로 k는 홀수
* 파라미터 : [https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)

### 실습

#### 데이터 준비

* 모듈 호출

  * ```python
    from sklearn.datasets import load_iris
    ```

* iris 데이터 불러와 데이터 지정

  * ```python
    iris = load_iris()
    X = iris.data
    y = iris.target
    ```

#### 데이터 분할

* 모듈 호출

  * ```python
    from sklearn.model_selection import train_test_split
    ```

* 트레이닝, 테스트 세트 분할

  * ```python
    train_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.3,random_state=1)
    ```

#### 준비

* 모듈 호출

  * ```python
    from sklearn.neighbors import KNeighborsClassifier
    ```

* 모델 생성

  * ```python
    model = KNeighborsClassifier()
    
    ```

#### 학습

* ```python
  model.fit(train_X, train_y)
  ```

#### 예측

* ```python
  pred = model.predict(test_X)
  for i in range(len(test_X)):
      print(f'{test_X[i]} 예측 : {iris.target_names[pred[i]]} / 실제 : {iris.target_names[test_y[i]]}')
  ```

* ```
  ...
  ...
  ...
  ...
  [6.3 2.9 5.6 1.8] 예측 : virginica / 실제 : virginica
  [6.6 2.9 4.6 1.3] 예측 : versicolor / 실제 : versicolor
  ```



#### 평가

* ```python
  print(f'acc : {model.score(test_X, test_y)}')
  ```

* ```
  acc : 0.9777777777777777
  ```





<br>



## 분류(SVM)

* Support Vector Machine
* Margin이 최대화가 되는 결정 경계(초평면)을 정의
* Hard Margin SVM 
  * 이상치를 허용하지 않음(overfitting)
  * 진짜 꼼꼼하게 완벽하게 분류하려고하는거
* Soft margin SVM
  * 이상치를 어느정도 허용(underfitting)
* **Kernel Trick**
  * 차원을 추가하여 분류
* 파라미터 : [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)

### 실습

#### 데이터 준비

* 모듈 호출

  * ```python
    from sklearn.datasets import load_iris
    ```

* 데이터 준비

  * ```python
    iris = load_iris()
    X = iris.data
    y = iris.target
    ```

#### 데이터 분할

* 모듈 호출

  * ```python
    from sklearn.model_selection import train_test_split
    ```

* 트레이닝/테스트 세트 분할

  * ```python
    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)
    ```

#### 준비

* 모듈 호출

  * ```python
    from sklearn.svm import SVC
    ```

* SVM 모델 생성

  * ```python
    model = SVC(kernel='linear')
    ```

#### 학습

* ```python
  model.fit(train_X, train_y)
  ```

#### 예측

* ```python
  pred = model.predict(test_X)
  print(test_X)
  print(pred)
  ```

* ```
  [[5.8 4.  1.2 0.2]
   [5.1 2.5 3.  1.1]
   [6.6 3.  4.4 1.4]
   [5.4 3.9 1.3 0.4]
   [7.9 3.8 6.4 2. ]
   [6.3 3.3 4.7 1.6]
   [6.9 3.1 5.1 2.3]
   [5.1 3.8 1.9 0.4]
   [4.7 3.2 1.6 0.2]
   [6.9 3.2 5.7 2.3]
   [5.6 2.7 4.2 1.3]
   [5.4 3.9 1.7 0.4]
   [7.1 3.  5.9 2.1]
   [6.4 3.2 4.5 1.5]
   [6.  2.9 4.5 1.5]
   [4.4 3.2 1.3 0.2]
   [5.8 2.6 4.  1.2]
   [5.6 3.  4.5 1.5]
   [5.4 3.4 1.5 0.4]
   [5.  3.2 1.2 0.2]
   [5.5 2.6 4.4 1.2]
   [5.4 3.  4.5 1.5]
   [6.7 3.  5.  1.7]
   [5.  3.5 1.3 0.3]
   [7.2 3.2 6.  1.8]
   [5.7 2.8 4.1 1.3]
   [5.5 4.2 1.4 0.2]
   [5.1 3.8 1.5 0.3]
   [6.1 2.8 4.7 1.2]
   [6.3 2.5 5.  1.9]
   [6.1 3.  4.6 1.4]
   [7.7 3.  6.1 2.3]
   [5.6 2.5 3.9 1.1]
   [6.4 2.8 5.6 2.1]
   [5.8 2.8 5.1 2.4]
   [5.3 3.7 1.5 0.2]
   [5.5 2.3 4.  1.3]
   [5.2 3.4 1.4 0.2]
   [6.5 2.8 4.6 1.5]
   [6.7 2.5 5.8 1.8]
   [6.8 3.  5.5 2.1]
   [5.1 3.5 1.4 0.3]
   [6.  2.2 5.  1.5]
   [6.3 2.9 5.6 1.8]
   [6.6 2.9 4.6 1.3]]
  [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1
   0 1 2 2 0 2 2 1]
  ```



#### 평가

* ```python
  print(model.score(test_X, test_y))
  ```

* ```
  1.0
  ```





<br>



## 분류(Decision Tree)

* Decision Tree
* 질문(Node)에 대한 답(참/거짓)을 반복해 분류
* 불순도(impurity)가 낮아지는 방향으로 진행(**스무고개를 생각하면 될듯**)
* 질문이 너무 많아지면 **과적합**에 빠질 수 있다.
* 파라미터 : [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)

### 실습

#### 데이터 준비

* 모듈 호출

  * ```python
    from sklearn.datasets import load_iris
    ```

* 데이터 준비

  * ```python
    iris = load_iris()
    X = iris.data
    y = iris.target
    ```

#### 데이터 분할

* 모듈 호출

  * ```python
    from sklearn.model_selection import train_test_split
    ```

* 트레이닝 테스트 데이터 분할

  * ```python
    train_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.3, random_state=1)
    ```

#### 준비

* 모듈 호출

  * ```python
    from sklearn.tree import DecisionTreeClassifier
    ```

* 결정트리 모델 생성

  * ```python
    model = DecisionTreeClassifier()
    ```

#### 학습

* ```python
  model.fit(train_X, train_y)
  ```

#### 예측

* ```python
  pred = model.predict(test_X)
  for i in range(len(test_X)):
      print(f'{test_X[i]} : {pred[i]}')
  ```

* ```
  [5.8 4.  1.2 0.2] : 0
  [5.1 2.5 3.  1.1] : 1
  [6.6 3.  4.4 1.4] : 1
  [5.4 3.9 1.3 0.4] : 0
  [7.9 3.8 6.4 2. ] : 2
  [6.3 3.3 4.7 1.6] : 1
  [6.9 3.1 5.1 2.3] : 2
  [5.1 3.8 1.9 0.4] : 0
  [4.7 3.2 1.6 0.2] : 0
  [6.9 3.2 5.7 2.3] : 2
  [5.6 2.7 4.2 1.3] : 1
  [5.4 3.9 1.7 0.4] : 0
  [7.1 3.  5.9 2.1] : 2
  [6.4 3.2 4.5 1.5] : 1
  [6.  2.9 4.5 1.5] : 1
  [4.4 3.2 1.3 0.2] : 0
  [5.8 2.6 4.  1.2] : 1
  [5.6 3.  4.5 1.5] : 1
  [5.4 3.4 1.5 0.4] : 0
  [5.  3.2 1.2 0.2] : 0
  [5.5 2.6 4.4 1.2] : 1
  [5.4 3.  4.5 1.5] : 1
  [6.7 3.  5.  1.7] : 2
  [5.  3.5 1.3 0.3] : 0
  [7.2 3.2 6.  1.8] : 2
  [5.7 2.8 4.1 1.3] : 1
  [5.5 4.2 1.4 0.2] : 0
  [5.1 3.8 1.5 0.3] : 0
  [6.1 2.8 4.7 1.2] : 1
  [6.3 2.5 5.  1.9] : 2
  [6.1 3.  4.6 1.4] : 1
  [7.7 3.  6.1 2.3] : 2
  [5.6 2.5 3.9 1.1] : 1
  [6.4 2.8 5.6 2.1] : 2
  [5.8 2.8 5.1 2.4] : 2
  [5.3 3.7 1.5 0.2] : 0
  [5.5 2.3 4.  1.3] : 1
  [5.2 3.4 1.4 0.2] : 0
  [6.5 2.8 4.6 1.5] : 1
  [6.7 2.5 5.8 1.8] : 2
  [6.8 3.  5.5 2.1] : 2
  [5.1 3.5 1.4 0.3] : 0
  [6.  2.2 5.  1.5] : 1
  [6.3 2.9 5.6 1.8] : 2
  [6.6 2.9 4.6 1.3] : 1
  ```

#### 평가

* ```python
  print(model.score(test_X, test_y))
  ```

* ```
  0.9555555555555556
  ```





<br>



## 분류(Random Forest)

* 여러 개의 결정 트리를 연결(앙상블)
* Bagging(Bootstrap AGGregatING)
* 임의의 부분집합(Forest) -> 각 결정 트리(Decision Tree)의 결과를 더해서 분류
* 파라미터 : [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)

### 실습

#### 데이터 준비

* 모듈 호출

  * ```python
    from sklearn.datasets import load_iris
    ```

* iris 데이터 준비

  * ```python
    iris = load_iris()
    X = iris.data
    y = iris.target
    ```

#### 데이터 분할

* 모듈 호출

  * ```python
    from sklearn.model_selection import train_test_split
    ```

* 트레이닝/테스트 데이터 셋으로 분할

  * ```python
    train_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.3,random_state=1)
    ```

#### 준비

* 랜덤포레스트 모듈 호출

  * ```python
    from sklearn.ensemble import RandomForestClassifier
    ```

* 랜덤포레스트 모델 생성

  * ```python
    model = RandomForestClassifier()
    ```

#### 학습

* 문제와 답을 넣어 줌

* ```python
  model.fit(train_X, train_y)
  ```

#### 예측

* ```python
  pred = model.predict(test_X)
  print(test_X)
  print(pred)
  ```

* ```
  [[5.8 4.  1.2 0.2]
   [5.1 2.5 3.  1.1]
   [6.6 3.  4.4 1.4]
   [5.4 3.9 1.3 0.4]
   [7.9 3.8 6.4 2. ]
   [6.3 3.3 4.7 1.6]
   [6.9 3.1 5.1 2.3]
   [5.1 3.8 1.9 0.4]
   [4.7 3.2 1.6 0.2]
   [6.9 3.2 5.7 2.3]
   [5.6 2.7 4.2 1.3]
   [5.4 3.9 1.7 0.4]
   [7.1 3.  5.9 2.1]
   [6.4 3.2 4.5 1.5]
   [6.  2.9 4.5 1.5]
   [4.4 3.2 1.3 0.2]
   [5.8 2.6 4.  1.2]
   [5.6 3.  4.5 1.5]
   [5.4 3.4 1.5 0.4]
   [5.  3.2 1.2 0.2]
   [5.5 2.6 4.4 1.2]
   [5.4 3.  4.5 1.5]
   [6.7 3.  5.  1.7]
   [5.  3.5 1.3 0.3]
   [7.2 3.2 6.  1.8]
   [5.7 2.8 4.1 1.3]
   [5.5 4.2 1.4 0.2]
   [5.1 3.8 1.5 0.3]
   [6.1 2.8 4.7 1.2]
   [6.3 2.5 5.  1.9]
   [6.1 3.  4.6 1.4]
   [7.7 3.  6.1 2.3]
   [5.6 2.5 3.9 1.1]
   [6.4 2.8 5.6 2.1]
   [5.8 2.8 5.1 2.4]
   [5.3 3.7 1.5 0.2]
   [5.5 2.3 4.  1.3]
   [5.2 3.4 1.4 0.2]
   [6.5 2.8 4.6 1.5]
   [6.7 2.5 5.8 1.8]
   [6.8 3.  5.5 2.1]
   [5.1 3.5 1.4 0.3]
   [6.  2.2 5.  1.5]
   [6.3 2.9 5.6 1.8]
   [6.6 2.9 4.6 1.3]]
  [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 2 0 2 1 0 0 1 2 1 2 1 2 2 0 1
   0 1 2 2 0 1 2 1]
  ```

#### 평가

* ```python
  print(model.score(test_X,test_y))
  ```

* ```
  0.9555555555555556
  ```

  * 학습된 것과 실제 값이 95.6% 일치한다.









<br>





## 분류(Ensemble)

* 여러 개의 모델을 결합하여 결과를 도출하는 모델
* Voting : 각각 다른 모델들의 결과를 다수결로 선택
* Bagging : 같은 모델을 여러 개 병렬로 실행하여 선형 결합
* Boosting : 가벼운 모델을 순차적으로 학습하여 결과 도출



<br>

<br>

---



# 머신러닝 비지도 학습

## 군집(clustering) - K-mean

* k개의 중심(Centroid)을 랜덤으로 지정
  * 각 데이터들을 가장 가까운 그룹(cluster)에 할당
  * 위의 단계를 반복하여 변경되는 데이터가 없을 때 까지 반복
* 각 데이터의 그룹과 중심의 거리 차이의 분산을 최소화
* 파라미터 : [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

### 실습

#### 데이터 준비

* 모듈 호출

  * ```python
    from sklearn.datasets import load_iris
    ```

* iris 데이터 준비

  * ```python
    iris = load_iris()
    X = iris.data
    y = iris.target
    ```

#### 데이터 분할

* 모듈 호출

  * ```python
    from sklearn.model_selection import train_test_split
    ```

* 트레이닝/테스트 데이터 세트 분할(테스트 사이즈는 30%, 랜덤스테이츠는 1)

  * ```python
    train_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.3,random_state=1)
    ```

#### 준비

* 모듈 호출

  * ```python
    from sklearn.cluster import KMeans
    ```

* K-means 모델 생성

  * 분류하고자 하는 개수가 3개이기 때문에 `n_clusters=3` (setosa, verginica, vergi~)

  * ```python
    model = KMeans(n_clusters=3)
    ```

#### 학습

* k-mean(군집)은 중심을 알아서 찾기 때문에(비지도) 문제만 넣고 답을 넣어주지 않아도 데이터를 알아서 넣어준다.

  * 문제(`train_X`)만 학습시켜주면 된다.

  * ```python
    model.fit(train_X)
    ```

#### 예측

* ```python
  pred = model.predict(test_X)
  print(test_X)
  print(pred)
  ```

* ```
  [[5.8 4.  1.2 0.2]
   [5.1 2.5 3.  1.1]
   [6.6 3.  4.4 1.4]
   [5.4 3.9 1.3 0.4]
   [7.9 3.8 6.4 2. ]
   [6.3 3.3 4.7 1.6]
   [6.9 3.1 5.1 2.3]
   [5.1 3.8 1.9 0.4]
   [4.7 3.2 1.6 0.2]
   [6.9 3.2 5.7 2.3]
   [5.6 2.7 4.2 1.3]
   [5.4 3.9 1.7 0.4]
   [7.1 3.  5.9 2.1]
   [6.4 3.2 4.5 1.5]
   [6.  2.9 4.5 1.5]
   [4.4 3.2 1.3 0.2]
   [5.8 2.6 4.  1.2]
   [5.6 3.  4.5 1.5]
   [5.4 3.4 1.5 0.4]
   [5.  3.2 1.2 0.2]
   [5.5 2.6 4.4 1.2]
   [5.4 3.  4.5 1.5]
   [6.7 3.  5.  1.7]
   [5.  3.5 1.3 0.3]
   [7.2 3.2 6.  1.8]
   [5.7 2.8 4.1 1.3]
   [5.5 4.2 1.4 0.2]
   [5.1 3.8 1.5 0.3]
   [6.1 2.8 4.7 1.2]
   [6.3 2.5 5.  1.9]
   [6.1 3.  4.6 1.4]
   [7.7 3.  6.1 2.3]
   [5.6 2.5 3.9 1.1]
   [6.4 2.8 5.6 2.1]
   [5.8 2.8 5.1 2.4]
   [5.3 3.7 1.5 0.2]
   [5.5 2.3 4.  1.3]
   [5.2 3.4 1.4 0.2]
   [6.5 2.8 4.6 1.5]
   [6.7 2.5 5.8 1.8]
   [6.8 3.  5.5 2.1]
   [5.1 3.5 1.4 0.3]
   [6.  2.2 5.  1.5]
   [6.3 2.9 5.6 1.8]
   [6.6 2.9 4.6 1.3]]
  [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 2 0 2 1 0 0 1 1 1 2 1 2 1 0 1
   0 1 2 2 0 1 2 1]
  ```

#### 그래프 그리기

* ```python
  df = pd.DataFrame(test_X)
  df.columns = ['sepal_length','sepal_width','petal_length','petal_width']
  df['category'] = pd.DataFrame(pred)
  
  
  centers = pd.DataFrame(model.cluster_centers_)
  centers.columns = ['sepal_length','sepal_width','petal_length','petal_width']
  center_X = centers['sepal_length']
  center_y = centers['sepal_width']
  
  plt.scatter(df['sepal_length'], df['sepal_width'], c=df['category'])
  plt.scatter(center_X, center_y, s=100, c='r', marker='*')
  plt.show()
  ```

* ![image](https://user-images.githubusercontent.com/75322297/156323900-dc306c1f-0cbe-4e78-a084-2003b88e7ceb.png)







<br>

<br>

---





