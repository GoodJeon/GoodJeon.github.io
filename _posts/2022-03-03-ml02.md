---
layout: posts
comments: true
title: "[ML/DL]TensorFlow 머신/딥러닝 기초 복습"
categories: ML/DL
tag: [머신러닝, Machine Learning, ML, Tensorflow, DL, 딥러닝]


toc: true
toc_icon: "cog"
toc_sticky: true
date: 2022-03-03
last_modified_at: 2022-03-03



---





# Tensorflow

* 머신러닝과 딥러닝을 사용할 수 있는 패키지이다!💭

* 학습 = h와 y를 비교하며 loss function을 사용해서 Weight와 Bias가 변경되는 과정이며, 이 일을 해주는 것이 optimizer다.
* `Tensor` : 데이터를 저장하는 객체(placeholder)
* `Variable` : Weight, Bias
* `Operation` : H = W * X + b (node, 식) -> 그래프 (tensor -> operation -> tensor -> operation -> ,,,)
* `Session` : 실행 환경

## 환경 설정

* 아나콘다 가상환경을 사용할 것이므로, 아나콘다가 설치되어 있어야 한다.

* 가상환경 생성
  * `conda create -n ai02 python=3.7 -y`
  * 텐서플로우 1.15version을 사용하기 위해 파이썬 3.7버전을 설치
* VSC에서 `Ctrl+Shift+P`로 해당 가상환경 터미널 띄우기
  * ![image](https://user-images.githubusercontent.com/75322297/156470845-925a7808-56f9-45cd-9819-9db4657b29fa.png)
* 텐서플로우 1.15버전 설치
  * `conda install tensorflow==1.15 -y`
  * `pip install tensorflow==1.15`



### 기본 작동 방법을 알아보자!😱

* `X, y -> W * X + b <-(optimizer)-> (H, y) : loss function`

* 모듈 호출

  * `import tensorflow as tf`

* 상수 노드를 생성

  * ```python
    node = tf.constant(100)
    ```

* `session` 사용

  * `session`은 그래프를 실행시켜주는 역할(runner)이다.

  * `.run()`을 통해 실행

  * ```python
    sess = tf.Session()
    
    print(sess.run(node))
    ```

* 결과

  * ```
    100
    ```

* `tf.constant()` 는 노드에 상수값을 넣어주는 기능을 한다.

  * ```python
    node1 = tf.constant(10, dtype=tf.float32) # 10 입력
    node2 = tf.constant(20, dtype=tf.float32) # 20 입력
    node3 = node1 + node2
    ```

  * 여기서, `dtype`은 텐서플로우 내부적으로 `numpy`를 사용 중이여서 데이터 타입을 사용하는 것이다.

* `.Session()` 사용

  * ```python
    sess = tf.Session()
    print(sess.run(node3))  # node1 + node2 = 10.0 + 20.0
    
    print(sess.run([node1, node3])) [10.0, 30.0]
    ```

* 결과

  * ```
    30.0
    [10.0, 30.0]
    ```

* 그리고 스크립트를 실행 시에 이러한 알림이 뜬다.

  * ```python
    WARNING:tensorflow:From 02_tensor.py:13: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.
    ```

  * 이럴 때는 import를 이렇게 하면 된다.

  * ```python
    import tensorflow.compat.v1 as tf
    ```

* `placeholder` : 그래프를 실행하는 시점에 데이터를 입력받을 수 있도록 공간만 만들어 놓음

  * ```python
    node1 = tf.placeholder(dtype=tf.float32)
    node2 = tf.placeholder(dtype=tf.float32)
    
    node3 = node1 + node2
    # 값은 받지 않고 미리 노드들을 더해놓는다.
    ```

* `.Session()` 객체 생성

  * ```python
    sess = tf.Session()
    ```

* 만들어 놓은 공간에 data 입력

  * ```python
    X = [10, 20, 30]
    y = [40, 50, 60]
    ```

* 여기가 그래프를 실행하는 시점

  * `feed_dict`를 사용해 데이터를 입력해줌

  * ```python
    print(sess.run(node3, feed_dict={node1:X,node2:y}))
    ```

* 결과

  * ```python
    [50. 70. 90.]
    ```





<br>

---



## 선형 회귀(Linear-Regression)

* 텐서플로우는 `sklearn`을 사용할 때와 과정이 조금 다르다.
  1. 데이터 준비
  2. 가설 설정
  3. 준비
  4. 학습
  5. 예측
* **데이터 분할** 부분이 **가설 설정**으로 바뀌었다.



### 실습

#### 데이터 준비

* ```python
  X = tf.placeholder(tf.float32)
  y = tf.placeholder(tf.float32)
  ```

* X와 y에 실수형이 들어갈 공간을 만들어 줌

#### 가설 설정

* **H(hypothesis) = W (weight) * X + b (bias)**

* ```python
  # Weight 생성
  W = tf.Variable(tf.random_normal([1]), name='weight')
  # Bias 생성
  b = tf.Variable(tf.random_normal([1]), name='bias')
  
  H = W * X + b
  ```

* `.random_normal()` 은 랜덤으로 표준분포 값을 설정해주는 놈

#### 준비

* 이 단계에서는 loss function, optimizer, session이 준비되어야 한다.

* loss function

  * MSE(Mean Square Error)

  * ```python
    loss = tf.reduce_mean(tf.square(H - y))
    ```

* optimizer

  * 경사 하강법(gradient descent) : loss가 최소화 되는 값을 찾기

  * ```python
    optimizer = tf.train.GradientDescentOptimizer(0.01)
    ```

  * 여기서 `0.01`은 `learning rate`로 얼마큼씩 움직일 건지 알려준다.

  * ```python
    train = optimizer.minimize(loss)
    ```

  * loss가 최소화 되도록 최적화

* session

  * `.Session()` 객체 생성

  * ```python
    sess = tf.Session()
    ```

  * 여기서 그래프에 있는 모든 변수들을 초기화해줘야 한다. 이때 사용하는 것은 `tf.global_variables_initializer()`

  * ```python
    sess.run(tf.global_variables_initializer())
    ```

#### 학습

* 과적합(overfitting)을 방지하기 위해 적절한 학습 횟수를 선정해야한다.

* ```python
  epochs = 5000
  for step in range(epochs):
      tmp, loss_val, W_val, b_val = sess.run([train, loss, W, b], feed_dict={X:[1,2,3,4,5],y:[3,5,7,9,11]})
      if step % 100 == 0:
          print(f'W:{W_val} \t b:{b_val} \t loss:{loss_val}')
  ```

* `loss_val, W_val, b_val`을 실행시키는 이유는 변하는 과정을 확인하기 위함

* `step % 1000 == 0` 은 반복문 1000번마다 확인하기 위함

* 결과

  * ```
    W:[-0.07149309]          b:[1.2598735]   loss:75.4944076538086
    W:[1.9927471]    b:[1.0261854]   loss:0.0001256559626199305
    W:[1.9997544]    b:[1.0008862]   loss:1.4397728875792382e-07
    W:[1.9999908]    b:[1.0000327]   loss:2.0108928411310956e-10
    W:[1.999994]     b:[1.0000207]   loss:7.644303245957218e-11
    ```

#### 예측

* 가설 H와 데이터를 입력해주고 실행

  * ```python
    print(sess.run(H, feed_dict={X:[10,11,12,13,14]}))
    ```

* 결과

  * ```
    [20.999962 22.999956 24.999949 26.999943 28.999937]
    ```



## 다중 선형회귀(Multi-Linear Regrression)

### 실습

#### 데이터 준비

* X_data는 세 번의 모의고사, y_data는 실제 시험 점수

  * ```python
    X_data = [
        [73, 80, 75],
        [93, 88, 93],
        [89, 91, 90],
        [96, 89, 100],
        [73, 66, 70]
    ]
    
    y_data = [
        [80],
        [91],
        [88],
        [94],
        [61]
    ]
    ```

* `placeholder`로 노드에 데이터가 들어갈 공간 만들어주기

  * ```python
    X = tf.placeholder(shape=[None,3], dtype=tf.float32)
    y = tf.placeholder(shape=[None,1], dtype=tf.float32)
    ```

  * 데이터 타입은 실수형

  * 여기서 `shape`은 안에 들어갈 형태를 정해준다.

    * 예시
    * shape = [5, 3] -> 5개 리스트 안에 3개의 요소
    * shape이 None : 갯수 상관 없다는 뜻

#### 가설 설정

* X와 W가 행렬 연산할 때 열과 행이 같아야 한다.

* X의 형태가 `[None, 3]`이므로 W에 `[3,1]`로 들어가야 한다.

  * ```python
    W = tf.Variable(tf.random_normal([3,1]), name='weight')
    b = tf.Variable(tf.random_normal([1]), name='bias')
    ```

* **H = W * X + b**

  * ```python
    H = tf.matmul(X, W) + b
    ```

#### 준비

* loss function

  * ```python
    loss = tf.reduce_mean(tf.square(H - y))
    ```

* optimizer

  * ```python
    learning_rate = 0.00004
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train = optimizer.minimize(loss)
    ```

* session

  * ```python
    sess = tf.Session()
    # 변수초기화 진행
    sess.run(tf.global_variables_initializer())
    ```

#### 학습

* 로스를 줄여나가는 과정을 확인하며 학습

  * ```python
    epochs = 10000
    for step in range(epochs):
        _, loss_val, W_val, b_val = sess.run([train, loss, W, b], feed_dict={X:X_data, y:y_data})
        if step % 1000 == 0:
            print(f'W:{W_val} \t b:{b_val} \t loss:{loss_val}')
    ```

* 확인

  * ```
    W:[[2.2694695 ]
     [0.02989042]
     [0.5611059 ]]   b:[-0.9297394]          loss:47803.86328125
    W:[[ 0.697498  ]
     [ 0.49425066]
     [-0.18740939]]          b:[-0.9556813]          loss:20.511674880981445
    W:[[0.27228674]
     [0.7211382 ]
     [0.01448941]]   b:[-0.97749513]         loss:13.250178337097168
    W:[[0.00078062]
     [0.76103204]
     [0.24469697]]   b:[-0.9971895]          loss:10.001592636108398
    W:[[-0.20451084]
     [ 0.7613147 ]
     [ 0.44759852]]          b:[-1.0140916]          loss:7.903079986572266
    W:[[-0.3688228 ]
     [ 0.75436056]
     [ 0.6169308 ]]          b:[-1.0284728]          loss:6.4997758865356445
    W:[[-0.50251806]
     [ 0.7470755 ]
     [ 0.75628585]]          b:[-1.0407374]          loss:5.558919429779053
    W:[[-0.6117952 ]
     [ 0.7407622 ]
     [ 0.87054116]]          b:[-1.051257]   loss:4.927996635437012
    W:[[-0.70122147]
     [ 0.73551846]
     [ 0.964121  ]]          b:[-1.0603446]          loss:4.504866600036621
    W:[[-0.77442473]
     [ 0.7312137 ]
     [ 1.0407418 ]]          b:[-1.0682586]          loss:4.221070289611816
    ```

#### 예측 및 평가

* 100점, 80점, 87점일 경우를 예측

  * ```python
    print(sess.run(H, feed_dict={X:[[100, 80, 87]]}))
    ```

* 결과

  * ```
    [[69.70761]]
    ```





<br>

---



## 이진 분류(Binary Classification)



### 실습

* 공부시간, 과외시간에 따라 합/불 여부를 학습시켜보자.

#### 데이터 준비

* `X_data = [[공부시간, 과외점수]]`, `y_data = [[0(합격)]or [1(불합격)]]`

  * ```python
    X_data =[
        [1,0],
        [2,0],
        [5,1],
        [2,3],
        [3,3],
        [8,1],
        [10,0]
    ]
    
    y_data = [
        [0],
        [1],
        [0],
        [0],
        [1],
        [1],
        [1]
    ]
    ```

* `placeholder`로 데이터가 들어갈 공간만 만들어 놓는다.

  * ```python
    X = tf.placeholder(shape= [None, 2], dtype=tf.float32)
    y = tf.placeholder(shape= [None, 1], dtype=tf.float32)
    ```

  * X의 `shape`이 `[None, 2]`인 이유는 X_data의 각 요소에는 요소가 2개씩 들어가기 때문이다.

#### 가설 설정

* X와 y의 `shape`에 맞춰 W와 b를 생성

  * ```python
    W = tf.Variable(tf.random_normal([2,1]), name='weight')
    b = tf.Variable(tf.random_normal([1]), name='bias')
    ```

* `.sigmoid()` : 0 ~ 1 사이의 실수로 판단 

  * (H>0.5 : True) => 0 / 1

  * 0, 1 로 결과를 확인하고 싶기 때문에 사용

  * ```python
    logit = tf.matmul(X, W) + b
    H = tf.sigmoid(logit)
    ```

#### 준비

* loss function

  * loss 값을 미분했을 때 0이 되는 지점이 1개가 아닐 경우 사용

  * ```python
    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=y))
    ```

* optimizer

  * ```python
    learning_rate = 0.1
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train = optimizer.minimize(loss)
    ```

  * `learning_rate`는 바꿔가면서 적절한 값을 찾아줘야 한다.

* session

  * ```python
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    ```

#### 학습

* epochs(횟수)는 10000번으로 설정하고 1000번 마다 변화 확인

  * ```python
    epochs = 10000
    for step in range(epochs):
        _, loss_val, W_val, b_val = sess.run([train, loss, W, b], feed_dict={X:X_data,y:y_data})
        if step % 1000 ==0:
            print(f'W:{W_val} \t b:{b_val} \t loss:{loss_val}')
    ```

  * ```
    W:[[-0.52216065]
     [-2.4144995 ]]          b:[1.2521563]   loss:3.6504814624786377
    W:[[ 0.4028973 ]
     [-0.05373459]]          b:[-1.2234944]          loss:0.5484178066253662
    W:[[ 0.40911108]
     [-0.04438331]]          b:[-1.2662275]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422136]]          b:[-1.2669672]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422118]]          b:[-1.2669675]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422118]]          b:[-1.2669675]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422118]]          b:[-1.2669675]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422118]]          b:[-1.2669675]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422118]]          b:[-1.2669675]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422118]]          b:[-1.2669675]          loss:0.5483764410018921
    ```

#### 예측 및 평가

* X값으로 4시간 공부 + 2시간 과외받은 경우, 2시간 공부 + 4시간 과외인 경우를 넣어보자.

  * ```python
    print(sess.run(H, feed_dict={X:[[4,2],[2,4]]}))
    ```

* 결과

  * ```
    [[0.5699053 ]
     [0.34855092]]
    ```

  * sigmoid에 따라 0.5보다 크면 True이므로, 첫 번째 경우는 합격, 두 번째 경우는 불합격으로 예측하고 있다.



<br>

---

## 다중 분류(Multi-Classification)

### 실습

* 4번의 쪽지 시험을 보고 결과를 상, 중, 하로 나눌 것이다.

#### 데이터 준비

* X_data, y_data 준비

  * 상중하를 나누기 위해 라벨링을 해줘야함. True/False로 따지게 하기 위해

  * 이 형태를 **one hot encoding** 이라고 함

  * ```python
    X_data =[
        [10,7,8,3],
        [8,8,9,4],
        [7,8,2,3],
        [6,3,9,3],
        [7,6,7,5],
        [3,5,6,2],
        [2,4,3,1]
    ]
    y_data =[
        [1, 0, 0],
        [1, 0, 0],
        [0, 1, 0],
        [0, 1, 0],
        [0, 1, 0],
        [0, 0, 1],
        [0, 0, 1]
    
    ]
    ```

* `placeholder`로 데이터가 들어갈 공간을 만들자

  * ```python
    X = tf.placeholder(shape=[None, 4], dtype=tf.float32)
    y = tf.placeholder(shape=[None, 3], dtype=tf.float32)
    ```

#### 가설 설정

* W, b 설정

  * ```python
    # 4개 요소가 값으로 들어가야 하고, 상/중/하로 표현하기 위해 [4,3]
    W = tf.Variable(tf.random_normal([4,3]), name='weight')
    b = tf.Variable(tf.random_normal([3]), name='bias')
    ```

* 가설 설정

  * ```python
    logit = tf.matmul(X, W) + b
    H = tf.nn.softmax(logit)
    ```

  * `.nn.softmax()` 

#### 준비

* loss function

  * `softmax`에 맞게

  * ```python
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))
    ```

* optimizer

  * ```python
    train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)
    ```

* session

  * ```python
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    ```

#### 학습

* 3000번 반복 학습

  * ```python
    for step in range(3000):
        _, cost_val = sess.run([train, loss], feed_dict={X:X_data, y:y_data})
        if step % 300 == 0:
            print(f'cost:{cost_val}')
    ```

  * ```
    cost:9.24937915802002
    cost:1.0242544412612915
    cost:0.43915197253227234
    cost:0.34667593240737915
    cost:0.2936878502368927
    cost:0.25652530789375305
    cost:0.22818665206432343
    cost:0.20555634796619415
    cost:0.18694519996643066
    cost:0.17132094502449036
    ```

#### 예측 및 평가

* 쪽지시험 점수가 4, 9, 8, 5일 경우 수준 예측

  * ```python
    print(sess.run(H, feed_dict={X:[[4,9,8,5]]}))
    ```

* 결과

  * ```
    [[1.6871903e-05 2.9129526e-05 9.9995399e-01]]
    ```

  * 약 99%의 확률로 수준을 `하`라고 예측한다.



<br>

---

## XOR

### 실습

#### 데이터 준비

* X_data, y_data 준비

  * ```python
    X_data = [
        [0,0],
        [0,1],
        [1,0],
        [1,1]
    ]
    
    y_data = [
        [0],
        [1],
        [1],
        [0]
    ]
    ```

* `.placeholder()` 로 데이터가 들어갈 공간 만들어주기

  * ```python
    X = tf.placeholder(shpae=[None, 2], dtype=tf.float32)
    y = tf.placeholder(shape=[None, 1], dtype=tf.float32)
    ```

#### 가설 설정

* W, b 설정

  * ```python
    W = tf.Variable(tf.random_normal([2,1]), name='weight')
    b = tf.Variable(tf.random_normal([1]), name='bias')
    ```

* sigmoid 사용 가설 설정

  * ```python
    logit = tf.matmul(X, W) + b
    H = tf.sigmoid(logit)
    ```

#### 준비

* loss function

  * `nn.sigmoid_cross_entropy_with_logits()` 사용

  * ```python
    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=y))
    ```

* optimizer

  * ```python
    learning_rate = 0.1
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train = optimizer.minimize(loss)
    ```

  * loss가 최소화 되도록 설정하고, learning rate를 0.1 로 설정

* session

  * `.Session()` 으로 세션 객체 생성 후 변수 초기화

  * ```python
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    ```

#### 학습

* 학습 횟수 100000회(epochs)로 설정하고 10000회때 마다 W,b,loss 값 확인

  * ```python
    epochs = 100000
    for step in range(epochs):
        _, loss_val, W_val, b_val = sess.run([train, loss, W, b], feed_dict={X:X_data, y:y_data})
        if step % 10000 == 0:
            print(f'W:{W_val} \t b:{b_val} \t loss:{loss_val}'
    ```

  * ```
    W:[[0.35206455]
     [0.32194328]]   b:[-0.584604]   loss:0.7081718444824219
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    ```

#### 예측 및 평가

* X_data 그대로 넣어서 예측시켜보자

  * ```python
    print(sess.run(H, feed_dict={X:X_data}))
    ```

* 결과

  * ```
    [[0.49999997]
     [0.5       ]
     [0.5       ]
     [0.50000006]]
    ```

  * **XOR문제를 잘 판단하지 못했다.** 0,1,1,0이어야 하는데 0,1,1,1이 나와버렸다.

  * 이럴 때는 **딥러닝**을 사용해야 한다. 

  * 가설 설정 부분 : W, b, H를 여러 번 반복해줘야 한다.







<br><br>

<br>

---



# Deep Learning

* DL = DNN(Deep Neural Network) = MLP(Multi-Layer Perceptron)
* Perceptron
  * 인간의 신경 전달 세포(뉴런)에서 착안된 개념
  * 자극(input) -> 반응(output)
* 개념
  * y = f(x1 * w1 + x2 * w2 + b)
  * x : input
  * w : weight
  * b : bias
  * f : activation function
  * y : output
* 학습 방향
  * 순전파 
    * foward propagation
    * 입력 -> 출력 순으로 계산
  * 역전파
    * back propagation
    * 계산된 결과를 가지고 출력, 입력 순으로 가중치 변경
    * **Gradient (기울기) 를 찾아가는 과정(=학습)의 속도가 빨라짐**
* 종류
  * RNN, CNN, GAN 등 

* 딥러닝은 가설설정 부분이 많아졌다. 여러 개의 레이어를 사용(**입력층, 히든층, 출력층**)
* 한 레이어안에 여러 개의 노드에 동일한 활성화 함수를 적용시킨다.
  * 활성화 함수
    * 값마다 output이 바뀌는 함수
    * ![image](https://user-images.githubusercontent.com/75322297/156496318-3a667a5e-b22b-4636-8138-fc575960af4c.png)
    * ![활성화함수](https://user-images.githubusercontent.com/75322297/156499507-247ee1a8-cc3b-4ced-97f4-023bc8b5bdf9.png)



<br>

---



## XOR

### 실습

#### 데이터 준비

* 머신러닝에서의 XOR 데이터를 그대로 써준다.

  * ```python
    X_data = [
        [0,0],
        [0,1],
        [1,0],
        [1,1]
    ]
    
    y_data = [
        [0],
        [1],
        [1],
        [0]
    ]
    ```

* `.placeholder()`

  * ```python
    X = tf.placeholder(shape=[None, 2], dtype=tf.float32)
    y = tf.placeholder(shape=[None, 1], dtype=tf.float32)
    ```

#### 가설 설정

* 입력층 : 2개를 넣어서 10개를 내보낼 것

  * ```python
    W1 = tf.Variable(tf.random_normal([2,10]), name='weight1')
    b1 = tf.Variable(tf.random_normal([10], name='bias1'))
    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)
    ```

* 히든층 1

  * 입력층에서 내보낸 10개를 받아 20개로 내보낼 것

  * 레이어 생성 시 X가 layer1로 들어가야함

  * ```python
    W2 = tf.Variable(tf.random_normal([10, 20]), name='weight2')
    b2 = tf.Variable(tf.random_normal([20]), name='bias2')
    layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)
    ```

* 히든층 2

  * 히든층1에서 내보낸 20개를 받아 10개로 내보낼 것

  * 레이어 생성 시 X가 layer2로 들어가야함

  * ```python
    W3 = tf.Variable(tf.random_normal([20, 10]), name='weight3')
    b3 = tf.Variable(tf.random_normal([10]), name='bias3')
    layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)
    ```

    

* 출력층 

  * y값이 1개.출력을 1개 받기로 했기 떄문에 내보내는 것을 1개로 설정

  * layer3를 받아줘야 함

  * ```python
    W4 = tf.Variable(tf.random_normal([10,1]), name='weight4')
    b4 = tf.Variable(tf.random.normal([1]), name='bias4')
    logit = tf.matmul(layer3, W4) + b4
    H = tf.sigmoid(logit)
    ```

#### 준비

* loss function

  * ```python
    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=y))
    ```

* optimizer

  * ```python
    train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)
    ```

* session

  * ```python
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    ```

#### 학습

* 10000번 학습, 1000번 마다 확인

  * ```python
    for step in range(10000):
        _, loss_val = sess.run([train, loss], feed_dict={X:X_data, y:y_data})
        if step % 1000 == 0:
            print(f'loss: {loss_val}')
    ```

  * ```
    loss: 1.289193868637085
    loss: 0.5938982367515564
    loss: 0.04909735172986984
    loss: 0.015268374234437943
    loss: 0.008632145822048187
    loss: 0.0059454115107655525
    loss: 0.004510801751166582
    loss: 0.0036234448198229074
    loss: 0.0030221338383853436
    loss: 0.002588545437902212
    ```

  * loss 값이 팍팍 떨어지는 것을 확인할 수 있음(머신러닝에 비해)

#### 예측 및 평가

* 기존 데이터로 예측

  * ```python
    print(sess.run(H, feed_dict={X:X_data}))
    ```

  * ```
    [[0.00150782]
     [0.9961239 ]
     [0.99868464]
     [0.00233424]]
    ```

  * **딥러닝을 사용하니** 0, 1, 1, 0 과 똑같이 나오는 것을 확인할 수 있다.

* 정확도 평가

  * ```python
    # 0.5보다 클 경우 True가 나오도록 조건 적용(sigmoid)
    predict = tf.cast(H > 0.5, dtype=tf.float32)
    # 바꿔지는 값과 실제 y값과 비교
    correct = tf.equal(predict, y)
    # 그걸 다시 1이랑 0으로 바꿔서 평균을 내줌
    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))
    print(sess.run(accuracy, feed_dict={X:X_data, y:y_data}))
    ```

  * ```
    1.0
    ```

  * 예측값과 실제값(y)가 일치할 확률이 100%인 것으로 나타났다.





<br>

---

## Neural Network Mnist

* 위키 : [https://ko.wikipedia.org/wiki/MNIST_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4](https://ko.wikipedia.org/wiki/MNIST_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4)

* mnist 손글씨 하나의 이미지 크기는 28 * 28 

### 실습

* mnist  데이터는 0~9까지의 손글씨 데이터

* 모듈 호출

  * ```python
    import tensorflow as tf
    from tensorflow_core.examples.tutorials.mnist import input_data
    ```

#### 데이터 준비

* mnist 데이터 불러와서 X, y에 데이터가 들어갈 공간 만들기

  * ```python
    mnist = input_data.read_data_sets('data/mnist/',one_hot=True)
    
    # mnist 손글씨 하나의 이미지 크기는 28 * 28 
    X = tf.placeholder(shape = [None, 784], dtype = tf.float32)
    # 0~9 까지 숫자 손글씨 이기에 10
    y = tf.placeholder(shape = [None, 10], dtype=tf.float32 )
    ```

#### 가설 설정

* 활성화 함수 `relu` 사용

* 입력층

  * ```python
    W1 = tf.Variable(tf.random_normal([784, 256]), name='weight1')
    b1 = tf.Variable(tf.random_normal([256]), name='bias1')
    layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)
    ```

* 히든층

  * ```python
    W2 = tf.Variable(tf.random_normal([256,256]), name='weight2')
    b2 = tf.Variable(tf.random_normal([256]), name='bias2')
    layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)
    ```

* 출력층

  * ```python
    W3 = tf.Variable(tf.random_normal([256, 10]), name='weight3')
    b3 = tf.Variable(tf.random_normal([10]), name='bias3')
    
    logit = tf.matmul(layer2, W3) + b3
    H = tf.nn.relu(logit)
    ```

#### 준비

* loss function

  * ```python
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))
    ```

* optimizer

  * Adam 사용

  * ```python
    train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)
    ```

* session

  * ```python
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    ```

#### 학습

* 총 학습 횟수 30회, 학습 한번 당 배치사이즈 100으로 설정

  * ```python
    num_of_epoch = 30
    batch_size = 100
    for step in range(num_of_epoch):
        total_iter = int(mnist.train.num_examples / batch_size)
        for i in range(total_iter):
            batch_x, batch_y = mnist.train.next_batch(batch_size)
            _, loss_val = sess.run([train, loss], feed_dict={X:batch_x, y:batch_y})
            if step % 3 == 0:
                print(f'loss : {loss_val}')
    ```

#### 예측 및 평가

* ```python
  predict = tf.arg_max(H, 1)
  correct = tf.equal(predict, tf.arg_max(y, 1))
  accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))
  print(f'acc: {sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels})}')
  ```

* ```
  acc: 0.9516000151634216
  ```











































<br>

---



## 번외

* 번외로 활성화함수 그래프 그리는 것도 보여주심

  * ```python
    import numpy as np
    import matplotlib.pyplot as plt
    
    def sigmoid():
        x = np.linspace(-10, 10, 100)
        y = 1 / (1 + np.exp(-x))
    
        plt.plot(x, y, 'black', linewidth=3)
        plt.xlim(-10, 10)
        plt.ylim(-1, 2)
        plt.grid(True)
        plt.show()
    
    def softmax(x1,x2,x3):
        y = np.exp(x1) + np.exp(x2) + np.exp(x3)
        return np.exp(x1) / y, np.exp(x2) / y, np.exp(x3)
    
    
    def softmax_test():
        x1 = np.linspace(-5, 5, 20)
        x2 = np.linspace(-5, 5, 20)
        # x3 = 1로 고정
        y = np.zeros([20, 20, 3])
        for i in range(20):
            for j in range(20):
                y[i, j, :] = softmax(x1[i], x2[j], 1)
    
        m_x1, m_x2 = np.meshgrid(x1, x2)
    
        plt.figure(figsize=(8, 3))
        for i in range(2):
            ax = plt.subplot(1, 2, i+1, projection='3d')
            ax.plot_surface(m_x1, m_x2, y[:, :, i], rstride=1, cstride=1, alpha=0.3, color='blue', edgecolor='black')
            ax.set_xlabel('x1',fontsize=14)
            ax.set_ylabel('y1', fontsize=14)
            ax.view_init(40, -125)
    
        plt.show()
    
    def gauss(x, mu, sigma, a):
        return a * np.exp(-(x - mu) ** 2 / sigma ** 2)
    
    def gauss_test():
        x = np.linspace(-4, 8, 100)
    
        plt.plot(x, gauss(x, 0, 1, 1), 'red', linewidth=3, label='y=exp(-x^2)')
        plt.plot(x, gauss(x, 2, 3, 0.5), 'blue', linewidth=3, label='y=0.5exp(-(x-2)^2 / 3^2')
        plt.legend(loc='upper left')
        plt.xlim(-4, 8)
        plt.ylim(-0.5, 1.5)
        plt.grid(True)
        plt.show()
    
    
    
    
    if __name__ == '__main__':
        sigmoid()
        softmax_test()
        gauss_test()
    
    
    ```

  * sigmoid

    * ![image](https://user-images.githubusercontent.com/75322297/156510441-f5e640fb-5a01-47a7-bb25-eec0eddb8a28.png)

  * softmax

    * ![image](https://user-images.githubusercontent.com/75322297/156510505-7970b765-935d-4930-a87f-b2f42151dae7.png)

  * gauss

    * ![image](https://user-images.githubusercontent.com/75322297/156511411-33111b9e-c65f-4bf1-b268-0db813e79e7d.png)



