---
layout: posts
comments: true
title: "[ML/DL]TensorFlow ë¨¸ì‹ /ë”¥ëŸ¬ë‹ ê¸°ì´ˆ ë³µìŠµ"
categories: ML/DL
tag: [ë¨¸ì‹ ëŸ¬ë‹, Machine Learning, ML, Tensorflow, DL, ë”¥ëŸ¬ë‹]


toc: true
toc_icon: "cog"
toc_sticky: true
date: 2022-03-03
last_modified_at: 2022-03-03



---





# Tensorflow

* ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŒ¨í‚¤ì§€ì´ë‹¤!ğŸ’­

* í•™ìŠµ = hì™€ yë¥¼ ë¹„êµí•˜ë©° loss functionì„ ì‚¬ìš©í•´ì„œ Weightì™€ Biasê°€ ë³€ê²½ë˜ëŠ” ê³¼ì •ì´ë©°, ì´ ì¼ì„ í•´ì£¼ëŠ” ê²ƒì´ optimizerë‹¤.
* `Tensor` : ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ê°ì²´(placeholder)
* `Variable` : Weight, Bias
* `Operation` : H = W * X + b (node, ì‹) -> ê·¸ë˜í”„ (tensor -> operation -> tensor -> operation -> ,,,)
* `Session` : ì‹¤í–‰ í™˜ê²½

## í™˜ê²½ ì„¤ì •

* ì•„ë‚˜ì½˜ë‹¤ ê°€ìƒí™˜ê²½ì„ ì‚¬ìš©í•  ê²ƒì´ë¯€ë¡œ, ì•„ë‚˜ì½˜ë‹¤ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.

* ê°€ìƒí™˜ê²½ ìƒì„±
  * `conda create -n ai02 python=3.7 -y`
  * í…ì„œí”Œë¡œìš° 1.15versionì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ íŒŒì´ì¬ 3.7ë²„ì „ì„ ì„¤ì¹˜
* VSCì—ì„œ `Ctrl+Shift+P`ë¡œ í•´ë‹¹ ê°€ìƒí™˜ê²½ í„°ë¯¸ë„ ë„ìš°ê¸°
  * ![image](https://user-images.githubusercontent.com/75322297/156470845-925a7808-56f9-45cd-9819-9db4657b29fa.png)
* í…ì„œí”Œë¡œìš° 1.15ë²„ì „ ì„¤ì¹˜
  * `conda install tensorflow==1.15 -y`
  * `pip install tensorflow==1.15`



### ê¸°ë³¸ ì‘ë™ ë°©ë²•ì„ ì•Œì•„ë³´ì!ğŸ˜±

* `X, y -> W * X + b <-(optimizer)-> (H, y) : loss function`

* ëª¨ë“ˆ í˜¸ì¶œ

  * `import tensorflow as tf`

* ìƒìˆ˜ ë…¸ë“œë¥¼ ìƒì„±

  * ```python
    node = tf.constant(100)
    ```

* `session` ì‚¬ìš©

  * `session`ì€ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰ì‹œì¼œì£¼ëŠ” ì—­í• (runner)ì´ë‹¤.

  * `.run()`ì„ í†µí•´ ì‹¤í–‰

  * ```python
    sess = tf.Session()
    
    print(sess.run(node))
    ```

* ê²°ê³¼

  * ```
    100
    ```

* `tf.constant()` ëŠ” ë…¸ë“œì— ìƒìˆ˜ê°’ì„ ë„£ì–´ì£¼ëŠ” ê¸°ëŠ¥ì„ í•œë‹¤.

  * ```python
    node1 = tf.constant(10, dtype=tf.float32) # 10 ì…ë ¥
    node2 = tf.constant(20, dtype=tf.float32) # 20 ì…ë ¥
    node3 = node1 + node2
    ```

  * ì—¬ê¸°ì„œ, `dtype`ì€ í…ì„œí”Œë¡œìš° ë‚´ë¶€ì ìœ¼ë¡œ `numpy`ë¥¼ ì‚¬ìš© ì¤‘ì´ì—¬ì„œ ë°ì´í„° íƒ€ì…ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.

* `.Session()` ì‚¬ìš©

  * ```python
    sess = tf.Session()
    print(sess.run(node3))  # node1 + node2 = 10.0 + 20.0
    
    print(sess.run([node1, node3])) [10.0, 30.0]
    ```

* ê²°ê³¼

  * ```
    30.0
    [10.0, 30.0]
    ```

* ê·¸ë¦¬ê³  ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰ ì‹œì— ì´ëŸ¬í•œ ì•Œë¦¼ì´ ëœ¬ë‹¤.

  * ```python
    WARNING:tensorflow:From 02_tensor.py:13: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.
    ```

  * ì´ëŸ´ ë•ŒëŠ” importë¥¼ ì´ë ‡ê²Œ í•˜ë©´ ëœë‹¤.

  * ```python
    import tensorflow.compat.v1 as tf
    ```

* `placeholder` : ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ëŠ” ì‹œì ì— ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì„ ìˆ˜ ìˆë„ë¡ ê³µê°„ë§Œ ë§Œë“¤ì–´ ë†“ìŒ

  * ```python
    node1 = tf.placeholder(dtype=tf.float32)
    node2 = tf.placeholder(dtype=tf.float32)
    
    node3 = node1 + node2
    # ê°’ì€ ë°›ì§€ ì•Šê³  ë¯¸ë¦¬ ë…¸ë“œë“¤ì„ ë”í•´ë†“ëŠ”ë‹¤.
    ```

* `.Session()` ê°ì²´ ìƒì„±

  * ```python
    sess = tf.Session()
    ```

* ë§Œë“¤ì–´ ë†“ì€ ê³µê°„ì— data ì…ë ¥

  * ```python
    X = [10, 20, 30]
    y = [40, 50, 60]
    ```

* ì—¬ê¸°ê°€ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ëŠ” ì‹œì 

  * `feed_dict`ë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ì…ë ¥í•´ì¤Œ

  * ```python
    print(sess.run(node3, feed_dict={node1:X,node2:y}))
    ```

* ê²°ê³¼

  * ```python
    [50. 70. 90.]
    ```





<br>

---



## ì„ í˜• íšŒê·€(Linear-Regression)

* í…ì„œí”Œë¡œìš°ëŠ” `sklearn`ì„ ì‚¬ìš©í•  ë•Œì™€ ê³¼ì •ì´ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤.
  1. ë°ì´í„° ì¤€ë¹„
  2. ê°€ì„¤ ì„¤ì •
  3. ì¤€ë¹„
  4. í•™ìŠµ
  5. ì˜ˆì¸¡
* **ë°ì´í„° ë¶„í• ** ë¶€ë¶„ì´ **ê°€ì„¤ ì„¤ì •**ìœ¼ë¡œ ë°”ë€Œì—ˆë‹¤.



### ì‹¤ìŠµ

#### ë°ì´í„° ì¤€ë¹„

* ```python
  X = tf.placeholder(tf.float32)
  y = tf.placeholder(tf.float32)
  ```

* Xì™€ yì— ì‹¤ìˆ˜í˜•ì´ ë“¤ì–´ê°ˆ ê³µê°„ì„ ë§Œë“¤ì–´ ì¤Œ

#### ê°€ì„¤ ì„¤ì •

* **H(hypothesis) = W (weight) * X + b (bias)**

* ```python
  # Weight ìƒì„±
  W = tf.Variable(tf.random_normal([1]), name='weight')
  # Bias ìƒì„±
  b = tf.Variable(tf.random_normal([1]), name='bias')
  
  H = W * X + b
  ```

* `.random_normal()` ì€ ëœë¤ìœ¼ë¡œ í‘œì¤€ë¶„í¬ ê°’ì„ ì„¤ì •í•´ì£¼ëŠ” ë†ˆ

#### ì¤€ë¹„

* ì´ ë‹¨ê³„ì—ì„œëŠ” loss function, optimizer, sessionì´ ì¤€ë¹„ë˜ì–´ì•¼ í•œë‹¤.

* loss function

  * MSE(Mean Square Error)

  * ```python
    loss = tf.reduce_mean(tf.square(H - y))
    ```

* optimizer

  * ê²½ì‚¬ í•˜ê°•ë²•(gradient descent) : lossê°€ ìµœì†Œí™” ë˜ëŠ” ê°’ì„ ì°¾ê¸°

  * ```python
    optimizer = tf.train.GradientDescentOptimizer(0.01)
    ```

  * ì—¬ê¸°ì„œ `0.01`ì€ `learning rate`ë¡œ ì–¼ë§ˆí¼ì”© ì›€ì§ì¼ ê±´ì§€ ì•Œë ¤ì¤€ë‹¤.

  * ```python
    train = optimizer.minimize(loss)
    ```

  * lossê°€ ìµœì†Œí™” ë˜ë„ë¡ ìµœì í™”

* session

  * `.Session()` ê°ì²´ ìƒì„±

  * ```python
    sess = tf.Session()
    ```

  * ì—¬ê¸°ì„œ ê·¸ë˜í”„ì— ìˆëŠ” ëª¨ë“  ë³€ìˆ˜ë“¤ì„ ì´ˆê¸°í™”í•´ì¤˜ì•¼ í•œë‹¤. ì´ë•Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ `tf.global_variables_initializer()`

  * ```python
    sess.run(tf.global_variables_initializer())
    ```

#### í•™ìŠµ

* ê³¼ì í•©(overfitting)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì ì ˆí•œ í•™ìŠµ íšŸìˆ˜ë¥¼ ì„ ì •í•´ì•¼í•œë‹¤.

* ```python
  epochs = 5000
  for step in range(epochs):
      tmp, loss_val, W_val, b_val = sess.run([train, loss, W, b], feed_dict={X:[1,2,3,4,5],y:[3,5,7,9,11]})
      if step % 100 == 0:
          print(f'W:{W_val} \t b:{b_val} \t loss:{loss_val}')
  ```

* `loss_val, W_val, b_val`ì„ ì‹¤í–‰ì‹œí‚¤ëŠ” ì´ìœ ëŠ” ë³€í•˜ëŠ” ê³¼ì •ì„ í™•ì¸í•˜ê¸° ìœ„í•¨

* `step % 1000 == 0` ì€ ë°˜ë³µë¬¸ 1000ë²ˆë§ˆë‹¤ í™•ì¸í•˜ê¸° ìœ„í•¨

* ê²°ê³¼

  * ```
    W:[-0.07149309]          b:[1.2598735]   loss:75.4944076538086
    W:[1.9927471]    b:[1.0261854]   loss:0.0001256559626199305
    W:[1.9997544]    b:[1.0008862]   loss:1.4397728875792382e-07
    W:[1.9999908]    b:[1.0000327]   loss:2.0108928411310956e-10
    W:[1.999994]     b:[1.0000207]   loss:7.644303245957218e-11
    ```

#### ì˜ˆì¸¡

* ê°€ì„¤ Hì™€ ë°ì´í„°ë¥¼ ì…ë ¥í•´ì£¼ê³  ì‹¤í–‰

  * ```python
    print(sess.run(H, feed_dict={X:[10,11,12,13,14]}))
    ```

* ê²°ê³¼

  * ```
    [20.999962 22.999956 24.999949 26.999943 28.999937]
    ```



## ë‹¤ì¤‘ ì„ í˜•íšŒê·€(Multi-Linear Regrression)

### ì‹¤ìŠµ

#### ë°ì´í„° ì¤€ë¹„

* X_dataëŠ” ì„¸ ë²ˆì˜ ëª¨ì˜ê³ ì‚¬, y_dataëŠ” ì‹¤ì œ ì‹œí—˜ ì ìˆ˜

  * ```python
    X_data = [
        [73, 80, 75],
        [93, 88, 93],
        [89, 91, 90],
        [96, 89, 100],
        [73, 66, 70]
    ]
    
    y_data = [
        [80],
        [91],
        [88],
        [94],
        [61]
    ]
    ```

* `placeholder`ë¡œ ë…¸ë“œì— ë°ì´í„°ê°€ ë“¤ì–´ê°ˆ ê³µê°„ ë§Œë“¤ì–´ì£¼ê¸°

  * ```python
    X = tf.placeholder(shape=[None,3], dtype=tf.float32)
    y = tf.placeholder(shape=[None,1], dtype=tf.float32)
    ```

  * ë°ì´í„° íƒ€ì…ì€ ì‹¤ìˆ˜í˜•

  * ì—¬ê¸°ì„œ `shape`ì€ ì•ˆì— ë“¤ì–´ê°ˆ í˜•íƒœë¥¼ ì •í•´ì¤€ë‹¤.

    * ì˜ˆì‹œ
    * shape = [5, 3] -> 5ê°œ ë¦¬ìŠ¤íŠ¸ ì•ˆì— 3ê°œì˜ ìš”ì†Œ
    * shapeì´ None : ê°¯ìˆ˜ ìƒê´€ ì—†ë‹¤ëŠ” ëœ»

#### ê°€ì„¤ ì„¤ì •

* Xì™€ Wê°€ í–‰ë ¬ ì—°ì‚°í•  ë•Œ ì—´ê³¼ í–‰ì´ ê°™ì•„ì•¼ í•œë‹¤.

* Xì˜ í˜•íƒœê°€ `[None, 3]`ì´ë¯€ë¡œ Wì— `[3,1]`ë¡œ ë“¤ì–´ê°€ì•¼ í•œë‹¤.

  * ```python
    W = tf.Variable(tf.random_normal([3,1]), name='weight')
    b = tf.Variable(tf.random_normal([1]), name='bias')
    ```

* **H = W * X + b**

  * ```python
    H = tf.matmul(X, W) + b
    ```

#### ì¤€ë¹„

* loss function

  * ```python
    loss = tf.reduce_mean(tf.square(H - y))
    ```

* optimizer

  * ```python
    learning_rate = 0.00004
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train = optimizer.minimize(loss)
    ```

* session

  * ```python
    sess = tf.Session()
    # ë³€ìˆ˜ì´ˆê¸°í™” ì§„í–‰
    sess.run(tf.global_variables_initializer())
    ```

#### í•™ìŠµ

* ë¡œìŠ¤ë¥¼ ì¤„ì—¬ë‚˜ê°€ëŠ” ê³¼ì •ì„ í™•ì¸í•˜ë©° í•™ìŠµ

  * ```python
    epochs = 10000
    for step in range(epochs):
        _, loss_val, W_val, b_val = sess.run([train, loss, W, b], feed_dict={X:X_data, y:y_data})
        if step % 1000 == 0:
            print(f'W:{W_val} \t b:{b_val} \t loss:{loss_val}')
    ```

* í™•ì¸

  * ```
    W:[[2.2694695 ]
     [0.02989042]
     [0.5611059 ]]   b:[-0.9297394]          loss:47803.86328125
    W:[[ 0.697498  ]
     [ 0.49425066]
     [-0.18740939]]          b:[-0.9556813]          loss:20.511674880981445
    W:[[0.27228674]
     [0.7211382 ]
     [0.01448941]]   b:[-0.97749513]         loss:13.250178337097168
    W:[[0.00078062]
     [0.76103204]
     [0.24469697]]   b:[-0.9971895]          loss:10.001592636108398
    W:[[-0.20451084]
     [ 0.7613147 ]
     [ 0.44759852]]          b:[-1.0140916]          loss:7.903079986572266
    W:[[-0.3688228 ]
     [ 0.75436056]
     [ 0.6169308 ]]          b:[-1.0284728]          loss:6.4997758865356445
    W:[[-0.50251806]
     [ 0.7470755 ]
     [ 0.75628585]]          b:[-1.0407374]          loss:5.558919429779053
    W:[[-0.6117952 ]
     [ 0.7407622 ]
     [ 0.87054116]]          b:[-1.051257]   loss:4.927996635437012
    W:[[-0.70122147]
     [ 0.73551846]
     [ 0.964121  ]]          b:[-1.0603446]          loss:4.504866600036621
    W:[[-0.77442473]
     [ 0.7312137 ]
     [ 1.0407418 ]]          b:[-1.0682586]          loss:4.221070289611816
    ```

#### ì˜ˆì¸¡ ë° í‰ê°€

* 100ì , 80ì , 87ì ì¼ ê²½ìš°ë¥¼ ì˜ˆì¸¡

  * ```python
    print(sess.run(H, feed_dict={X:[[100, 80, 87]]}))
    ```

* ê²°ê³¼

  * ```
    [[69.70761]]
    ```





<br>

---



## ì´ì§„ ë¶„ë¥˜(Binary Classification)



### ì‹¤ìŠµ

* ê³µë¶€ì‹œê°„, ê³¼ì™¸ì‹œê°„ì— ë”°ë¼ í•©/ë¶ˆ ì—¬ë¶€ë¥¼ í•™ìŠµì‹œì¼œë³´ì.

#### ë°ì´í„° ì¤€ë¹„

* `X_data = [[ê³µë¶€ì‹œê°„, ê³¼ì™¸ì ìˆ˜]]`, `y_data = [[0(í•©ê²©)]or [1(ë¶ˆí•©ê²©)]]`

  * ```python
    X_data =[
        [1,0],
        [2,0],
        [5,1],
        [2,3],
        [3,3],
        [8,1],
        [10,0]
    ]
    
    y_data = [
        [0],
        [1],
        [0],
        [0],
        [1],
        [1],
        [1]
    ]
    ```

* `placeholder`ë¡œ ë°ì´í„°ê°€ ë“¤ì–´ê°ˆ ê³µê°„ë§Œ ë§Œë“¤ì–´ ë†“ëŠ”ë‹¤.

  * ```python
    X = tf.placeholder(shape= [None, 2], dtype=tf.float32)
    y = tf.placeholder(shape= [None, 1], dtype=tf.float32)
    ```

  * Xì˜ `shape`ì´ `[None, 2]`ì¸ ì´ìœ ëŠ” X_dataì˜ ê° ìš”ì†Œì—ëŠ” ìš”ì†Œê°€ 2ê°œì”© ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì´ë‹¤.

#### ê°€ì„¤ ì„¤ì •

* Xì™€ yì˜ `shape`ì— ë§ì¶° Wì™€ bë¥¼ ìƒì„±

  * ```python
    W = tf.Variable(tf.random_normal([2,1]), name='weight')
    b = tf.Variable(tf.random_normal([1]), name='bias')
    ```

* `.sigmoid()` : 0 ~ 1 ì‚¬ì´ì˜ ì‹¤ìˆ˜ë¡œ íŒë‹¨ 

  * (H>0.5 : True) => 0 / 1

  * 0, 1 ë¡œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  ì‹¶ê¸° ë•Œë¬¸ì— ì‚¬ìš©

  * ```python
    logit = tf.matmul(X, W) + b
    H = tf.sigmoid(logit)
    ```

#### ì¤€ë¹„

* loss function

  * loss ê°’ì„ ë¯¸ë¶„í–ˆì„ ë•Œ 0ì´ ë˜ëŠ” ì§€ì ì´ 1ê°œê°€ ì•„ë‹ ê²½ìš° ì‚¬ìš©

  * ```python
    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=y))
    ```

* optimizer

  * ```python
    learning_rate = 0.1
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train = optimizer.minimize(loss)
    ```

  * `learning_rate`ëŠ” ë°”ê¿”ê°€ë©´ì„œ ì ì ˆí•œ ê°’ì„ ì°¾ì•„ì¤˜ì•¼ í•œë‹¤.

* session

  * ```python
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    ```

#### í•™ìŠµ

* epochs(íšŸìˆ˜)ëŠ” 10000ë²ˆìœ¼ë¡œ ì„¤ì •í•˜ê³  1000ë²ˆ ë§ˆë‹¤ ë³€í™” í™•ì¸

  * ```python
    epochs = 10000
    for step in range(epochs):
        _, loss_val, W_val, b_val = sess.run([train, loss, W, b], feed_dict={X:X_data,y:y_data})
        if step % 1000 ==0:
            print(f'W:{W_val} \t b:{b_val} \t loss:{loss_val}')
    ```

  * ```
    W:[[-0.52216065]
     [-2.4144995 ]]          b:[1.2521563]   loss:3.6504814624786377
    W:[[ 0.4028973 ]
     [-0.05373459]]          b:[-1.2234944]          loss:0.5484178066253662
    W:[[ 0.40911108]
     [-0.04438331]]          b:[-1.2662275]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422136]]          b:[-1.2669672]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422118]]          b:[-1.2669675]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422118]]          b:[-1.2669675]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422118]]          b:[-1.2669675]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422118]]          b:[-1.2669675]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422118]]          b:[-1.2669675]          loss:0.5483764410018921
    W:[[ 0.40921867]
     [-0.04422118]]          b:[-1.2669675]          loss:0.5483764410018921
    ```

#### ì˜ˆì¸¡ ë° í‰ê°€

* Xê°’ìœ¼ë¡œ 4ì‹œê°„ ê³µë¶€ + 2ì‹œê°„ ê³¼ì™¸ë°›ì€ ê²½ìš°, 2ì‹œê°„ ê³µë¶€ + 4ì‹œê°„ ê³¼ì™¸ì¸ ê²½ìš°ë¥¼ ë„£ì–´ë³´ì.

  * ```python
    print(sess.run(H, feed_dict={X:[[4,2],[2,4]]}))
    ```

* ê²°ê³¼

  * ```
    [[0.5699053 ]
     [0.34855092]]
    ```

  * sigmoidì— ë”°ë¼ 0.5ë³´ë‹¤ í¬ë©´ Trueì´ë¯€ë¡œ, ì²« ë²ˆì§¸ ê²½ìš°ëŠ” í•©ê²©, ë‘ ë²ˆì§¸ ê²½ìš°ëŠ” ë¶ˆí•©ê²©ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³  ìˆë‹¤.



<br>

---

## ë‹¤ì¤‘ ë¶„ë¥˜(Multi-Classification)

### ì‹¤ìŠµ

* 4ë²ˆì˜ ìª½ì§€ ì‹œí—˜ì„ ë³´ê³  ê²°ê³¼ë¥¼ ìƒ, ì¤‘, í•˜ë¡œ ë‚˜ëˆŒ ê²ƒì´ë‹¤.

#### ë°ì´í„° ì¤€ë¹„

* X_data, y_data ì¤€ë¹„

  * ìƒì¤‘í•˜ë¥¼ ë‚˜ëˆ„ê¸° ìœ„í•´ ë¼ë²¨ë§ì„ í•´ì¤˜ì•¼í•¨. True/Falseë¡œ ë”°ì§€ê²Œ í•˜ê¸° ìœ„í•´

  * ì´ í˜•íƒœë¥¼ **one hot encoding** ì´ë¼ê³  í•¨

  * ```python
    X_data =[
        [10,7,8,3],
        [8,8,9,4],
        [7,8,2,3],
        [6,3,9,3],
        [7,6,7,5],
        [3,5,6,2],
        [2,4,3,1]
    ]
    y_data =[
        [1, 0, 0],
        [1, 0, 0],
        [0, 1, 0],
        [0, 1, 0],
        [0, 1, 0],
        [0, 0, 1],
        [0, 0, 1]
    
    ]
    ```

* `placeholder`ë¡œ ë°ì´í„°ê°€ ë“¤ì–´ê°ˆ ê³µê°„ì„ ë§Œë“¤ì

  * ```python
    X = tf.placeholder(shape=[None, 4], dtype=tf.float32)
    y = tf.placeholder(shape=[None, 3], dtype=tf.float32)
    ```

#### ê°€ì„¤ ì„¤ì •

* W, b ì„¤ì •

  * ```python
    # 4ê°œ ìš”ì†Œê°€ ê°’ìœ¼ë¡œ ë“¤ì–´ê°€ì•¼ í•˜ê³ , ìƒ/ì¤‘/í•˜ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ [4,3]
    W = tf.Variable(tf.random_normal([4,3]), name='weight')
    b = tf.Variable(tf.random_normal([3]), name='bias')
    ```

* ê°€ì„¤ ì„¤ì •

  * ```python
    logit = tf.matmul(X, W) + b
    H = tf.nn.softmax(logit)
    ```

  * `.nn.softmax()` 

#### ì¤€ë¹„

* loss function

  * `softmax`ì— ë§ê²Œ

  * ```python
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))
    ```

* optimizer

  * ```python
    train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)
    ```

* session

  * ```python
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    ```

#### í•™ìŠµ

* 3000ë²ˆ ë°˜ë³µ í•™ìŠµ

  * ```python
    for step in range(3000):
        _, cost_val = sess.run([train, loss], feed_dict={X:X_data, y:y_data})
        if step % 300 == 0:
            print(f'cost:{cost_val}')
    ```

  * ```
    cost:9.24937915802002
    cost:1.0242544412612915
    cost:0.43915197253227234
    cost:0.34667593240737915
    cost:0.2936878502368927
    cost:0.25652530789375305
    cost:0.22818665206432343
    cost:0.20555634796619415
    cost:0.18694519996643066
    cost:0.17132094502449036
    ```

#### ì˜ˆì¸¡ ë° í‰ê°€

* ìª½ì§€ì‹œí—˜ ì ìˆ˜ê°€ 4, 9, 8, 5ì¼ ê²½ìš° ìˆ˜ì¤€ ì˜ˆì¸¡

  * ```python
    print(sess.run(H, feed_dict={X:[[4,9,8,5]]}))
    ```

* ê²°ê³¼

  * ```
    [[1.6871903e-05 2.9129526e-05 9.9995399e-01]]
    ```

  * ì•½ 99%ì˜ í™•ë¥ ë¡œ ìˆ˜ì¤€ì„ `í•˜`ë¼ê³  ì˜ˆì¸¡í•œë‹¤.



<br>

---

## XOR

### ì‹¤ìŠµ

#### ë°ì´í„° ì¤€ë¹„

* X_data, y_data ì¤€ë¹„

  * ```python
    X_data = [
        [0,0],
        [0,1],
        [1,0],
        [1,1]
    ]
    
    y_data = [
        [0],
        [1],
        [1],
        [0]
    ]
    ```

* `.placeholder()` ë¡œ ë°ì´í„°ê°€ ë“¤ì–´ê°ˆ ê³µê°„ ë§Œë“¤ì–´ì£¼ê¸°

  * ```python
    X = tf.placeholder(shpae=[None, 2], dtype=tf.float32)
    y = tf.placeholder(shape=[None, 1], dtype=tf.float32)
    ```

#### ê°€ì„¤ ì„¤ì •

* W, b ì„¤ì •

  * ```python
    W = tf.Variable(tf.random_normal([2,1]), name='weight')
    b = tf.Variable(tf.random_normal([1]), name='bias')
    ```

* sigmoid ì‚¬ìš© ê°€ì„¤ ì„¤ì •

  * ```python
    logit = tf.matmul(X, W) + b
    H = tf.sigmoid(logit)
    ```

#### ì¤€ë¹„

* loss function

  * `nn.sigmoid_cross_entropy_with_logits()` ì‚¬ìš©

  * ```python
    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=y))
    ```

* optimizer

  * ```python
    learning_rate = 0.1
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train = optimizer.minimize(loss)
    ```

  * lossê°€ ìµœì†Œí™” ë˜ë„ë¡ ì„¤ì •í•˜ê³ , learning rateë¥¼ 0.1 ë¡œ ì„¤ì •

* session

  * `.Session()` ìœ¼ë¡œ ì„¸ì…˜ ê°ì²´ ìƒì„± í›„ ë³€ìˆ˜ ì´ˆê¸°í™”

  * ```python
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    ```

#### í•™ìŠµ

* í•™ìŠµ íšŸìˆ˜ 100000íšŒ(epochs)ë¡œ ì„¤ì •í•˜ê³  10000íšŒë•Œ ë§ˆë‹¤ W,b,loss ê°’ í™•ì¸

  * ```python
    epochs = 100000
    for step in range(epochs):
        _, loss_val, W_val, b_val = sess.run([train, loss, W, b], feed_dict={X:X_data, y:y_data})
        if step % 10000 == 0:
            print(f'W:{W_val} \t b:{b_val} \t loss:{loss_val}'
    ```

  * ```
    W:[[0.35206455]
     [0.32194328]]   b:[-0.584604]   loss:0.7081718444824219
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    W:[[1.7762821e-07]
     [1.7756454e-07]]        b:[-1.4846731e-07]      loss:0.6931471824645996
    ```

#### ì˜ˆì¸¡ ë° í‰ê°€

* X_data ê·¸ëŒ€ë¡œ ë„£ì–´ì„œ ì˜ˆì¸¡ì‹œì¼œë³´ì

  * ```python
    print(sess.run(H, feed_dict={X:X_data}))
    ```

* ê²°ê³¼

  * ```
    [[0.49999997]
     [0.5       ]
     [0.5       ]
     [0.50000006]]
    ```

  * **XORë¬¸ì œë¥¼ ì˜ íŒë‹¨í•˜ì§€ ëª»í–ˆë‹¤.** 0,1,1,0ì´ì–´ì•¼ í•˜ëŠ”ë° 0,1,1,1ì´ ë‚˜ì™€ë²„ë ¸ë‹¤.

  * ì´ëŸ´ ë•ŒëŠ” **ë”¥ëŸ¬ë‹**ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤. 

  * ê°€ì„¤ ì„¤ì • ë¶€ë¶„ : W, b, Hë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•´ì¤˜ì•¼ í•œë‹¤.







<br><br>

<br>

---



# Deep Learning

* DL = DNN(Deep Neural Network) = MLP(Multi-Layer Perceptron)
* Perceptron
  * ì¸ê°„ì˜ ì‹ ê²½ ì „ë‹¬ ì„¸í¬(ë‰´ëŸ°)ì—ì„œ ì°©ì•ˆëœ ê°œë…
  * ìê·¹(input) -> ë°˜ì‘(output)
* ê°œë…
  * y = f(x1 * w1 + x2 * w2 + b)
  * x : input
  * w : weight
  * b : bias
  * f : activation function
  * y : output
* í•™ìŠµ ë°©í–¥
  * ìˆœì „íŒŒ 
    * foward propagation
    * ì…ë ¥ -> ì¶œë ¥ ìˆœìœ¼ë¡œ ê³„ì‚°
  * ì—­ì „íŒŒ
    * back propagation
    * ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ê°€ì§€ê³  ì¶œë ¥, ì…ë ¥ ìˆœìœ¼ë¡œ ê°€ì¤‘ì¹˜ ë³€ê²½
    * **Gradient (ê¸°ìš¸ê¸°) ë¥¼ ì°¾ì•„ê°€ëŠ” ê³¼ì •(=í•™ìŠµ)ì˜ ì†ë„ê°€ ë¹¨ë¼ì§**
* ì¢…ë¥˜
  * RNN, CNN, GAN ë“± 

* ë”¥ëŸ¬ë‹ì€ ê°€ì„¤ì„¤ì • ë¶€ë¶„ì´ ë§ì•„ì¡Œë‹¤. ì—¬ëŸ¬ ê°œì˜ ë ˆì´ì–´ë¥¼ ì‚¬ìš©(**ì…ë ¥ì¸µ, íˆë“ ì¸µ, ì¶œë ¥ì¸µ**)
* í•œ ë ˆì´ì–´ì•ˆì— ì—¬ëŸ¬ ê°œì˜ ë…¸ë“œì— ë™ì¼í•œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©ì‹œí‚¨ë‹¤.
  * í™œì„±í™” í•¨ìˆ˜
    * ê°’ë§ˆë‹¤ outputì´ ë°”ë€ŒëŠ” í•¨ìˆ˜
    * ![image](https://user-images.githubusercontent.com/75322297/156496318-3a667a5e-b22b-4636-8138-fc575960af4c.png)
    * ![á„’á…ªá†¯á„‰á…¥á†¼á„’á…ªá„’á…¡á†·á„‰á…®](https://user-images.githubusercontent.com/75322297/156499507-247ee1a8-cc3b-4ced-97f4-023bc8b5bdf9.png)



<br>

---



## XOR

### ì‹¤ìŠµ

#### ë°ì´í„° ì¤€ë¹„

* ë¨¸ì‹ ëŸ¬ë‹ì—ì„œì˜ XOR ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì¨ì¤€ë‹¤.

  * ```python
    X_data = [
        [0,0],
        [0,1],
        [1,0],
        [1,1]
    ]
    
    y_data = [
        [0],
        [1],
        [1],
        [0]
    ]
    ```

* `.placeholder()`

  * ```python
    X = tf.placeholder(shape=[None, 2], dtype=tf.float32)
    y = tf.placeholder(shape=[None, 1], dtype=tf.float32)
    ```

#### ê°€ì„¤ ì„¤ì •

* ì…ë ¥ì¸µ : 2ê°œë¥¼ ë„£ì–´ì„œ 10ê°œë¥¼ ë‚´ë³´ë‚¼ ê²ƒ

  * ```python
    W1 = tf.Variable(tf.random_normal([2,10]), name='weight1')
    b1 = tf.Variable(tf.random_normal([10], name='bias1'))
    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)
    ```

* íˆë“ ì¸µ 1

  * ì…ë ¥ì¸µì—ì„œ ë‚´ë³´ë‚¸ 10ê°œë¥¼ ë°›ì•„ 20ê°œë¡œ ë‚´ë³´ë‚¼ ê²ƒ

  * ë ˆì´ì–´ ìƒì„± ì‹œ Xê°€ layer1ë¡œ ë“¤ì–´ê°€ì•¼í•¨

  * ```python
    W2 = tf.Variable(tf.random_normal([10, 20]), name='weight2')
    b2 = tf.Variable(tf.random_normal([20]), name='bias2')
    layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)
    ```

* íˆë“ ì¸µ 2

  * íˆë“ ì¸µ1ì—ì„œ ë‚´ë³´ë‚¸ 20ê°œë¥¼ ë°›ì•„ 10ê°œë¡œ ë‚´ë³´ë‚¼ ê²ƒ

  * ë ˆì´ì–´ ìƒì„± ì‹œ Xê°€ layer2ë¡œ ë“¤ì–´ê°€ì•¼í•¨

  * ```python
    W3 = tf.Variable(tf.random_normal([20, 10]), name='weight3')
    b3 = tf.Variable(tf.random_normal([10]), name='bias3')
    layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)
    ```

    

* ì¶œë ¥ì¸µ 

  * yê°’ì´ 1ê°œ.ì¶œë ¥ì„ 1ê°œ ë°›ê¸°ë¡œ í–ˆê¸° ë–„ë¬¸ì— ë‚´ë³´ë‚´ëŠ” ê²ƒì„ 1ê°œë¡œ ì„¤ì •

  * layer3ë¥¼ ë°›ì•„ì¤˜ì•¼ í•¨

  * ```python
    W4 = tf.Variable(tf.random_normal([10,1]), name='weight4')
    b4 = tf.Variable(tf.random.normal([1]), name='bias4')
    logit = tf.matmul(layer3, W4) + b4
    H = tf.sigmoid(logit)
    ```

#### ì¤€ë¹„

* loss function

  * ```python
    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=y))
    ```

* optimizer

  * ```python
    train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)
    ```

* session

  * ```python
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    ```

#### í•™ìŠµ

* 10000ë²ˆ í•™ìŠµ, 1000ë²ˆ ë§ˆë‹¤ í™•ì¸

  * ```python
    for step in range(10000):
        _, loss_val = sess.run([train, loss], feed_dict={X:X_data, y:y_data})
        if step % 1000 == 0:
            print(f'loss: {loss_val}')
    ```

  * ```
    loss: 1.289193868637085
    loss: 0.5938982367515564
    loss: 0.04909735172986984
    loss: 0.015268374234437943
    loss: 0.008632145822048187
    loss: 0.0059454115107655525
    loss: 0.004510801751166582
    loss: 0.0036234448198229074
    loss: 0.0030221338383853436
    loss: 0.002588545437902212
    ```

  * loss ê°’ì´ íŒíŒ ë–¨ì–´ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ(ë¨¸ì‹ ëŸ¬ë‹ì— ë¹„í•´)

#### ì˜ˆì¸¡ ë° í‰ê°€

* ê¸°ì¡´ ë°ì´í„°ë¡œ ì˜ˆì¸¡

  * ```python
    print(sess.run(H, feed_dict={X:X_data}))
    ```

  * ```
    [[0.00150782]
     [0.9961239 ]
     [0.99868464]
     [0.00233424]]
    ```

  * **ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ë‹ˆ** 0, 1, 1, 0 ê³¼ ë˜‘ê°™ì´ ë‚˜ì˜¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

* ì •í™•ë„ í‰ê°€

  * ```python
    # 0.5ë³´ë‹¤ í´ ê²½ìš° Trueê°€ ë‚˜ì˜¤ë„ë¡ ì¡°ê±´ ì ìš©(sigmoid)
    predict = tf.cast(H > 0.5, dtype=tf.float32)
    # ë°”ê¿”ì§€ëŠ” ê°’ê³¼ ì‹¤ì œ yê°’ê³¼ ë¹„êµ
    correct = tf.equal(predict, y)
    # ê·¸ê±¸ ë‹¤ì‹œ 1ì´ë‘ 0ìœ¼ë¡œ ë°”ê¿”ì„œ í‰ê· ì„ ë‚´ì¤Œ
    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))
    print(sess.run(accuracy, feed_dict={X:X_data, y:y_data}))
    ```

  * ```
    1.0
    ```

  * ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’(y)ê°€ ì¼ì¹˜í•  í™•ë¥ ì´ 100%ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.





<br>

---

## Neural Network Mnist

* ìœ„í‚¤ : [https://ko.wikipedia.org/wiki/MNIST_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4](https://ko.wikipedia.org/wiki/MNIST_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4)

* mnist ì†ê¸€ì”¨ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ í¬ê¸°ëŠ” 28 * 28 

### ì‹¤ìŠµ

* mnist  ë°ì´í„°ëŠ” 0~9ê¹Œì§€ì˜ ì†ê¸€ì”¨ ë°ì´í„°

* ëª¨ë“ˆ í˜¸ì¶œ

  * ```python
    import tensorflow as tf
    from tensorflow_core.examples.tutorials.mnist import input_data
    ```

#### ë°ì´í„° ì¤€ë¹„

* mnist ë°ì´í„° ë¶ˆëŸ¬ì™€ì„œ X, yì— ë°ì´í„°ê°€ ë“¤ì–´ê°ˆ ê³µê°„ ë§Œë“¤ê¸°

  * ```python
    mnist = input_data.read_data_sets('data/mnist/',one_hot=True)
    
    # mnist ì†ê¸€ì”¨ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ í¬ê¸°ëŠ” 28 * 28 
    X = tf.placeholder(shape = [None, 784], dtype = tf.float32)
    # 0~9 ê¹Œì§€ ìˆ«ì ì†ê¸€ì”¨ ì´ê¸°ì— 10
    y = tf.placeholder(shape = [None, 10], dtype=tf.float32 )
    ```

#### ê°€ì„¤ ì„¤ì •

* í™œì„±í™” í•¨ìˆ˜ `relu` ì‚¬ìš©

* ì…ë ¥ì¸µ

  * ```python
    W1 = tf.Variable(tf.random_normal([784, 256]), name='weight1')
    b1 = tf.Variable(tf.random_normal([256]), name='bias1')
    layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)
    ```

* íˆë“ ì¸µ

  * ```python
    W2 = tf.Variable(tf.random_normal([256,256]), name='weight2')
    b2 = tf.Variable(tf.random_normal([256]), name='bias2')
    layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)
    ```

* ì¶œë ¥ì¸µ

  * ```python
    W3 = tf.Variable(tf.random_normal([256, 10]), name='weight3')
    b3 = tf.Variable(tf.random_normal([10]), name='bias3')
    
    logit = tf.matmul(layer2, W3) + b3
    H = tf.nn.relu(logit)
    ```

#### ì¤€ë¹„

* loss function

  * ```python
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))
    ```

* optimizer

  * Adam ì‚¬ìš©

  * ```python
    train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)
    ```

* session

  * ```python
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    ```

#### í•™ìŠµ

* ì´ í•™ìŠµ íšŸìˆ˜ 30íšŒ, í•™ìŠµ í•œë²ˆ ë‹¹ ë°°ì¹˜ì‚¬ì´ì¦ˆ 100ìœ¼ë¡œ ì„¤ì •

  * ```python
    num_of_epoch = 30
    batch_size = 100
    for step in range(num_of_epoch):
        total_iter = int(mnist.train.num_examples / batch_size)
        for i in range(total_iter):
            batch_x, batch_y = mnist.train.next_batch(batch_size)
            _, loss_val = sess.run([train, loss], feed_dict={X:batch_x, y:batch_y})
            if step % 3 == 0:
                print(f'loss : {loss_val}')
    ```

#### ì˜ˆì¸¡ ë° í‰ê°€

* ```python
  predict = tf.arg_max(H, 1)
  correct = tf.equal(predict, tf.arg_max(y, 1))
  accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))
  print(f'acc: {sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels})}')
  ```

* ```
  acc: 0.9516000151634216
  ```











































<br>

---



## ë²ˆì™¸

* ë²ˆì™¸ë¡œ í™œì„±í™”í•¨ìˆ˜ ê·¸ë˜í”„ ê·¸ë¦¬ëŠ” ê²ƒë„ ë³´ì—¬ì£¼ì‹¬

  * ```python
    import numpy as np
    import matplotlib.pyplot as plt
    
    def sigmoid():
        x = np.linspace(-10, 10, 100)
        y = 1 / (1 + np.exp(-x))
    
        plt.plot(x, y, 'black', linewidth=3)
        plt.xlim(-10, 10)
        plt.ylim(-1, 2)
        plt.grid(True)
        plt.show()
    
    def softmax(x1,x2,x3):
        y = np.exp(x1) + np.exp(x2) + np.exp(x3)
        return np.exp(x1) / y, np.exp(x2) / y, np.exp(x3)
    
    
    def softmax_test():
        x1 = np.linspace(-5, 5, 20)
        x2 = np.linspace(-5, 5, 20)
        # x3 = 1ë¡œ ê³ ì •
        y = np.zeros([20, 20, 3])
        for i in range(20):
            for j in range(20):
                y[i, j, :] = softmax(x1[i], x2[j], 1)
    
        m_x1, m_x2 = np.meshgrid(x1, x2)
    
        plt.figure(figsize=(8, 3))
        for i in range(2):
            ax = plt.subplot(1, 2, i+1, projection='3d')
            ax.plot_surface(m_x1, m_x2, y[:, :, i], rstride=1, cstride=1, alpha=0.3, color='blue', edgecolor='black')
            ax.set_xlabel('x1',fontsize=14)
            ax.set_ylabel('y1', fontsize=14)
            ax.view_init(40, -125)
    
        plt.show()
    
    def gauss(x, mu, sigma, a):
        return a * np.exp(-(x - mu) ** 2 / sigma ** 2)
    
    def gauss_test():
        x = np.linspace(-4, 8, 100)
    
        plt.plot(x, gauss(x, 0, 1, 1), 'red', linewidth=3, label='y=exp(-x^2)')
        plt.plot(x, gauss(x, 2, 3, 0.5), 'blue', linewidth=3, label='y=0.5exp(-(x-2)^2 / 3^2')
        plt.legend(loc='upper left')
        plt.xlim(-4, 8)
        plt.ylim(-0.5, 1.5)
        plt.grid(True)
        plt.show()
    
    
    
    
    if __name__ == '__main__':
        sigmoid()
        softmax_test()
        gauss_test()
    
    
    ```

  * sigmoid

    * ![image](https://user-images.githubusercontent.com/75322297/156510441-f5e640fb-5a01-47a7-bb25-eec0eddb8a28.png)

  * softmax

    * ![image](https://user-images.githubusercontent.com/75322297/156510505-7970b765-935d-4930-a87f-b2f42151dae7.png)

  * gauss

    * ![image](https://user-images.githubusercontent.com/75322297/156511411-33111b9e-c65f-4bf1-b268-0db813e79e7d.png)



